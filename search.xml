<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>Cost Function and Gradient Descent</title>
      <link href="/cost-function-and-gradient-descent/"/>
      <url>/cost-function-and-gradient-descent/</url>
      <content type="html"><![CDATA[<p>In the last <a href="/multilayer-perceptrons-and-activation-function">post</a> we’ve discussed how a Multilayer Perceptron works. Now let’s see how to search/update a set of weights to achieve a good classifer.<br><a id="more"></a></p><h2 id="Error-Function"><a href="#Error-Function" class="headerlink" title="Error Function"></a>Error Function</h2><p>We are on the search for the best set of weights for our classifier. Before we do that, we need something that tells us how good we are doing with the current set of weights. This is exactly what an <em><strong>error function</strong></em> AKA <em><strong>loss function</strong></em> AKA <em><strong>cost function</strong></em> does. It provides us a quantifiable metric depending on which we can update our set of weights. Let’s say our error function is $E(W)$ where W is the set of weights.</p><h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><p>Our goal now is to minimize $E(W)$ for the classifier to work. To do that, we have $W$ that we can use to change $E(W)$. Here is where a basic idea from calculus helps. A small change in the input of a function is related to the derivative in the following way</p><div class="note info"><p>$$f(x + \epsilon) \approx f(x) + \epsilon f’(x) $$</p></div><p>So, an $\epsilon$ change in the input produces a $\epsilon f’(x)$ change in the output. This is all we need to decrease our error function. Some math recap before we build error minimizing algorithm.<br>The derivative of a function at a point is</p><ul><li><strong>positive</strong> if the function is increasing in the neighborhood of the point.</li><li><strong>negative</strong> if it’s decreasing in the neighborhood of the point.</li><li><strong>zero</strong> if it’s neither increasing or decreasing.(saddle points)</li></ul><p>We can use this fact in our error function as shown below</p><div class="note info"><p>$$E(W+\epsilon) \approx E(W) + \epsilon E’(W) \hspace{1cm}\text{for a small enough}\hspace{0.2cm} \epsilon$$<br>$$\implies E(W+\epsilon) &lt; E(W)\hspace{1cm} if \hspace{0.2cm}E’(W) &lt; 0 $$<br>$$\implies E(W-\epsilon) &lt; E(W)\hspace{1cm} if \hspace{0.2cm}E’(W) &gt; 0 $$</p></div><p>So, all we have to do now is decrease the weights when derivate is positive and vice versa. If we repeat this operation with a small enough $\epsilon$ we can minimize the error function. This is the whole idea of <strong>Gradient Descent</strong>. A simple form for the algorithm is</p><div class="note info"><p>until we reach an error threshold or $E’(W) = 0$<br>$$W ← W - \epsilon sgn(E’(W))<br>\hspace{1cm}where\hspace{0.2cm}sgn(x) :=<br>\begin{cases}<br>-1 &amp; {x &lt; 0,} \\<br>0 &amp; {x = 0,} \\<br>1 &amp; {x &gt; 0.}<br>\end{cases}<br>$$</p></div><h2 id="Choosing-an-error-function"><a href="#Choosing-an-error-function" class="headerlink" title="Choosing an error function"></a>Choosing an error function</h2><p>The simple yet powerful algorithm depends on the error function to work. So, let’s explore how to choose an error function. In the <a href="/neuron-and-the-perceptron-algorithm/">perceptron post</a> we chose the number of misclassified points as our error function. But, <code># misclassified points</code> can only have integer values and is a discrete function. This is a poor choice for an error function, because</p><ul><li>In many situations you’ll find that the number of misclassified points can’t be decreased. The weights keep bouncing between a set of values.</li><li>Differentiating a discrete function.</li><li>All the misclassified points receive the same treatment. Points misclassified by a small margin aren’t any different from large error margins.</li></ul><p>To overcome these challenges we can emphasize on the following characteristics</p><ul><li>The error function must be <em><strong>continuous and differentiable</strong></em>.</li><li>Penalize misclassified points based on their error margins.</li></ul><p>Designing such function is for another post. Let’s see some limitations of gradient descent before moving to the next post.</p><h2 id="The-limitations-gradient-descent"><a href="#The-limitations-gradient-descent" class="headerlink" title="The limitations gradient descent"></a>The limitations gradient descent</h2><p>As long as $E’(W)$ is not zero, our weights keep updating and we’ll be approaching a minima in the error function. But what about the case when it’s zero, our algorithm terminates. But we don’t have to be at a minima when this happens. Points where $E’(W) = 0$ are called <strong>critical points</strong>. Three different scenarios for critical points are</p><ul><li><p><strong>Minima:</strong> Moving in any direction doesn’t decrease the function. <img src="https://www.dropbox.com/s/ekpujat5y46ey9z/minima.png?raw=1" alt="Minima figure"></p></li><li><p><strong>Maxima:</strong> Moving in any direction doesn’t increase the function. <img src="https://www.dropbox.com/s/iwq9jj1htsbak19/maxima.png?raw=1" alt="Maxima figure"></p></li><li><p><strong>Saddle Point:</strong> Moving in any direction doesn’t increase or decrease the function. <img src="https://www.dropbox.com/s/03wdjpjsih8tc2f/saddle.png?raw=1" alt="Saddle point figure"></p></li></ul><p>So our algorithm can stop at any of these situations. Even if we land on a minima it may not be the absolute lowest that is possible. When there are many local minima, we find ourselves stuck at the local minima. Different scenarios that can occur are shown in the figure below.<br><img src="https://www.dropbox.com/s/xapmmirt8xgmyt0/approximate_minimization.png?raw=1" alt="Approximate Minimization"></p><ol><li>Ideally, we would like to achieve this point (Global Minima), but it may not be possible.</li><li>This is at par with the global minima and is acceptable.</li><li>These points lead to poor performance.</li></ol><p>We’ll discuss tricks and tips to mitigate these limitations in another post. Here’s a little treat for making it till the end.</p><p><img src="https://imgs.xkcd.com/comics/listening.png" alt="Alexa joke"></p>]]></content>
      
      <categories>
          
          <category> AI </category>
          
          <category> Deep Learning Fundamentals </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Neural Networks </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Multilayer Perceptrons and Activation function</title>
      <link href="/multilayer-perceptrons-and-activation-function/"/>
      <url>/multilayer-perceptrons-and-activation-function/</url>
      <content type="html"><![CDATA[<h2 id="The-XOR-problem"><a href="#The-XOR-problem" class="headerlink" title="The XOR problem"></a>The XOR problem</h2><p>In the previous <a href="/deep-learning-fundamentals-neuron-and-the-perceptron-algorithm">post</a> you’ve seen how a perceptron works. Now let’s dive into some more interesting problems in deep learning. What follows is the classic XOR problem. Develop a method for the correct classification of the following points.</p><a id="more"></a><p><img src="https://www.dropbox.com/s/rks7mlcp2vubj3c/XOR_problem.png?raw=1" alt="XOR PROBLEM"></p><h2 id="Multi-Layer-Perceptron"><a href="#Multi-Layer-Perceptron" class="headerlink" title="Multi Layer Perceptron"></a>Multi Layer Perceptron</h2><p>It is clear that these points are linear inseparable. So, a perceptron fails at classifying these points. To classify these points it is clear that we need a non-linear boundary. If one perceptron can’t do the job, let’s try combining many of them. To keep it simple and the weights manageable, let’s use the architecture below.<br>(Note: Every layer stacked between the input and output layer is a hidden layer. We’ll see more about it later in this post.)</p><p><img src="https://www.dropbox.com/s/4ulft6rx1jnsrc7/mlp_without_activation.png?raw=1" alt="MLP without activation"></p><p>But this doesn’t work. Because, the whole output of the last unit is a linear function in $x$ and $y$. We can write the output of $h_3$ as<br>$h_3 = Ax + By + C$ where<br>$A = aw_{11} + bw_{12}$<br>$B = aw_{12} + bw_{22}$<br>$C = aw_{01} + bw_{02} + c$</p><h2 id="Activation-Function"><a href="#Activation-Function" class="headerlink" title="Activation Function"></a>Activation Function</h2><p>This is where an activation function comes to play. An activation function is a non-linear function applied to the end result of a perceptron. This introduces non-linearity to the perceptron and to the network.</p><p>Some common activation functions are</p><h3 id="ReLU-Rectified-Linear-Unit"><a href="#ReLU-Rectified-Linear-Unit" class="headerlink" title="ReLU (Rectified Linear Unit)"></a>ReLU (Rectified Linear Unit)</h3><p><strong><br>$$y = max(0, x)$$</strong><br><img src="https://www.dropbox.com/s/7evkd5143dsk8tu/relu.png?raw=1" alt="ReLU activation graph"></p><h3 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h3><p><strong><br>$$y =\frac{1}{1 + e^{-x}}$$</strong><br><img src="https://www.dropbox.com/s/wacrmulmj3b090i/sigmoid.png?raw=1" alt="Sigmoid activation graph"></p><h3 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h3><p><strong><br>$$y = \frac{ e^x - e^{-x}}{ e^x + e^{-x} }$$</strong><br><img src="https://www.dropbox.com/s/n8wm85xjqwav69t/tanh.png?raw=1" alt="Tanh activation graph"></p><p>Ok, so let’s apply “ReLU” to a perceptron and see what the response is. You can fiddle around with ReLU and become more familiar with it’s response <a href="https://www.desmos.com/calculator/ap5gwdoznu" target="_blank" rel="noopener">here</a></p><h2 id="Multi-Layer-Perceptron-Activated"><a href="#Multi-Layer-Perceptron-Activated" class="headerlink" title="Multi Layer Perceptron Activated"></a>Multi Layer Perceptron Activated</h2><p>Now let’s try activating the perceptrons in our network with relu. Our previous network will now be<br><img src="https://www.dropbox.com/s/n7m305d5noa5205/activation_and_mlp.png?raw=1" alt="MLP with activated perceptrons"></p><p><strong>Note:</strong> The final unit isn’t activated by relu, because we only need it to do binary classification. So we use a <a href="https://en.wikipedia.org/wiki/Heaviside_step_function" target="_blank" rel="noopener">Heaviside step function</a> instead. The weights $w_{01}, w_{02}, c$ are biases and are added implicitly and are not shown in the graph.</p><p>Now let’s fiddle around with the parameters we mentioned in the formulas above. Try to come up with a set of weights ($w_{01}, w_{11}, w_{21}, w_{02}, w_{12}, w_{22}, a, b, c$), so that you create a region that sepearates these points. If the graph below is too small you can work on it <a href="https://www.desmos.com/calculator/ggs1zd0ogl" target="_blank" rel="noopener">here</a>.</p><iframe src="https://www.desmos.com/calculator/ggs1zd0ogl" width="100%" height="600px" frameborder="0" allowfullscreen></iframe><h2 id="MLP-Terminology"><a href="#MLP-Terminology" class="headerlink" title="MLP Terminology"></a>MLP Terminology</h2><p>Hope you got an intuitive sense of how it works. Before we go further, let’s clear up some terminology. An MLP is a class of feed-forward neural network which we’ll know more about in a later post. In a feed-forward network</p><ul><li><p><strong>Hidden Layer</strong>: Any layer that doesn’t directly provide input or output is a hidden layer. The outputs and inputs of these layers are not directly visible or not a point of interest. Thus the name, <em>hidden layer</em>.</p></li><li><p><strong>Depth of a neural network</strong>: The depth of a neural network is the number of hidden layers plus one. Plus one because output weights can be modified. So by increasing the number of hidden layers, you’re increasing the depth of a neural network.</p></li><li><p><strong>Width of a hidden layer</strong>: The number of units in a hidden layer is also known as the width of the layer.</p></li></ul><p><img src="https://www.dropbox.com/s/q0c8n47q6xzb9hk/nn_depth_width.png?raw=1" alt="Neural network - Depth and Width"></p><h2 id="The-search-for-parameters"><a href="#The-search-for-parameters" class="headerlink" title="The search for parameters"></a>The search for parameters</h2><p>In the example above you came up with the set of weights that classify the XOR points. But, it’s not practical to search for a set of a weights, especially when we stack many more perceptrons. To understand why a naive search for a best set of weights doesn’t work, let’s consider a simple scenario. Let’s say we have a fully connected network(every node in a layer is connected to every node in the next layer.) and we have 3 hidden layers each with a 100 nodes. Input nodes are 2 and the output is a single node. So the number of parameters will be</p><ul><li><strong>between input and hidden layer 1:</strong> 2 x 100</li><li><strong>between hidden layer 1 and hidden layer 2:</strong> 100 x 100</li><li><strong>between hidden layer 2 and hidden layer 3:</strong> 100 x 100</li><li><strong>between hidden layer 3 and output:</strong> 100 x 1</li></ul><p>So, we end up with a total of 200 + 10000 + 10000 + 100 = <strong>20,300</strong> parameters. But those are just the number of parameters. Each parameter can have any arbitrary weight. But for the sake of a concrete example, let’s say we bound the weights between -100 and 100 with a step size of 0.01. So the number of possible values for each parameter is $(100 - (-100))/0.01$ = $20,000$. To find an ideal set of weights through naive search, you must search for one instance in 20,300 * 20,000 = 406,000,000. Searching naively through 406 million instances for a simple network is inefficient and unnecessary. This is where <strong>Stochastic Gradient Descent</strong> can make the search faster towards the goal. We’ll learn all about it in the next post.</p><p>Don’t forget your dopamine shot for making it till the end.<br><img src="https://imgs.xkcd.com/comics/computers_vs_humans.png" alt="xkcd - too cool to care"></p>]]></content>
      
      <categories>
          
          <category> AI </category>
          
          <category> Deep Learning Fundamentals </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Neural Networks </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Neuron and the Perceptron Algorithm</title>
      <link href="/neuron-and-the-perceptron-algorithm/"/>
      <url>/neuron-and-the-perceptron-algorithm/</url>
      <content type="html"><![CDATA[<p>This is the first part of the series <em>“Deep Learning Fundamentals”</em>. The goal of this series is to explore the mechanisms of artificial neural networks. The focus is more on presenting an intuitive way of understanding neural networks. So, you can expect an emphasis on how and why things work rather than what does the job. More often than not I’ll try to use simple math without focusing on notation. Let’s jump into the fundamental unit of most of the neural networks - “a neuron”.<br><a id="more"></a></p><h2 id="Neuron"><a href="#Neuron" class="headerlink" title="Neuron"></a>Neuron</h2><p>A neuron is nothing but a binary classifier. Which means given an n-dimensional space, it can divide the space into two regions. Let’s try to come up with an algorithm that does this, given that the data is linearly separable. For the sake of brevity, let’s consider that the data we deal with is numerical across all dimensions. (Dealing with other kinds of data is for another post.)</p><p>Without loss of generality let’s start working with two-dimensional data. So, let’s say there are some green(0) and blue(1) points on a plane and we want a line that separates these points. An intuitive way would be to draw a line like shown in the graph below. So far so good. But what we need finally is an algorithm that does this for us all this by itself and adjusts to any new points.<br><iframe src="https://www.desmos.com/calculator/smmkhlshap?embed" width="100%" height="300px" frameborder="0" allowfullscreen></iframe></p><h2 id="The-Search-for-a-Classifier"><a href="#The-Search-for-a-Classifier" class="headerlink" title="The Search for a Classifier"></a>The Search for a Classifier</h2><p>Let’s start with a random line in this space and then see what we must do, for creating the two regions. Let’s try it ourselves first. One way would be to rotate and translate the line until it separates the two regions. This seems intuitive enough. Play around with the interactive graph below to separate the two groups of points. Make sure the points are in their respective colored regions.</p><iframe src="https://www.desmos.com/calculator/cadms8e21r" width="100%" height="600px" frameborder="0" allowfullscreen></iframe><p>You would’ve noticed that $a$ and $b$ contribute to the rotation and $c$ contributes to translation. This makes sense because $a$ and $b$ control the slope and $c$ controls the distance from the origin. But how did you know when to stop fiddling around with those parameters? A trivial but useful answer would be to stop when there are no misclassified points.</p><p>At this point, we are good to develop a general algorithm for classifying two groups of points. We would do something like this</p><div class="note info"><pre><code>Initialize random line parameterswhile misclassified points are present:    fiddle around with the line parameters</code></pre></div><h2 id="The-perceptron-trick"><a href="#The-perceptron-trick" class="headerlink" title="The perceptron trick"></a>The perceptron trick</h2><p>Let’s start making the algorithm a little more concrete. Let the line be <strong>$ax_1 + bx_2 + c = 0$</strong>, where we initialize <strong>$a$</strong>, <strong>$b$</strong>, <strong>$c$</strong> at random. For classifying the points, let’s first give the points some labels(0 and 1) to differentiate them. Let $P(x_1, x_2)$ be a point in our data space with a label <strong>$L_P$</strong>. If <strong>$(ax_1+ bx_2 + c &gt; 0) = L_P$</strong> then <strong>$P$</strong>‘s classification is correct.</p><p>Now let’s deal with the fiddling part of the algorithm. Before we see any math on this, let’s try to build our intuition for it. Below are two cases</p><ul><li>case 1: <strong>$0$</strong> label point is misclassified.</li><li>case 2: <strong>$1$</strong> label point is misclassified.</li></ul><p>See if you can come up with a general rule for modifying <strong>$a$</strong>, <strong>$b$</strong> and <strong>$c$</strong> that leads to the correct classification of these points.</p><iframe src="https://www.desmos.com/calculator/684cghhob5" width="100%" height="600px" frameborder="0" allowfullscreen></iframe><iframe src="https://www.desmos.com/calculator/nxtk6rxo4z" width="100%" height="600px" frameborder="0" allowfullscreen></iframe><p>With a bit of effort, you can observe that the following pattern works for any situation</p><ul><li>increase parameters for a point labeled <strong>$1$</strong> and classified as <strong>$0$</strong>.</li><li>decrease parameters for a point labeled <strong>$0$</strong> and classified as <strong>$1$</strong>.</li></ul><p>Don’t worry if you haven’t found this out. Try it now and see if it works. The reason you might have missed the pattern is that you were trying to</p><ul><li>increase one parameter and decrease another.</li><li>increase or decrease the parameters by a random amount until it does the job.</li></ul><p>There’s nothing wrong with the above two operations because they do get the job done. But the problem is that we can’t generalize those operations for any given scenario. So, by this point, you should have a decent amount of intuition on how all this works. So, here’s how you can update the parameters. The amount by which they have to be updated is directly proportional to <strong>$x_1$</strong> for <strong>$a$</strong>, <strong>$x_2$</strong> for <strong>$b$</strong>. This translates to the following formula which works for both kinds of misclassification.<br></p><div class="note info"><p>For every misclassified point <strong>$P(x_1, x_2)$</strong>:<br><strong>$\hspace{1cm}a = a + \alpha (expected - predicted) x_1$</strong><br><strong>$\hspace{1cm}b = b + \alpha (expected - predicted) x_2$</strong><br><strong>$\hspace{1cm}c = c + \alpha (expected - predicted)$</strong></p></div><p></p><p>Here <strong>$\alpha$</strong> is the proportionality constant and is like a fine-tuning knob. It controls the rate at which we change these parameters, also known as the <em>“learning rate”</em>. We need our perceptron to change the parameters in a slow manner. Because large changes often tend to misclassify points that were correctly classified before. There aren’t any fixed good values for the learning rate. See for yourself how learning rate affects the speed and performance <a href="https://www.cs.utexas.edu/~teammco/misc/perceptron/" target="_blank" rel="noopener">here</a>.</p><p>In a Deep Learning setting the parameters $a$, $b$ are usually denoted by $w_1$, $w_2$ and are part of a vector $w$. $c$ is known as bias. The inputs and outputs of a neuron are as shown in the figure below.</p><p>$$z = w_0 + w_1x_1 + w_2x_2 + … + w_mx_m$$</p><p>$$<br>H(z) =<br>\begin{cases}<br>0 &amp; {n &lt; 0} \\<br>1 &amp; {n \geq 0}<br>\end{cases}<br>$$<br><img src="https://www.dropbox.com/s/j6vksnlibf0yhgq/perceptron.png?raw=1" alt="Perceptron"></p><h2 id="The-Perceptron-Algorithm"><a href="#The-Perceptron-Algorithm" class="headerlink" title="The Perceptron Algorithm"></a>The Perceptron Algorithm</h2><p>To wrap up this section, here’s the formal definition in a deep learning setting.</p><div class="note info"><p><strong>Data</strong>: Training Data:<strong>$(x_i , y_i )$</strong>; <strong>$\forall i \in {0, 1, 2, . . . , N }$</strong>, Learning Rate: <strong>$\eta$</strong>, where</p><ul><li>$x_i$ is a m-dimensional input vector and $N$ is the total number of instances of our data.</li><li>$x_{i, 0} = 1;$ $\forall i \in {0, 1, 2, . . . , N }$</li><li>${\hat y}_i$ is the prediction of a point <strong>$(x_i , y_i )$</strong></li></ul><p><strong>Result</strong>: Separating Hyper-plane coefficients :<strong>$w^∗$</strong><br><strong>Initialize</strong> <strong>$w$</strong> ← random weights ; (Since $x_{i, 0} = 1, w_0$ acts as the bias without setting it explicitly.)<br><strong>repeat</strong></p><p>get example <strong>$(x_i , y_i )$</strong>;<br><strong>$\hspace{1cm}\hat y_i ← w^T x_i $</strong>;<br><strong>$\hspace{1cm}w ← w + \eta(y_i − {\hat y}_i )x_i$</strong></p><p><strong>until</strong> convergence;</p></div><p>Here’s your dopamine shot for making it till the end.</p><p><img src="https://imgs.xkcd.com/comics/machine_learning.png" alt="XKCD comic"></p>]]></content>
      
      <categories>
          
          <category> AI </category>
          
          <category> Deep Learning Fundamentals </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Neural Networks </tag>
            
        </tags>
      
    </entry>
    
  
  
    
    <entry>
      <title>Categories</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
    <entry>
      <title>Portfolio</title>
      <link href="/portfolio/index.html"/>
      <url>/portfolio/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
    <entry>
      <title>links</title>
      <link href="/resources/index.html"/>
      <url>/resources/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
    <entry>
      <title>Topics</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
    <entry>
      <title>timeline</title>
      <link href="/timeline/index.html"/>
      <url>/timeline/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
    <entry>
      <title>Hello World</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
  
</search>
