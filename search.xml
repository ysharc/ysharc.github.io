<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>Overfitting and Underfitting</title>
      <link href="/overfitting-and-underfitting/"/>
      <url>/overfitting-and-underfitting/</url>
      <content type="html"><![CDATA[<p>Just to recap what we’ve seen so far,</p><ul><li><a href="/neuron-and-the-perceptron-algorithm/">Part 1: Neuron and the perceptron algorithm</a></li><li><a href="/multilayer-perceptrons-and-activation-function/">Part 2: Multilayer Perceptrons and Activation Function</a></li><li><a href="/cost-function-and-gradient-descent/">Part 3: Cost Function and Gradient Descent</a></li><li><a href="/forward-and-backward-propagation/">Part 4: Forward and Backward Propagation</a></li><li><a href="/loss-function-and-cross-entropy/">Part 5: Loss function and Cross-entropy</a></li></ul><a id="more"></a><p>By this point, we know how a feed-forward neural network works.</p><ol><li>Take in some input and calculate the output</li><li>Calculate the error</li><li>Calculate each weight’s contribution to the error.</li><li>Update weights such that the error decreases.</li><li>Repeat until the desired classification is achieved.</li></ol><p>This is it. But it looks too good to work. In fact, you’re right. Following just these steps above will land you in a lot of trouble in practice. Although it should work, the sheer volume of the data we have presents us many challenges. From this post forward, we’ll be looking at these problems one after the other.</p><h2 id="Training-and-Testing"><a href="#Training-and-Testing" class="headerlink" title="Training and Testing"></a>Training and Testing</h2><p>Till this post, we’ve only used the classifier to classify the points we give it while <a href="/forward-and-backward-propagation/#Neural-Network-terminology">training</a>. But, our goal often is to use our trained network to predict the outputs of instances which don’t have a label. This makes sense, because if we have classified points, why would we classify them again. The part where we predict the outputs of the new inputs is called <strong>testing</strong>.</p><p>Our goal now is to not only do good on the training data but also on the testing data. For example, let’s say you’ve trained a Dog breed classifier. Let’s say your training data had a million pictures of dogs and you achieved a 100% accuracy on the training set. All that would be useless if it doesn’t correctly classify the pictures of dogs you took on the sidewalk. Though this is very unlikely.</p><p>So, we train our neural network on the training set and expect it do well on the testing data. Here you assume that a <strong>data generating function($f$)</strong> has created your training and testing data. It is this function that our network tries to mimic. By adjusting our weights we’re trying to identify the data generating function $f$. Thus learning the characteristics of training data usually tells us something about the testing set as well. In practice, we can rarely identify the true data generating function. Because it is generated by so many natural random processes. We can only estimate it to a certain extent. This is called <strong>Generalization</strong>. When we generalize well, we perform well on both the training and testing sets.</p><h2 id="Expressive-power-Capacity"><a href="#Expressive-power-Capacity" class="headerlink" title="Expressive power / Capacity"></a>Expressive power / Capacity</h2><p>Can a neural network with one hidden layer and only two nodes in the hidden layer classify a set of points such as these</p><p><img src="https://lh3.googleusercontent.com/TUCifVfyqXEbIFYYgEOBw7Uojelx0b66NCAKyJ3JNw32B16epg6JILzRyYCmM_KwKnnaX041QgSQN_qY5scnwe2qIRFNHF20eAty4aN4Sr2CkHRuHuG3sfPEP0tSRVARFx3I-TGjLtTss7XklDVX8lT6z8W_qSEwGVCSBhmttDBUvTQaVRBgTSABOZf1P6IlTSD1TttCUk4lFCbDTsZWINJn5orVcPRwRO8p4k7pZWa7Ht3HEIZywmy62jeMeD4U64hkllpjYuIqYfKrYiCXAFAf7EKiKVrSE49_GWqh9gP9vh9PwZhMhf8q9UvdGhEt4up2beHRRYR1CBWTDvfQFvrhnk3oJNnKDGsVL8fBgRs1Z9b3yh5Ayz-bDqnhnW93RGVHvoSbNMyVGLyujz8ycqlz-JCiLb8tRnyERfso2WK6xm4HvqKDC8zOQDsJsp2fQ9BolbWq2mFBgUmEW3LjvesrG022xoHYDmyQK8AFVJJxASBUnWH-qsTG7dMZkUiUcbBgZrARtBBf7AzEoJGHreGqJoD31r0QCF3485UsAobiyCANmQ8W_eTjRH9881wSBHFvjKXXjYOCkn4EfU9KxrL7ukx9Yf8Hepw9ag=s710-no" alt="annular points"></p><p>You can play with it <a href="https://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=2&amp;seed=0.97760&amp;showTestData=false&amp;discretize=true&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false&amp;problem_hide=true&amp;noise_hide=true&amp;dataset_hide=true&amp;batchSize_hide=true&amp;regularizationRate_hide=true&amp;percTrainData_hide=true&amp;discretize_hide=true&amp;regularization_hide=true" target="_blank" rel="noopener">here</a> if you want to try it out. Change the learning rate and activation functions as you like. Finally, press the play button to train the network and see how well it fits. You just can’t do it perfectly, unless you design a new activation. This is where we can talk about the expressive power/capacity of a neural network. It is the ability of a neural network to mimic the underlying data generator function. In other words, how well can it generalize over the given data.</p><h2 id="Under-fitting"><a href="#Under-fitting" class="headerlink" title="Under-fitting"></a>Under-fitting</h2><p>When a network has a low capacity, it struggles to generalize well. This results in a highly biased network. What does it mean for a network to be biased? It means that the network has learned very few rules to classify the data. For example, let’s say we’re trying to classify a set of cars. Let’s say our training set has 3 different cars, a cat, a dog and a bike. Our testing set contains two cars, a bike, and a chicken. We want our classifier to do something like this</p><p><img src="https://lh3.googleusercontent.com/ztOH7E0ZnQh3s4vJWx3o3fJtYP2PJyKyCjAYcp7YA0H_zaanoc58c5djoG6vxnID0EHs75Xp8z99xc6CIgE7fwbCxnPSgjzKHuPpzKHTlOX_Q1w2zgrVDlQDzc3zzqik9Pc5DMotNkayCDJE-elgztb-G5YlorB0_j1jGY9xaqDS_MzaRO3IlMLO3M5H0LXLJkG1p3ZgHCW2xVvjOrllk6zC_M5Zq0DwI_nZNcf9oPAx8p-qIUNA8-9Xsv-4mU4KIWAMT9iYBiS8pVyG4k0LihSWOHaDsU5HSGJn_0gvdkxHhqfCfLXIeel_k2mqDusc_EIKatz5yoZ0TRoWHqRlpS02FmCWXjqmzHqTxcgFfpmjqkdBbHI66l9rBEc7uEYTLBhaw1vqM6l_rqrIkRV236I_G7lfDdrIhNXHwzBEzA3y2MqVmxl0ra9ThZaWgT_gFM39Q3knioVTdZdWfPad5zEWDkoEFp4XGdDXHh8BNKcbTKTTpqgY2nsH50fyUfQPnBxR4O8WIiME-cCbWTqkHmmF7ks6dItgE7mBApt5EC3D9ACPBLv_ACRdsDoZzZPa8jsAXXQlaxgm8Gs3eD5wN9XXbEaWizZE_AyWug=w1005-h710-no" alt="perfect fit illustration"></p><p>As you can see a good classifier identifies cars and not cars. A low capacity network, which is highly biased is said to be under-fit. It learns something like the following</p><p><img src="https://lh3.googleusercontent.com/6IfsRnUJpc10j-0amRpiKcbFEPcmYvAl0AsmtxMAvDMn_hKDivNBWPS176dgPEzn23pADLzwSzRd1LAUcaNC3HhUXBReHNzu0McD5K3Np32RXhPorP1JGxGGfK1c9ilyI4yHIll4NWonY5RySB7SeMNfVM88ukVcbu7l7qijpI8EporEMvUSTgwuPKBR5NdcKbsKoUY-skrqV9Ah7Wb6PThHuZmIm2DVFKuoKo9y-xK4kzzoaYQIr2dMVa_SCixQzZEKiAuKbJxxi6yumEgBlP1EQG20JL109oEn1USxntPHUuQGJ4h0HrOF8bXyGr98G39G-ysY0fwrigao4d-cxUC4T9h4isyPiPrdMxQcFWzraiqUk8kA-xcQngbsgm4vpqizB3zG02tZzgMb9P7LRAikMpVr42Xk4x0ydvBMDzGUvGLp4lUEFDCkEuqC0KmbSifPlz6x8IJAfJ2UD1NOwLTwi1YzBpy4x2iwx49BpVVeawlLSWmBkiIDUPk2aHiyqgpwSr5_cNVq_OKOVRGOFPzd5Z5JgQwd1XbhhZcsRCWI0oqPbYJ1TI-wC3YpN1UPp877pyPD95xHL7YcQupuy76oxOatYAg8cnVxCA=w1005-h710-no" alt="under fit illustration"></p><p>So it has only learned to distinguish between vehicles and not vehicles, because of it’s less capacity.</p><h2 id="Over-fitting"><a href="#Over-fitting" class="headerlink" title="Over-fitting"></a>Over-fitting</h2><p>In contrast to under-fitting, when a model has more capacity, there is a chance that it can over-fit. when overfitting your network fits too good to the training set and doesn’t generalize well. It results in a high-variance network. It means that your network learns rules that are too specific to the training data.</p><p><img src="https://lh3.googleusercontent.com/TwCAPdPbIinxr2lS0cX-UWn6t6f8LRWxkANuUt5CyTJ_1z-KUTyRYvkipM_3tYm0OycfH0lzAxX2LqOSWXmR4vqS_X4oUAsLi4TAOkGpIAiczCE1EJAtFktksdj_1yWO-TKrg2Mtq9aGjO2EcIqi-DgLJnDtfQ5VM2YgJcISnWf9YTd-8F0wN80IimWEoYkmqoFdUqav_oF8k0HYFVkdri7LZ4u3XCcEXxuRSo0h2WvcpF_srecVYrNw0ClOSyf3WNFfh3QdodIB_BNKRQUtvnsMhj-Z6vHpyi26mmbUF9Q5s1p-rB0fmGjGHteQbrfTwPc0QPNeEH-esgaHbfOJQi74qHYeKL52g7KGuVYP1U3m3LpHyGMf9Rsxs6agVakys6AJ1ZvXhOpROyx7CXVdlqg90nwYIcrweS0YrB9Xq9pVJNIGp4EIpZnh3W4yLectfR72yaCzO8KvFP0vQ8CypSitYqtEhKf2S0xNJZ_IrF6KKhl3klG0TTOiLJS5dM-3aqZynwycvzFh-gfSwqsMD79_j-1cypZOdSlTSse3kubUzu1GFMMuXyri3OtO_KKfDYfmBJY9kLlHRwFNYV4D-BfB9xmdOPfQE7Srwg=w1005-h710-no" alt="over fit illustration"></p><p>Here it has learned the rule to classify red, blue and yellow cars. Because of its high capacity and maybe due to overtraining.</p><p><strong>Note:</strong> This version of underfitting and overfitting is just for illustrative purposes. In practice, you cannot really know what rules the network has learned. We’re dealing with very high dimensional data. Our understanding of it is very limited and is an active area of research.</p><p>Let’s see some techniques for overcoming these problems in the next post. Enjoy your comic for now :)</p><p><img src="https://imgs.xkcd.com/comics/electoral_precedent.png" alt="overfitting xkcd"></p><p>Icon Credits: Icons by <a href="https://www.freepik.com" target="_blank" rel="noopener">freepik</a></p>]]></content>
      
      <categories>
          
          <category> AI </category>
          
          <category> Deep Learning Fundamentals </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Neural Networks </tag>
            
            <tag> common problems </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Loss function and Cross Entropy</title>
      <link href="/loss-function-and-cross-entropy/"/>
      <url>/loss-function-and-cross-entropy/</url>
      <content type="html"><![CDATA[<p>In the <a href="/cost-function-and-gradient-descent">gradient descent post</a>, you’ve seen what an error function is. What the characteristics of a good loss function are. Let’s take a deep dive into how we achieve this.<br><a id="more"></a><br>The desired characteristics of a good loss function are</p><ul><li>Continuous and Differentiable.</li><li>Penalizes wrong classification heavily and correct classifications weakly.</li></ul><h2 id="Prediction-as-a-probability"><a href="#Prediction-as-a-probability" class="headerlink" title="Prediction as a probability"></a>Prediction as a probability</h2><p>So in the <a href="/neuron-and-the-perceptron-algorithm/">perceptron post</a> we’ve used Heaviside step function to decide whether the output was 0 or 1. But our ideal loss function demands a continuous and differentiable function. Thus we can hack our loss function in such a way that it gives the probability that the given input belongs to a class. For example if there are two classes $A$ and $B$. After the hack we expect a result $P(A)$ from our error function. The probability of our input being $B$ can be easily calculated with $P(B) = 1 - P(A)$.</p><p>All we have to do to achieve this is change the activation function. In the <a href="/multilayer-perceptrons-and-activation-function/">activation post</a> we’ve seen the three most common activation functions - $ReLU$, $sigmoid$ and $tanh$. $ReLU$ is only piece-wise continuous. For this reason it is avoided as the activation function of the final output. $tanh$ produces outputs in the range of $(-1, 1)$. But we want the output to be in the range of $[0, 1]$ for representing the probability. $sigmoid$ does the job here. It is a good choice producing outputs in the range of $(0, 1)$.</p><p>$$sigmoid(x) = \frac{1}{1+e^{-x}}$$<br><img src="https://www.dropbox.com/s/wacrmulmj3b090i/sigmoid.png?raw=1" alt="Sigmoid function graph"></p><h2 id="Finding-the-ideal-function"><a href="#Finding-the-ideal-function" class="headerlink" title="Finding the ideal function"></a>Finding the ideal function</h2><p>We now have a neural network that produces a probability $p$. Our error can take advantage of this fact. Now the only thing left is the penalty part. For a point labeled 1, we need our error function to be</p><ul><li>high when it’s output is closer to 0 (since we have a low probability that the point is 1)</li><li>low when it’s output is closer to 1. (zero when we’re absolutely confident i.e probability is 1.)</li></ul><p>You can explore functions that have this characteristic. One of the most used function for  this purpose is $-ln(p)$. See the response of $-ln(p)$ below.</p><p><img src="https://www.dropbox.com/s/mzsndm5pgell6uq/log-loss.png?raw=1" alt="log-loss or cross entropy"></p><h2 id="Cross-Entropy"><a href="#Cross-Entropy" class="headerlink" title="Cross Entropy"></a>Cross Entropy</h2><p>It’s time to design ourselves an error function that would work for both the labels $0$ and $1$. If $-ln(p)$ gives us the penalty for classifying label $1$, $-ln(1-p)$ is the penalty for classifying label $0$. Since $1-p$ is the probability of the point being 0. Now all we have to do is take the penalty for 1 when the true label is $1$ and the penalty for 0 when the label is $0$. We can do so easily by the following formula</p><p>$$<br>\text{when y is the true label and p is the probability that the label is 1} \\<br>-y.ln(p) - (1-y).ln(1-p) =<br>\begin{cases}<br>-ln(1-p)  &amp; {\text{if y=0}} \\<br>-ln(p) &amp; {\text{if y=1}}<br>\end{cases}\\<br>$$</p><p>$-y.ln(p) - (1-y).ln(1-p)$ is known as <strong>cross-entropy</strong>.</p><h2 id="Multi-class-Classification"><a href="#Multi-class-Classification" class="headerlink" title="Multi-class Classification"></a>Multi-class Classification</h2><p>Till now we’ve seen the problems where there only two outcomes i.e binary classification. Now let’s see a problem with more than two labels. Assume we are given four temperatures taken at 12PM, 6PM, 12AM, 6AM of a particular city. Our task is to classify the given temperatures into spring, summer, autumn or winter. So, we need to have four outputs here. Each corresponding to a season. In binary classification, our labels were straight forward, either $0$ or $1$. But how are labels for a multi-class problems set? We prefer not to set the labels to $0, 1, 2, 3$. We’ll discuss why this is the case in another post. But for now let’s see a popular way of setting labels for multi-class problems.</p><h3 id="One-hot-encoding"><a href="#One-hot-encoding" class="headerlink" title="One-hot encoding"></a>One-hot encoding</h3><p>We create an array of size $n$, where $n$ is the number of classes. Then we designate each of the classes a position in the array. For a label that needs to specify a class, the position corresponding to the class is filled with $1$ and rest all with $0$. Let’s see this in a concrete example. For the season problem above, one way to set labels is as follows<br>$$<br>label_{\text{spring}} = \begin{pmatrix}<br>1 \\<br>0 \\<br>0 \\<br>0<br>\end{pmatrix}, label_{\text{summer}} = \begin{pmatrix}<br>0 \\<br>1 \\<br>0 \\<br>0<br>\end{pmatrix}, label_{\text{autumn}} = \begin{pmatrix}<br>0 \\<br>0 \\<br>1 \\<br>0<br>\end{pmatrix}, label_{\text{winter}} = \begin{pmatrix}<br>0 \\<br>0 \\<br>0 \\<br>1<br>\end{pmatrix}<br>$$</p><p>This is known as <strong>One-hot encoding</strong>.</p><h2 id="Multi-class-Cross-Entropy"><a href="#Multi-class-Cross-Entropy" class="headerlink" title="Multi-class Cross Entropy"></a>Multi-class Cross Entropy</h2><p>We also need to change <em>cross entropy</em> to adapt to the multi-class problem. Before we do that, we need to change the output of our neural network accordingly. In binary classification the output was a probability of the label belonging to a class. Here we need multiple outputs producing the probability of each class.</p><h3 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h3><p>We used sigmoid function for a binary classifier to convert the output to a probability. We use a similar trick in the multi-class problem. The idea is to normalize each of the output with the sum of the outputs. In order to that we need all the outputs to be positive, as probability cannot be negative. So, we change each output $y_i$ to $e^{y_i}$. Now, we can normalize and get the probability of each output. This procedure is known as <strong>softmax</strong>. Let’s see a concrete example from the season problem defined above</p><p>$$\hat{Y} = \begin{pmatrix}<br>\hat{y_1} \\<br>\hat{y_2} \\<br>\hat{y_3} \\<br>\hat{y_4}<br>\end{pmatrix}<br>\begin{matrix}<br>\rightarrow \text{ this corresponds to spring} \\<br>\rightarrow \text{ this corresponds to summer} \\<br>\rightarrow \text{ this corresponds to autumn} \\<br>\rightarrow \text{ this corresponds to winter}<br>\end{matrix} \\<br>softmax(\hat{Y}) = \begin{pmatrix}<br>\dfrac{e^{\hat{y_1}}}{e^{\hat{y_1}}+e^{\hat{y_2}}+e^{\hat{y_3}}+e^{\hat{y_4}}} \\<br>\dfrac{e^{\hat{y_2}}}{e^{\hat{y_1}}+e^{\hat{y_2}}+e^{\hat{y_3}}+e^{\hat{y_4}}} \\<br>\dfrac{e^{\hat{y_3}}}{e^{\hat{y_1}}+e^{\hat{y_2}}+e^{\hat{y_3}}+e^{\hat{y_4}}} \\<br>\dfrac{e^{\hat{y_4}}}{e^{\hat{y_1}}+e^{\hat{y_2}}+e^{\hat{y_3}}+e^{\hat{y_4}}}<br>\end{pmatrix}<br>\begin{matrix}<br>\dfrac{\rightarrow \text{ probability of output being spring}}{} \\<br>\dfrac{\rightarrow \text{ probability of output being summer}}{} \\<br>\dfrac{\rightarrow \text{ probability of output being autumn}}{} \\<br>\dfrac{\rightarrow \text{ probability of output being winter}}{}<br>\end{matrix}<br>$$</p><p>With this, we’ve all the necessary elements for training and testing neural networks. From the next post, we’ll look at common problems and their respective solutions. Don’t forget your comic.</p><p><img src="https://imgs.xkcd.com/comics/wisdom_of_the_ancients.png" alt="XKCD Wisdom of the ancients"></p>]]></content>
      
      <categories>
          
          <category> AI </category>
          
          <category> Deep Learning Fundamentals </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Neural Networks </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Forward and Backward Propagation</title>
      <link href="/forward-and-backward-propagation/"/>
      <url>/forward-and-backward-propagation/</url>
      <content type="html"><![CDATA[<p>Till now we’ve seen</p><ul><li><a href="/multilayer-perceptrons-and-activation-function/">How a Multilayer Perceptron works</a></li><li><a href="/cost-function-and-gradient-descent/">How Gradient Descent helps Multilayer Perceptron</a></li></ul><p>Now let’s see some tricks that extend these ideas to Deep neural networks.<br><a id="more"></a></p><h2 id="Neural-Network-terminology"><a href="#Neural-Network-terminology" class="headerlink" title="Neural Network terminology"></a>Neural Network terminology</h2><p>From the previous posts, you know that neural networks are good at classification. It does this in four steps</p><ol><li>Calculate the output of the current network.</li><li>Compare the output with the correct output.</li><li>Update weights of the network by Gradient Descent.</li><li>Repeat steps 1-3 until you achieve a good classifier.</li></ol><p>Before we proceed further it’s better to know some terminology and notations.</p><div class="note info"><ul><li><strong>Training</strong>: The whole process shown in the 4 steps above.</li><li><strong>instance</strong>: A single data point from the set of data points we’re trying to classify.</li><li><strong>Label</strong>: The correct/desired output of an instance.</li><li><strong>Prediction</strong>: Calculating the output for a given input using our current set of weights.</li></ul></div><h2 id="Some-standard-notations"><a href="#Some-standard-notations" class="headerlink" title="Some standard notations"></a>Some standard notations</h2><p>In the <a href="/multilayer-perceptrons-and-activation-function/">activation and MLP post</a> the final output looked something like this $a(Max(0, w_{01}+w_{11}x+w_{21}y)) + b(Max(0, w_{02}+w_{12}x+w_{22}y))+c &gt; 0$. Imagine what a network with hundreds or even thousands of perceptrons would produce. To simplify this, we can stick to a standard notation and abstract away all the cumbersome stuff. Let’s use the following neural network as a reference.<br>(Note: This notation is adapted from <a href="https://www.deeplearning.ai/" target="_blank" rel="noopener">Andrew Ng’s Deep Learning series</a> and tweaked a little.)</p><p><img src="https://www.dropbox.com/s/iws8e9brfxmaj23/neural_network_terminology.png?raw=1" alt="Neural Network Architecture"></p><h3 id="Representation-of-the-neural-network"><a href="#Representation-of-the-neural-network" class="headerlink" title="Representation of the neural network"></a>Representation of the neural network</h3><p>Let $L$ be the number of layers. Input is not considered a layer and is often called Layer 0. The hidden layers till the output are numbered from 1 to L. Denote the number of units in the $l^{th}$ layer by $n^{[l]}$. Notations of other units follow.</p><h3 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h3><p>We use $X/A^{[0]}$ to denote the input. In our network it will be</p><div class="note info"><p>$$X = A^{[0]} = \begin{bmatrix}<br>x_1 \\<br>x_2<br>\end{bmatrix}$$</p></div><h3 id="Weights-and-Bias"><a href="#Weights-and-Bias" class="headerlink" title="Weights and Bias"></a>Weights and Bias</h3><p>Consider the weights between input layer and the first hidden layer as shown below</p><p><img src="https://www.dropbox.com/s/ddyj4njtrru1nzq/weights_and_biases.png?raw=1" alt="Weights and Biases"></p><p>We denote the weights between $i^{th}$ and ${i-1}^{th}$ layer as $W^{[i]}$ and the bias before $i^{th}$ layer as $B^{[i]}$. So in the figure above $W^{[1]}$ and $B^{[1]}$ represent</p><div class="note info"><p>$$<br>W^{[1]} = \begin{bmatrix}<br>w_{11}^{[1]} &amp; w_{21}^{[1]} \\<br>w_{12}^{[1]} &amp; w_{22}^{[1]}<br>\end{bmatrix}<br>$$</p><p>$$<br>B^{[1]} = \begin{bmatrix}<br>b_1^{[1]} \\<br>b_2^{[1]}<br>\end{bmatrix}<br>$$</p></div><h3 id="Activations"><a href="#Activations" class="headerlink" title="Activations"></a>Activations</h3><p>Activations are a little tricky. Because two operations are performed in the a single activation unit. For the $i^{th}$ activation layer the operations are</p><ol><li>$Z^{[i]} = W^{[i]}A^{[i-1]} + B^{[i]}$</li><li>$A^{[i]} = g^{[i]}(Z^{[i]}), \hspace{0.5cm} \text{where g is the activation function.}$</li></ol><p>Let’s make this a little more concrete by considering the first activation layer.</p><div class="note info"><p>$$<br>Z^{[1]} = W^{[1]}A^{[0]} + B^{[1]}\\<br>\implies Z^{[1]} = \begin{bmatrix}<br>w_{11}^{[1]} &amp; w_{21}^{[1]} \\<br>w_{12}^{[1]} &amp; w_{22}^{[1]}<br>\end{bmatrix}\begin{bmatrix}<br>x_1 \\<br>x_2<br>\end{bmatrix} + \begin{bmatrix}<br>b_1^{[1]} \\<br>b_2^{[1]}<br>\end{bmatrix}<br>$$</p><p>$$<br>\text{Assuming the activation is ReLU}\\<br>A^{[1]} = ReLU(Z^{[1]})\\<br>\implies A^{[1]} = ReLU\left(\begin{bmatrix}<br>w_{11}^{[1]} &amp; w_{21}^{[1]} \\<br>w_{12}^{[1]} &amp; w_{22}^{[1]}<br>\end{bmatrix}\begin{bmatrix}<br>x_1 \\<br>x_2<br>\end{bmatrix} + \begin{bmatrix}<br>b_1^{[1]} \\<br>b_2^{[1]}<br>\end{bmatrix}\right)\\<br>\implies A^{[1]} = ReLU\left(\begin{bmatrix}<br>w_{11}^{[1]}x_1 + w_{21}^{[1]}x_2 + b_1^{[1]} \\<br>w_{12}^{[1]}x_1 + w_{22}^{[1]}x_2 + b_2^{[1]}<br>\end{bmatrix}\right)<br>$$</p></div><h3 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h3><p>The output is the last activation layer $A^{[L]}$. It is also denoted by $\hat{Y}$ and calculated by the same logic of the activation layer.</p><h2 id="Forward-Propagation"><a href="#Forward-Propagation" class="headerlink" title="Forward Propagation"></a>Forward Propagation</h2><p>The process of forward propagation is to calculate the outputs of each hidden layer. Setting $A^{[0]} = X$, we can start calculating the outputs as follows</p><p>$$<br>Z^{[1]} = W^{[1]}A^{[0]} + B^{[1]},\hspace{0.5cm} A^{[1]} = g^{[1]}(Z^{[1]}) \\<br>Z^{[2]} = W^{[2]}A^{[1]} + B^{[2]},\hspace{0.5cm} A^{[2]} = g^{[2]}(Z^{[2]}) \\<br>Z^{[3]} = W^{[3]}A^{[2]} + B^{[3]},\hspace{0.5cm} A^{[3]} = g^{[3]}(Z^{[3]}) \\<br>. \\<br>. \\<br>. \\<br>Z^{[L]} = W^{[L]}A^{[L-1]} + B^{[L]},\hspace{0.5cm} A^{[L]} = g^{[L]}(Z^{[L]})$$</p><p>In a nutshell, given $A^{[l-1]}$ feed forward produces the output $A^{[l]}$. The outputs are calculated in a linear way, hence the name feed forward networks.</p><div class="note info"><ul><li><strong>Feed Forward</strong>: The process of calculating outputs of each hidden layer for a given input.</li></ul></div><h2 id="Backward-Propagation"><a href="#Backward-Propagation" class="headerlink" title="Backward Propagation"></a>Backward Propagation</h2><p>At this stage we have our predicted labels. It is time for us to update the weights based on our error. To update our weights, we must first look at the gradient with respect to the weights. Use the gradients and move the weights in the direction that decreases our error. In the gradient descent post we took it for granted that our error function is a function of weights. But this is indirectly the case. In practice any error function calculates it’s metric by comparing two things</p><ul><li>the predicted output $\hat{Y}$ and </li><li>the desired output $Y$(label)</li></ul><p>So it’s fair to write $E$ is a function of $Y $ and $\hat{Y}$.</p><p>Remember that $\hat{Y}$ is dependent on all the weights and biases of the previous layer. So, we can easily show $E$ as a function of every weight layer.</p><p>$$<br>\begin{align}<br>E(Y, \hat{Y}) &amp;= E(Y, A^{[L]})\\<br>&amp;= E(Y, g^{[L]}(Z^{[L]})) \\<br>&amp;= E(Y, g^{[L]}(W^{[L]}A^{[L-1]} + B^{[L]})) \\<br>&amp;= E(Y, g^{[L]}(W^{[L]}g^{[L-1]}(Z^{[L-1]}) + B^{[L]})) \\<br>&amp;= E(Y, g^{[L]}(W^{[L]}g^{[L-1]}(W^{[L-1]}A^{[L-2]} + B^{[L-1]}) + B^{[L]})) \\<br>\end{align}<br>$$<br>expanding any further is sheer lunacy. But, you get the point that the error function can be written in terms of each weight layer. Now, we can calculate the gradient with respect to each weight layer.</p><h3 id="Chain-Rule"><a href="#Chain-Rule" class="headerlink" title="Chain Rule"></a>Chain Rule</h3><p>For obtaining the partial derivative, we have to differentiate this composite function. This is where the chain rule from calculus can help us. If a variable z depends on variable y, which in turn depends on the variable x then</p><p>$$\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} \frac{\partial y}{\partial x}$$</p><p>Using this we can write the derivative of $E$ with respect to $W^{[L]}$ as follows<br>$$<br>\begin{align}<br>\frac{\partial E}{\partial W^{[L]}} &amp;= \frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial W^{[L]}} \\<br>&amp;= \frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial W^{[L]}}\\<br>&amp;=\frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}} \frac{\partial Z^{[L]}}{\partial W^{[L]}}\\<br>&amp;=\underbrace{\frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}}}_{\frac{\partial E}{\partial Z^{[L]}}} A^{[L-1]}<br>\end{align}<br>$$</p><p>In a similar fashion<br>$$<br>\frac{\partial E}{\partial B^{[L]}} = \frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}} \frac{\partial Z^{[L]}}{\partial B^{[L]}}\\<br>\implies \frac{\partial E}{\partial B^{[L]}} = \frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}} = \frac{\partial E}{\partial Z^{[L]}}\\<br>$$</p><h3 id="Cache-the-previous-output"><a href="#Cache-the-previous-output" class="headerlink" title="Cache the previous output"></a>Cache the previous output</h3><p>This looks fine for the $L^{th}$ layer but doesn’t it get huge as we approach the derivative with respect to weight layer 1. Let’s see what happens at the ${L-1}^{th}$ derviative.</p><p>$$<br>\begin{align}<br>\frac{\partial E}{\partial W^{[L-1]}} &amp;= \frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial W^{[L-1]}} \\<br>&amp;= \frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial W^{[L-1]}}\\<br>&amp;=\frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}} \frac{\partial Z^{[L]}}{\partial W^{[L-1]}}\\<br>&amp;=\frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}} \frac{\partial Z^{[L]}}{\partial W^{[L-1]}} \\<br>&amp;=\frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}} \frac{\partial Z^{[L]}}{\partial A^{[L-1]}} \frac{\partial A^{[L-1]}}{\partial W^{[L-1]}} \\<br>&amp;=\frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}} \frac{\partial Z^{[L]}}{\partial A^{[L-1]}} \frac{\partial A^{[L-1]}}{\partial g^{[L-1]}} \frac{\partial g^{[L-1]}}{\partial W^{[L-1]}}\\<br>&amp;=\frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}} \frac{\partial Z^{[L]}}{\partial A^{[L-1]}} \frac{\partial A^{[L-1]}}{\partial g^{[L-1]}} \frac{\partial g^{[L-1]}}{\partial Z^{[L-1]}} \frac{\partial Z^{[L-1]}}{\partial W^{[L-1]}}\\<br>&amp;=\underbrace{\frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}} \frac{\partial Z^{[L]}}{\partial A^{[L-1]}} \frac{\partial A^{[L-1]}}{\partial g^{[L-1]}} \frac{\partial g^{[L-1]}}{\partial Z^{[L-1]}}}_{\frac{\partial E}{\partial Z^{[L-1]}}} A^{[L-2]}\\<br>\end{align}<br>$$</p><p>Notice while calculating $\frac{\partial E}{\partial W^{[L-1]}}$ we already had the result of $\frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}}$ from computing $\frac{\partial E}{\partial W^{[L]}}$. We can save this calculation from every previous layer and use it in our current computation. So, it won’t be messy when we get down to the last layer. In general going down a layer each time you’ll notice a pattern that<br><img src="https://www.dropbox.com/s/l15ikka0kx50w82/back_prop_steps.svg?raw=1" alt="BackPropagation trick"></p><h3 id="Updating-the-weights"><a href="#Updating-the-weights" class="headerlink" title="Updating the weights"></a>Updating the weights</h3><p>So we’ve come to the final part of this simple algorithm. Remember that we need to take small steps in the direction decreasing the error. Here $\alpha$, the learning rate does the job. As long as you don’t set the learning rate high, you should be fine. Finding a good learning rate is for another post.</p><p>Updating the weights is straightforward</p><p>$$W^{[i]} := W^{[i]} - \alpha \frac{\partial E}{\partial W^{[i]}}$$<br>$$B^{[i]} := B^{[i]} - \alpha \frac{\partial E}{\partial B^{[i]}}$$</p><div class="note info"><ul><li><strong>Backward Propagation</strong>: The process of calculating each weights’ contribution to the error and updating them accordingly.</li></ul></div><p>If you don’t get something from the post, relax! Take a step back and rework it again. Despite what you’ve heard, back propagation is as simple idea. Uses basic calculus and linear algebra. Ending this post on a relief note :)<br><img src="https://imgs.xkcd.com/comics/too_old_for_this_shit.png" alt="Too old for math"></p>]]></content>
      
      <categories>
          
          <category> AI </category>
          
          <category> Deep Learning Fundamentals </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Neural Networks </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Cost Function and Gradient Descent</title>
      <link href="/cost-function-and-gradient-descent/"/>
      <url>/cost-function-and-gradient-descent/</url>
      <content type="html"><![CDATA[<p>In the last <a href="/multilayer-perceptrons-and-activation-function">post</a> we’ve discussed how a Multilayer Perceptron works. Now let’s see how to search/update a set of weights to achieve a good classifer.<br><a id="more"></a></p><h2 id="Error-Function"><a href="#Error-Function" class="headerlink" title="Error Function"></a>Error Function</h2><p>We are on the search for the best set of weights for our classifier. Before we do that, we need something that tells us how good we are doing with the current set of weights. This is exactly what an <em><strong>error function</strong></em> AKA <em><strong>loss function</strong></em> AKA <em><strong>cost function</strong></em> does. It provides us a quantifiable metric depending on which we can update our set of weights. Let’s say our error function is $E(W)$ where W is the set of weights.</p><h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><p>Our goal now is to minimize $E(W)$ for the classifier to work. To do that, we have $W$ that we can use to change $E(W)$. Here is where a basic idea from calculus helps. A small change in the input of a function is related to the derivative in the following way</p><div class="note info"><p>$$f(x + \epsilon) \approx f(x) + \epsilon f’(x) $$</p></div><p>So, an $\epsilon$ change in the input produces a $\epsilon f’(x)$ change in the output. This is all we need to decrease our error function. Some math recap before we build error minimizing algorithm.<br>The derivative of a function at a point is</p><ul><li><strong>positive</strong> if the function is increasing in the neighborhood of the point.</li><li><strong>negative</strong> if it’s decreasing in the neighborhood of the point.</li><li><strong>zero</strong> if it’s neither increasing or decreasing.(saddle points)</li></ul><p>We can use this fact in our error function as shown below</p><div class="note info"><p>$$E(W+\epsilon) \approx E(W) + \epsilon E’(W) \hspace{1cm}\text{for a small enough}\hspace{0.2cm} \epsilon$$<br>$$\implies E(W+\epsilon) &lt; E(W)\hspace{1cm} if \hspace{0.2cm}E’(W) &lt; 0 $$<br>$$\implies E(W-\epsilon) &lt; E(W)\hspace{1cm} if \hspace{0.2cm}E’(W) &gt; 0 $$</p></div><p>So, all we have to do now is decrease the weights when derivate is positive and vice versa. If we repeat this operation with a small enough $\epsilon$ we can minimize the error function. This is the whole idea of <strong>Gradient Descent</strong>. A simple form for the algorithm is</p><div class="note info"><p>until we reach an error threshold or $E’(W) = 0$<br>$$W ← W - \epsilon sgn(E’(W))<br>\hspace{1cm}where\hspace{0.2cm}sgn(x) :=<br>\begin{cases}<br>-1 &amp; {x &lt; 0,} \\<br>0 &amp; {x = 0,} \\<br>1 &amp; {x &gt; 0.}<br>\end{cases}<br>$$</p></div><h2 id="Choosing-an-error-function"><a href="#Choosing-an-error-function" class="headerlink" title="Choosing an error function"></a>Choosing an error function</h2><p>The simple yet powerful algorithm depends on the error function to work. So, let’s explore how to choose an error function. In the <a href="/neuron-and-the-perceptron-algorithm/">perceptron post</a> we chose the number of misclassified points as our error function. But, <code># misclassified points</code> can only have integer values and is a discrete function. This is a poor choice for an error function, because</p><ul><li>In many situations you’ll find that the number of misclassified points can’t be decreased. The weights keep bouncing between a set of values. </li><li>Differentiating a discrete function.</li><li>All the misclassified points receive the same treatment. Points misclassified by a small margin aren’t any different from large error margins.</li></ul><p>To overcome these challenges we can emphasize on the following characteristics</p><ul><li>The error function must be <em><strong>continuous and differentiable</strong></em>.</li><li>Penalize misclassified points based on their error margins.</li></ul><p>Designing such function is for another post. Let’s see some limitations of gradient descent before moving to the next post.</p><h2 id="The-limitations-gradient-descent"><a href="#The-limitations-gradient-descent" class="headerlink" title="The limitations gradient descent"></a>The limitations gradient descent</h2><p>As long as $E’(W)$ is not zero, our weights keep updating and we’ll be approaching a minima in the error function. But what about the case when it’s zero, our algorithm terminates. But we don’t have to be at a minima when this happens. Points where $E’(W) = 0$ are called <strong>critical points</strong>. Three different scenarios for critical points are</p><ul><li><p><strong>Minima:</strong> Moving in any direction doesn’t decrease the function. <img src="https://www.dropbox.com/s/ekpujat5y46ey9z/minima.png?raw=1" alt="Minima figure"></p></li><li><p><strong>Maxima:</strong> Moving in any direction doesn’t increase the function. <img src="https://www.dropbox.com/s/iwq9jj1htsbak19/maxima.png?raw=1" alt="Maxima figure"></p></li><li><p><strong>Saddle Point:</strong> Moving in any direction doesn’t increase or decrease the function. <img src="https://www.dropbox.com/s/03wdjpjsih8tc2f/saddle.png?raw=1" alt="Saddle point figure"></p></li></ul><p>So our algorithm can stop at any of these situations. Even if we land on a minima it may not be the absolute lowest that is possible. When there are many local minima, we find ourselves stuck at the local minima. Different scenarios that can occur are shown in the figure below.<br><img src="https://www.dropbox.com/s/xapmmirt8xgmyt0/approximate_minimization.png?raw=1" alt="Approximate Minimization"></p><ol><li>Ideally, we would like to achieve this point (Global Minima), but it may not be possible.</li><li>This is at par with the global minima and is acceptable.</li><li>These points lead to poor performance.</li></ol><p>We’ll discuss tricks and tips to mitigate these limitations in another post. Here’s a little treat for making it till the end.</p><p><img src="https://imgs.xkcd.com/comics/listening.png" alt="Alexa joke"></p>]]></content>
      
      <categories>
          
          <category> AI </category>
          
          <category> Deep Learning Fundamentals </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Neural Networks </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Multilayer Perceptrons and Activation function</title>
      <link href="/multilayer-perceptrons-and-activation-function/"/>
      <url>/multilayer-perceptrons-and-activation-function/</url>
      <content type="html"><![CDATA[<h2 id="The-XOR-problem"><a href="#The-XOR-problem" class="headerlink" title="The XOR problem"></a>The XOR problem</h2><p>In the previous <a href="/deep-learning-fundamentals-neuron-and-the-perceptron-algorithm">post</a> you’ve seen how a perceptron works. Now let’s dive into some more interesting problems in deep learning. What follows is the classic XOR problem. Develop a method for the correct classification of the following points.</p><a id="more"></a><p><img src="https://www.dropbox.com/s/rks7mlcp2vubj3c/XOR_problem.png?raw=1" alt="XOR PROBLEM"></p><h2 id="Multi-Layer-Perceptron"><a href="#Multi-Layer-Perceptron" class="headerlink" title="Multi Layer Perceptron"></a>Multi Layer Perceptron</h2><p>It is clear that these points are linear inseparable. So, a perceptron fails at classifying these points. To classify these points it is clear that we need a non-linear boundary. If one perceptron can’t do the job, let’s try combining many of them. To keep it simple and the weights manageable, let’s use the architecture below.<br>(Note: Every layer stacked between the input and output layer is a hidden layer. We’ll see more about it later in this post.)</p><p><img src="https://www.dropbox.com/s/4ulft6rx1jnsrc7/mlp_without_activation.png?raw=1" alt="MLP without activation"></p><p>But this doesn’t work. Because, the whole output of the last unit is a linear function in $x$ and $y$. We can write the output of $h_3$ as<br>$h_3 = Ax + By + C$  where<br>$A = aw_{11} + bw_{12}$<br>$B = aw_{12} + bw_{22}$<br>$C = aw_{01} + bw_{02} + c$</p><h2 id="Activation-Function"><a href="#Activation-Function" class="headerlink" title="Activation Function"></a>Activation Function</h2><p>This is where an activation function comes to play. An activation function is a non-linear function applied to the end result of a perceptron. This introduces non-linearity to the perceptron and to the network.</p><p>Some common activation functions are</p><h3 id="ReLU-Rectified-Linear-Unit"><a href="#ReLU-Rectified-Linear-Unit" class="headerlink" title="ReLU (Rectified Linear Unit)"></a>ReLU (Rectified Linear Unit)</h3><p><strong><br>$$y = max(0, x)$$</strong><br><img src="https://www.dropbox.com/s/7evkd5143dsk8tu/relu.png?raw=1" alt="ReLU activation graph"></p><h3 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h3><p><strong><br>$$y =\frac{1}{1 + e^{-x}}$$</strong><br><img src="https://www.dropbox.com/s/wacrmulmj3b090i/sigmoid.png?raw=1" alt="Sigmoid activation graph"></p><h3 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h3><p><strong><br>$$y = \frac{ e^x - e^{-x}}{ e^x + e^{-x} }$$</strong><br><img src="https://www.dropbox.com/s/n8wm85xjqwav69t/tanh.png?raw=1" alt="Tanh activation graph"></p><p>Ok, so let’s apply “ReLU” to a perceptron and see what the response is. You can fiddle around with ReLU and become more familiar with it’s response <a href="https://www.desmos.com/calculator/ap5gwdoznu" target="_blank" rel="noopener">here</a></p><h2 id="Multi-Layer-Perceptron-Activated"><a href="#Multi-Layer-Perceptron-Activated" class="headerlink" title="Multi Layer Perceptron Activated"></a>Multi Layer Perceptron Activated</h2><p>Now let’s try activating the perceptrons in our network with relu. Our previous network will now be<br><img src="https://www.dropbox.com/s/n7m305d5noa5205/activation_and_mlp.png?raw=1" alt="MLP with activated perceptrons"></p><p><strong>Note:</strong> The final unit isn’t activated by relu, because we only need it to do binary classification. So we use a <a href="https://en.wikipedia.org/wiki/Heaviside_step_function" target="_blank" rel="noopener">Heaviside step function</a> instead. The weights $w_{01}, w_{02}, c$ are biases and are added implicitly and are not shown in the graph.</p><p>Now let’s fiddle around with the parameters we mentioned in the formulas above. Try to come up with a set of weights ($w_{01}, w_{11}, w_{21}, w_{02}, w_{12}, w_{22}, a, b, c$), so that you create a region that sepearates these points. If the graph below is too small you can work on it <a href="https://www.desmos.com/calculator/ggs1zd0ogl" target="_blank" rel="noopener">here</a>.</p><iframe src="https://www.desmos.com/calculator/ggs1zd0ogl" width="100%" height="600px" frameborder="0" allowfullscreen></iframe><h2 id="MLP-Terminology"><a href="#MLP-Terminology" class="headerlink" title="MLP Terminology"></a>MLP Terminology</h2><p>Hope you got an intuitive sense of how it works. Before we go further, let’s clear up some terminology. An MLP is a class of feed-forward neural network which we’ll know more about in a later post. In a feed-forward network</p><ul><li><p><strong>Hidden Layer</strong>: Any layer that doesn’t directly provide input or output is a hidden layer. The outputs and inputs of these layers are not directly visible or not a point of interest. Thus the name, <em>hidden layer</em>.</p></li><li><p><strong>Depth of a neural network</strong>: The depth of a neural network is the number of hidden layers plus one. Plus one because output weights can be modified. So by increasing the number of hidden layers, you’re increasing the depth of a neural network.</p></li><li><p><strong>Width of a hidden layer</strong>: The number of units in a hidden layer is also known as the width of the layer.</p></li></ul><p><img src="https://www.dropbox.com/s/q0c8n47q6xzb9hk/nn_depth_width.png?raw=1" alt="Neural network - Depth and Width"></p><h2 id="The-search-for-parameters"><a href="#The-search-for-parameters" class="headerlink" title="The search for parameters"></a>The search for parameters</h2><p>In the example above you came up with the set of weights that classify the XOR points. But, it’s not practical to search for a set of a weights, especially when we stack many more perceptrons. To understand why a naive search for a best set of weights doesn’t work, let’s consider a simple scenario. Let’s say we have a fully connected network(every node in a layer is connected to every node in the next layer.) and we have 3 hidden layers each with a 100 nodes. Input nodes are 2 and the output is a single node. So the number of parameters will be</p><ul><li><strong>between input and hidden layer 1:</strong> 2 x 100</li><li><strong>between hidden layer 1 and hidden layer 2:</strong> 100 x 100</li><li><strong>between hidden layer 2 and hidden layer 3:</strong> 100 x 100</li><li><strong>between hidden layer 3 and output:</strong> 100 x 1</li></ul><p>So, we end up with a total of 200 + 10000 + 10000 + 100 = <strong>20,300</strong> parameters. But those are just the number of parameters. Each parameter can have any arbitrary weight. But for the sake of a concrete example, let’s say we bound the weights between -100 and 100 with a step size of 0.01. So the number of possible values for each parameter is $(100 - (-100))/0.01$ = $20,000$. To find an ideal set of weights through naive search, you must search for one instance in 20,300 * 20,000 = 406,000,000. Searching naively through 406 million instances for a simple network is inefficient and unnecessary. This is where <strong>Stochastic Gradient Descent</strong> can make the search faster towards the goal. We’ll learn all about it in the next post.</p><p>Don’t forget your dopamine shot for making it till the end.<br><img src="https://imgs.xkcd.com/comics/computers_vs_humans.png" alt="xkcd - too cool to care"></p>]]></content>
      
      <categories>
          
          <category> AI </category>
          
          <category> Deep Learning Fundamentals </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Neural Networks </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Neuron and the Perceptron Algorithm</title>
      <link href="/neuron-and-the-perceptron-algorithm/"/>
      <url>/neuron-and-the-perceptron-algorithm/</url>
      <content type="html"><![CDATA[<p>This is the first part of the series <em>“Deep Learning Fundamentals”</em>. The goal of this series is to explore the mechanisms of artificial neural networks. The focus is more on presenting an intuitive way of understanding neural networks. So, you can expect an emphasis on how and why things work rather than what does the job. More often than not I’ll try to use simple math without focusing on notation. Let’s jump into the fundamental unit of most of the neural networks - “a neuron”.<br><a id="more"></a></p><h2 id="Neuron"><a href="#Neuron" class="headerlink" title="Neuron"></a>Neuron</h2><p>A neuron is nothing but a binary classifier. Which means given an n-dimensional space, it can divide the space into two regions. Let’s try to come up with an algorithm that does this, given that the data is linearly separable. For the sake of brevity, let’s consider that the data we deal with is numerical across all dimensions. (Dealing with other kinds of data is for another post.)</p><p>Without loss of generality let’s start working with two-dimensional data. So, let’s say there are some green(0) and blue(1) points on a plane and we want a line that separates these points. An intuitive way would be to draw a line like shown in the graph below. So far so good. But what we need finally is an algorithm that does this for us all this by itself and adjusts to any new points.<br><iframe src="https://www.desmos.com/calculator/smmkhlshap?embed" width="100%" height="300px" frameborder="0" allowfullscreen></iframe></p><h2 id="The-Search-for-a-Classifier"><a href="#The-Search-for-a-Classifier" class="headerlink" title="The Search for a Classifier"></a>The Search for a Classifier</h2><p>Let’s start with a random line in this space and then see what we must do, for creating the two regions. Let’s try it ourselves first. One way would be to rotate and translate the line until it separates the two regions. This seems intuitive enough. Play around with the interactive graph below to separate the two groups of points. Make sure the points are in their respective colored regions.</p><iframe src="https://www.desmos.com/calculator/cadms8e21r" width="100%" height="600px" frameborder="0" allowfullscreen></iframe><p>You would’ve noticed that $a$ and $b$ contribute to the rotation and $c$ contributes to translation. This makes sense because $a$ and $b$ control the slope and $c$ controls the distance from the origin. But how did you know when to stop fiddling around with those parameters? A trivial but useful answer would be to stop when there are no misclassified points.</p><p>At this point, we are good to develop a general algorithm for classifying two groups of points. We would do something like this</p><div class="note info"><pre><code>Initialize random line parameterswhile misclassified points are present:    fiddle around with the line parameters</code></pre></div><h2 id="The-perceptron-trick"><a href="#The-perceptron-trick" class="headerlink" title="The perceptron trick"></a>The perceptron trick</h2><p>Let’s start making the algorithm a little more concrete. Let the line be <strong>$ax_1 + bx_2 + c = 0$</strong>, where we initialize <strong>$a$</strong>, <strong>$b$</strong>, <strong>$c$</strong> at random. For classifying the points, let’s first give the points some labels(0 and 1) to differentiate them. Let $P(x_1, x_2)$ be a point in our data space with a label <strong>$L_P$</strong>. If <strong>$(ax_1+ bx_2 + c &gt; 0) = L_P$</strong> then <strong>$P$</strong>‘s classification is correct.</p><p>Now let’s deal with the fiddling part of the algorithm. Before we see any math on this, let’s try to build our intuition for it. Below are two cases </p><ul><li>case 1: <strong>$0$</strong> label point is misclassified.</li><li>case 2: <strong>$1$</strong> label point is misclassified.</li></ul><p>See if you can come up with a general rule for modifying <strong>$a$</strong>, <strong>$b$</strong> and <strong>$c$</strong> that leads to the correct classification of these points.</p><iframe src="https://www.desmos.com/calculator/684cghhob5" width="100%" height="600px" frameborder="0" allowfullscreen></iframe><iframe src="https://www.desmos.com/calculator/nxtk6rxo4z" width="100%" height="600px" frameborder="0" allowfullscreen></iframe><p>With a bit of effort, you can observe that the following pattern works for any situation</p><ul><li>increase parameters for a point labeled <strong>$1$</strong> and classified as <strong>$0$</strong>.</li><li>decrease parameters for a point labeled <strong>$0$</strong> and classified as <strong>$1$</strong>.</li></ul><p>Don’t worry if you haven’t found this out. Try it now and see if it works. The reason you might have missed the pattern is that you were trying to</p><ul><li>increase one parameter and decrease another.</li><li>increase or decrease the parameters by a random amount until it does the job.</li></ul><p>There’s nothing wrong with the above two operations because they do get the job done. But the problem is that we can’t generalize those operations for any given scenario. So, by this point, you should have a decent amount of intuition on how all this works. So, here’s how you can update the parameters. The amount by which they have to be updated is directly proportional to <strong>$x_1$</strong> for <strong>$a$</strong>, <strong>$x_2$</strong> for <strong>$b$</strong>. This translates to the following formula which works for both kinds of misclassification.<br><div class="note info"><p>For every misclassified point <strong>$P(x_1, x_2)$</strong>:<br><strong>$\hspace{1cm}a = a + \alpha (expected - predicted) x_1$</strong><br><strong>$\hspace{1cm}b = b + \alpha (expected - predicted) x_2$</strong><br><strong>$\hspace{1cm}c = c + \alpha (expected - predicted)$</strong></p></div></p><p>Here <strong>$\alpha$</strong> is the proportionality constant and is like a fine-tuning knob. It controls the rate at which we change these parameters, also known as the <em>“learning rate”</em>. We need our perceptron to change the parameters in a slow manner. Because large changes often tend to misclassify points that were correctly classified before. There aren’t any fixed good values for the learning rate. See for yourself how learning rate affects the speed and performance <a href="https://www.cs.utexas.edu/~teammco/misc/perceptron/" target="_blank" rel="noopener">here</a>.</p><p>In a Deep Learning setting the parameters $a$, $b$ are usually denoted by $w_1$, $w_2$ and are part of a vector $w$. $c$ is known as bias. The inputs and outputs of a neuron are as shown in the figure below.</p><p>$$z = w_0 + w_1x_1 + w_2x_2 + … + w_mx_m$$</p><p>$$<br>H(z) =<br>\begin{cases}<br>0  &amp; {n &lt; 0} \\<br>1 &amp; {n \geq 0}<br>\end{cases}<br>$$<br><img src="https://www.dropbox.com/s/j6vksnlibf0yhgq/perceptron.png?raw=1" alt="Perceptron"></p><h2 id="The-Perceptron-Algorithm"><a href="#The-Perceptron-Algorithm" class="headerlink" title="The Perceptron Algorithm"></a>The Perceptron Algorithm</h2><p>To wrap up this section, here’s the formal definition in a deep learning setting.</p><div class="note info"><p><strong>Data</strong>: Training Data:<strong>$(x_i , y_i )$</strong>; <strong>$\forall i \in {0, 1, 2, . . . , N }$</strong>, Learning Rate: <strong>$\eta$</strong>, where</p><ul><li>$x_i$ is a m-dimensional input vector and $N$ is the total number of instances of our data.</li><li>$x_{i, 0} = 1;$ $\forall i \in {0, 1, 2, . . . , N }$</li><li>${\hat y}_i$ is the prediction of a point <strong>$(x_i , y_i )$</strong></li></ul><p><strong>Result</strong>: Separating Hyper-plane coefficients :<strong>$w^∗$</strong><br><strong>Initialize</strong> <strong>$w$</strong> ← random weights ; (Since $x_{i, 0} = 1, w_0$ acts as the bias without setting it explicitly.)<br><strong>repeat</strong></p><p>get example <strong>$(x_i , y_i )$</strong>;<br><strong>$\hspace{1cm}\hat y_i ← w^T x_i $</strong>;<br><strong>$\hspace{1cm}w ← w + \eta(y_i −  {\hat y}_i )x_i$</strong></p><p><strong>until</strong> convergence;</p></div><p>Here’s your dopamine shot for making it till the end.</p><p><img src="https://imgs.xkcd.com/comics/machine_learning.png" alt="XKCD comic"></p>]]></content>
      
      <categories>
          
          <category> AI </category>
          
          <category> Deep Learning Fundamentals </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Neural Networks </tag>
            
        </tags>
      
    </entry>
    
  
  
    
    <entry>
      <title>Hello World</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
    <entry>
      <title>Categories</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
    <entry>
      <title>Portfolio</title>
      <link href="/portfolio/index.html"/>
      <url>/portfolio/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
    <entry>
      <title>links</title>
      <link href="/resources/index.html"/>
      <url>/resources/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
    <entry>
      <title>Topics</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
    <entry>
      <title>timeline</title>
      <link href="/timeline/index.html"/>
      <url>/timeline/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
  
</search>
