<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>RMSProp</title>
      <link href="/rmsprop/"/>
      <url>/rmsprop/</url>
      <content type="html"><![CDATA[<h2 id="Exponentially-Decaying-average"><a href="#Exponentially-Decaying-average" class="headerlink" title="Exponentially Decaying average"></a>Exponentially Decaying average</h2><p>The idea of RMSProp is similar to what you saw in the AdaDelta’s first step. It maintains a exponentially decaying average of the squared gradients.<br><a id="more"></a><br>$$<br>\text{let} \frac{\partial E}{\partial W} = g, \text{ At the step t of training}\\<br>\underbrace{E[g^2]_t}_{\text{average of squared gradient}} = \gamma E[g^2]_{t-1} + (1-\gamma){g_t}^2\\<br>W \leftarrow W - \dfrac{\eta}{\sqrt{\epsilon + E[g^2]_t}}g_t<br>$$</p><p>A good suggested value for $\gamma$ is 0.9, and for $\eta$ is 0.001. $\epsilon$ has the same job as in AdaGrad.</p><p>In the next post let’s see “Adam” (Adaptive Moments), which improvises on RMSProp and momentum. It is considered a default in many DeepLearning settings. Enjoy the end of post comic.</p><p><img src="https://imgs.xkcd.com/comics/turing_test.png" alt="XKCD Comic"></p>]]></content>
      
      <categories>
          
          <category> AI </category>
          
          <category> Deep Learning Fundamentals </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Neural Networks </tag>
            
            <tag> Momentum Mini Series </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>AdaDelta</title>
      <link href="/adadelta/"/>
      <url>/adadelta/</url>
      <content type="html"><![CDATA[<h2 id="Average-over-a-window"><a href="#Average-over-a-window" class="headerlink" title="Average over a window"></a>Average over a window</h2><p><strong>AdaDelta</strong> corrects the decreasing gradient problem caused by AdaGrad. It does this with a simple idea.<a id="more"></a> Instead of summing over all the squared gradients so far, only sum over the last $w$ squared gradients and take the mean of it. Since storing all the previous $w$ squared gradients is inefficient, just take the exponentially decaying average of the squared gradients. The equations would be as follows</p><p>$$<br>\text{let} \frac{\partial E}{\partial W} = g, \text{ At the step t of training}\\<br>\underbrace{E[g^2]_t}_{\text{average of squared gradient}} = \gamma E[g^2]_{t-1} + (1-\gamma){g_t}^2\\<br>$$<br>But we don’t need the squared gradient, only the gradient. So apply square root. Which gives you nothing but the Root Mean Squared (RMS)} result.<br>$$<br>RMS[g]_t = \sqrt{E[g^2]_t + \epsilon}\\<br>\text{Here} \epsilon \text{ does the same job as in AdaGrad. The update in the step t is}\\<br>update_t = \frac{-\eta}{RMS[g]_t}g_t<br>$$</p><h2 id="Correct-for-units"><a href="#Correct-for-units" class="headerlink" title="Correct for units"></a>Correct for units</h2><p>When applying the update to the weights, the updates should be of the same units as of the weights. The units here are hypothetical. So, the following idea is based on the assumption that if the weights had a unit, the update should have the same. Maintaining the integrity of the units is not done in SGD, momentum or AdaGrad. AdaDelta corrects this by adding another exponentially decaying average of $\Delta W$and taking the RMS of it. (As to why this works, you need to understand Newton’s method. If you do, then read this <a href="https://arxiv.org/pdf/1212.5701.pdf#subsection.3.2" target="_blank" rel="noopener">section</a> in the original paper. Else, move on without any worries.)</p><p>$$<br>E[\Delta W^2]_t = \gamma E[\Delta W^2]_{t-1} + (1 - \gamma) \Delta W^2 \\<br>RMS[\Delta W]_t = \sqrt{E[\Delta W^2]_t + \epsilon}<br>$$</p><p>As we don’t know $RMS[\Delta W]_t$ at the time step $t$, approximate it with the RMS of the weight updates till the last step, which is $RMS[\Delta W]_{t-1}$. So our final AdaDelta update is obtained by replacing the learning rate with the $RMS[\Delta W]_{t-1}$.</p><p>$$<br>\Delta W = -\frac{RMS[\Delta W]_{t-1}}{RMS[g]_t}g_t\\<br>W \leftarrow W + \Delta W<br>$$</p><h2 id="Advantages-over-Momentum-Nesterov-and-AdaGrad"><a href="#Advantages-over-Momentum-Nesterov-and-AdaGrad" class="headerlink" title="Advantages over Momentum, Nesterov and AdaGrad"></a>Advantages over Momentum, Nesterov and AdaGrad</h2><p>Notice that there is no need to set a learning parameter in the AdaDelta method. This is the main advantage over all the others we’ve seen so far. Let’s see a similar algorithm (RMSProp) which tries to correct the gradient explosion problem of AdaGrad. Enjoy the end of the post comic.</p><p><img src="https://imgs.xkcd.com/comics/magnus.png" alt="XKCD Comic"></p>]]></content>
      
      <categories>
          
          <category> AI </category>
          
          <category> Deep Learning Fundamentals </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Neural Networks </tag>
            
            <tag> Momentum Mini Series </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>AdaGrad</title>
      <link href="/adagrad/"/>
      <url>/adagrad/</url>
      <content type="html"><![CDATA[<h2 id="Recognizing-Important-Features"><a href="#Recognizing-Important-Features" class="headerlink" title="Recognizing Important Features"></a>Recognizing Important Features</h2><p>The idea of <strong>AdaGrad</strong> is a simple one. Normalize the gradient updates by the sum of the gradients so far. Suppose we run the training for t steps then the weight update for the t+1 step is as follows<br><a id="more"></a><br>$$<br>W_i^{(t+1)} \leftarrow W_i^t - \frac{\alpha}{\sqrt{\epsilon + \sum_{\tau = 1}^{t}\frac{\partial E}{\partial W_i^{t}}}}\frac{\partial E}{\partial W_i^{t}}<br>$$</p><p>This has a very nice consequence. Weights that have high gradients frequently get a reduced update, due to the sum of all the updates so far in the denominator. Weights that have a small gradient or ones that receive infrequent updates are given more preference, due to the small sum in the denominator. $\epsilon$ is just a safeguard to prevent dividing by zero. Its value is in the order of $10^{-8}$.</p><h2 id="Potential-Drawbacks"><a href="#Potential-Drawbacks" class="headerlink" title="Potential Drawbacks"></a>Potential Drawbacks</h2><p>Adagrad helps in tuning the learning rate and provides more weight to rare features. But this comes at a cost of learning rate becoming too small due to the sum of squared gradients in the denominator. In its training cycle, Adagrad reaches a point where the updates aren’t relevant anymore and weights are stuck at a point.</p><p>Let’s see some more algorithms which try to cover this flaw in adagrad starting with “Adadelta”. Enjoy your end of the post comic.<br><img src="" alt="XKCD Comic"></p>]]></content>
      
      <categories>
          
          <category> AI </category>
          
          <category> Deep Learning Fundamentals </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Neural Networks </tag>
            
            <tag> Momentum Mini Series </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Nesterov Accelerated Gradient</title>
      <link href="/nesterov-accelerated-gradient/"/>
      <url>/nesterov-accelerated-gradient/</url>
      <content type="html"><![CDATA[<h2 id="Correction-Factor"><a href="#Correction-Factor" class="headerlink" title="Correction Factor"></a>Correction Factor</h2><p>While using momentum, we updated $V$ and then applied part of $V$ to the weight update. The idea of Nesterov Accelerated Gradient(NAG) is to update the current weight by a small part of $V$ and then calculate the gradient of the updated weight. The small part by which we update the current weight is called the <strong>correction factor</strong> which is $\alpha V$. Simply put, we changed the position where the gradient is calculated. So for each iteration in gradient descent, the updates would be as follows<br><a id="more"></a><br>$$<br>W \leftarrow W + \alpha V\\<br>V \leftarrow \alpha V - \epsilon \frac{\partial E}{\partial W}\\<br>W \leftarrow W + V<br>$$</p><p>Here $\alpha, \epsilon$ function the same way as $\beta$ in the standard momentum algorithm.</p><h2 id="Comparison-with-momentum"><a href="#Comparison-with-momentum" class="headerlink" title="Comparison with momentum"></a>Comparison with momentum</h2><p>The illustrations below show the difference between momentum and Nesterov Accelerated Gradient.</p><p><img src="https://lh3.googleusercontent.com/HJi5B-iOdVPwLvRf6jivUTP8KKDxqydCejwt38teS-jkrgkhmeeBJo0EOOD6-b4xqzARO8HvfaXAxTM0I-G68rJzIO5YVLYoLKn4nCngP21x_zTYLk4sKN1RZmF2NLIKOytdjyWcyPoxXaTUlpSRRbJcTYO_XRe7wzlliP8IfPRwVd_yYll7bt-J-0X6m9NgHTnXFkkbegYaZTMKF99CsKpxukweDRWlkPEsd8UYUQUygOpAgGuJ2GDVN_8HgJKIpAFGL-xDIZ40PI2LXlKHMVhIhH3RRaKihmC4pE71vZ6b9xcpfWOgM8nkt4QyUz0Lc5-70Ih_j8k_iPk-1QUKzqrQHZ7O148ERbZwKr1FtRuszxBjZC7eCFMWHlttxs13HMHuIe9WF_KDoC6KKBYsucuRO4IOZQfRDgL7iBPiI828zmr0Gfa8a00s0tDWDW1nM4sFFXhFonzr4t6uwSxEoPUgrzX-vUY4w44o6KO1E8zxs-NSnaPy-lVRpdzoHb6CnbZOmJE2StUIlEB4cSkbwGLc2o94LzUONNw9xIV3dgcpFxeKeU93uKb4nbtEUzW7Bh5FUAF1EajsQzx3SrTgCrerrhiYCRHollJfDIo=w1123-h612-no" alt="momentum illustration"></p><p><img src="https://lh3.googleusercontent.com/nd6jy-hBWI_bb3TG-TvQRXQ195xnxZdYFP8nr_bnCkFgR16v2ivxIOma8ztFjDj5XfXVLCYmF0AIri2zN0n0nnyq9kt_SuFKpknCtVlqsPgo_jTMjFy-qWSJNO6GxvzEtJ6c4sK_y6Uo9BnR-dMPGex3nbslAufu1piiC9lOcyfyFixKy8jAzsHh1aFNhPC8_YnOiEnPFJEyNbjQTww2QU79CNOnz7y_RjlGJoYNaKDhu6QSWADVwxfunfK7p9OAJpQ0Cp-3r7ga0y87kvEzajGbTRfyYS-tTUmc0PoPTRQHaXZLNBnfDSIAZlPqPnFX8rXshDUWPDVl41jjoeT72ORA1874wzwMLIzg2V4W5fRXHL2HlFdSCf2VmtPXVameRlAjNndPs8RZUVNadK5TKfQdXEqjmDaY_qr90N56vLI2tBZhShGBrPXkE1ofMvxHiiTG0XvB37u3ECpySWYB3LjXkzPSzy5y8FEG91ORIlv4FtWXWB_swpAT7O2ywZKoe9gv4JhFzo1xs0FWk88CB6yHBRNECWdVhgu5qUyHskc4lPRLz29b0da_j1enaXJo8ppjdNndfgwrkRTz_Bii7q2Gk9T1_xi8Okskzqo=w1123-h567-no" alt="Nesterov Accelerated Gradient illustration"></p><p>Though this seems like a trivial change, it usually makes the velocity change in a quicker and more responsive way. This leads to better stability(not many fluctuations) than momentum and works better with a high $\alpha$ value.</p><p>In the next post let’s see AdaGrad. Enjoy the end of the post comic.<br><img src="https://imgs.xkcd.com/comics/einstein.png" alt="XKCD Comic"></p>]]></content>
      
      <categories>
          
          <category> AI </category>
          
          <category> Deep Learning Fundamentals </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Neural Networks </tag>
            
            <tag> Momentum Mini Series </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Momentum</title>
      <link href="/momentum/"/>
      <url>/momentum/</url>
      <content type="html"><![CDATA[<p>Gradient Descent promises us to take a step closer to a minima (local or global) on each iteration. But due to the high dimensionality, it’s dealing with a very complicated error surface. This introduces a problem of very slow convergence towards the minima. Let’s see a simple yet effective technique to overcome this problem.<br><a id="more"></a></p><h2 id="The-problem-with-Gradient-Descent"><a href="#The-problem-with-Gradient-Descent" class="headerlink" title="The problem with Gradient Descent"></a>The problem with Gradient Descent</h2><p>Updates of gradient descent around ravines are similar to the ones shown in the figure below.<br><img src="https://lh3.googleusercontent.com/tMSmvzo6VXoBnLFAxcFJEKKT5B9Nk7hIACXZ-zV1kP6vr9plWmBcWTaZjmCY2W4_p_jSscWqmqADHVBG9df3jULZrhRFEo3NHIIJN23PmDSzAMEkffe2Fi5tozz0TDyP9IJqXgN3OZulGRwlNol3m8UBIrRQ_I96WSKiriibjHoTzHHz_khuSerQNaxDPDlI8pZA_JIgopDPhkZCMfKBCmHLIyc0P2enG77_N7tVo0Rp6BirOlLbiWC8706DLddBpoxSs2JHwvzN59jZNFTqcbm0RXloVA6-4-UU-RRFI-YiBJ20jOci0StMztPQBWgKpWibmTZNzyXHm9HeEHxNwHS12GcTNr-VGEYRLv6o1bgKv-wIetIoWdfYwWvbe997lkPIz6cxyGQaiCiktdLaqIr2jbqX82HyfByVNSO0tdtwFcMyi5ajcyuW-Kw9kW1-CcRwAvTI7LTq_XTUZJUFGOUPMu-fuy_ZdkbeQ3ZmW0TYvSDzEOATtJkcf7Xwi3hDtYnRyhIy2RoHvaAE2sPAHlIWvWVCkYOGfmHgYCFM7ScoD52XRwDikvLoOYQuc6wXeB6_qJ-vey3LUTVSf6OPZt2Xu_2YEVmS_e-o1Wo=w1005-h710-no" alt="Gradient-descent updates around ravines"></p><p>The error oscillates so much in an unwanted direction and makes little progress towards the minima. So to overcome these problems, all we’ve to do is decrease the motion in the unwanted direction and increase the motion towards the minima.</p><h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2><p>To do this, we can use the updates that are done so far. Think of each update as a vector. Let’s see the result of adding the previous updates and the update in the current stage.</p><p><img src="https://lh3.googleusercontent.com/DdTsCRoSLrnXAc1nvk5YedI0tWHxr9eVtuwARUfXBb1WdTnTLjZnswaUeE6GufQczTOcEA7yyaP-4FXzLHMFv_lCeA8fSNZMLDr_9JyEj2nIBzk5NrE30yHSIOc1QErj_w1dNh_FHAj2vbppbI8YOBbuKr3AmQ2vZNp6Ze0DvMlVIxTKFTG-hokRMfrSFRsJZGtOiU92i_J46eHZB3MhT9z9oU_oomi7biU82CFtrHJGcdqoaRivxs2uOJRx-CdNuVmwIcZo6o5JkT15vnZYIh3QChLSTl8AxPjKbba_PXtx29XmpOSV1pYGGXI9YkqYndJlX-_dm5qEWsR_Sqek9iFyOorTnxT2IQkHZgWM34klkb4lqRRnC6z0DfN7UlH7FkvBcmEYZIxFW0GAJ2fpLsWxBhOpAZb5DIOY7dy8rEtAOXiqcb72qRhkGPRwucjHkZncb7rHUNAQyse6ZwFU95afPpQRwzpW2Edr-fDdAaMzf28B52Dw-07Yzv5m8ajjfQwKzNQbsxxBToLeiFjqRYl_iR-jEPEjvUPyfM4Rw1YEmMzM3cAdJLmmbpc5W45_T8edzNQHg-6AjX3o_0_jzchmVJvj1JlWmrHMDv0=w1005-h710-no" alt="Result of adding the updates so far"></p><p>The resulting vector after adding the previous updates, not only shortens the oscillation but is much better directed towards the minima. This is due to the oscillations canceling out each other in the unwanted direction and adding up in the direction towards the minima. This is the idea of <strong>Momentum</strong>.</p><h2 id="Updates-of-Gradient-Descent"><a href="#Updates-of-Gradient-Descent" class="headerlink" title="Updates of Gradient Descent"></a>Updates of Gradient Descent</h2><p>To apply this idea to our gradient descent equations, we need to be storing the sum of the gradients. Let’s call this variable $V$. We now need to update our weights with $V$ rather than $\frac{\partial E}{\partial W}$. Also, we need to be updating V on every iteration to store the sum of the updates so far. So the updated equations for our gradient descent will be</p><p>$$<br>w \leftarrow w - \alpha V \\<br>V \leftarrow \beta V + (1-\beta)\frac{\partial E}{\partial W}\\<br>$$</p><p>You might be thinking where did the $\beta$ come from? If you think about it, each update vector introduces an unnecessary error in the wrong direction. So it makes sense to take a small part of our current update to minimize the error it makes and add in a large part of the previous sum which is much better in the direction towards the minima. That is what $\beta$ is doing in the equation. It takes a small part of the large error and big part of the small error. The figure below illustrates this fact.</p><p><img src="https://lh3.googleusercontent.com/Nyc2t2aotsFBQhcgPFSssjnQWPlBhjdF2Le_sZK-W1UUEWTivo1T7S5JSiYK9ykuWc8SK27DWV-Ljqv-Jza2KPnLhvOPJZx8wxqUKcq2m7-8Xm1Sjg1ueHtLh7wc6-ouV0SE3a3CCfV832KE5ne87XT8nvjAkfzb6FgewMRCnVYCIjAW2ekUDEsBjwg-0QopxSL-yAZ8ozG_uVJdov-O6FQEOQCod_cp4b6TmFPrj7Cdb00AOPjVlUbQJocYE8f6PThQdmLdK4Foq5XsetzS2Uh2PsR4i2hJGUClvcU_DYXjnfmr_x_iKxt4u4vahINk6MeilR_XPJjfOWefwmTxKoq9BNn_7AZruZV8vLUZADNaoAwyE_IzCKQZ7sSes4jJwS7djyKt0RucTRUT7K1N590jqhMuxdwlNo6vG0npydpdqmnzSPQaRjWa00IovyK48xPG1ROhiHUj9XQ72zYQGXsAQUjRVJ6oUrK-tKbw7q9K8I3pATOsofN5DuFIwYTw73X4SHaQii8-pYjPwkLfH3uIy7eqTITETEobBTP-UA6uOP-1JGgDtu2fjbqlQnhzotde-GWcIpWl1-1t3QeFmI6WVf59F6djmjp-PLI=w1005-h710-no" alt="Momentum in work"></p><h2 id="More-advanced-updates"><a href="#More-advanced-updates" class="headerlink" title="More advanced updates"></a>More advanced updates</h2><p>The momentum update we’ve seen so far is a very simple one. We’ll be looking at many other more good ones in the mini-series on adaptive learning rate algorithms. In this mini-series, we’ll cover one algorithm at a time with an intuitive way of understanding each. Till then, review all our posts so far in the Deep Learning Fundamentals. Don’t forget your comic of this post.</p><p><img src="https://imgs.xkcd.com/comics/power_cord.png" alt="XKCD Comic"></p>]]></content>
      
      <categories>
          
          <category> AI </category>
          
          <category> Deep Learning Fundamentals </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Neural Networks </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Optimizing Gradient Descent</title>
      <link href="/optimizing-gradient-descent/"/>
      <url>/optimizing-gradient-descent/</url>
      <content type="html"><![CDATA[<p>The Gradient descent algorithm we saw in an <a href="/cost-function-and-gradient-descent">earlier post</a> just gives you an idea of how gradient descent works. We’ve seen that</p><p>$$W \leftarrow W - \alpha E’(W)\\<br>$$</p><p>This is just a rough formula to get you started. In practice what $E’(W)$ is calculating is the average of all the gradients due to every point in your training data. This is known as Vanilla Gradient Descent or Batch Gradient Descent. Large datasets with millions of instances are easily available nowadays. This presents a challenge to the vanilla gradient descent. The weights are updated after calculating the average of all the data points available, which significantly increases training time.</p><p>Let’s see some optimizations to keep these under control.<br><a id="more"></a></p><h2 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h2><p>The trivial solution to overcome this problem is that you update the weights for every instance. This is known as Stochastic Gradient Descent. This reduces our training time in terms of updating the weights. But in practice, this method approaches the minima rather slowly. This is due to the fact that each of the instances is biased and generally doesn’t point in the direction of the global minima.</p><h2 id="Mini-Batch-Gradient-Descent"><a href="#Mini-Batch-Gradient-Descent" class="headerlink" title="Mini Batch Gradient Descent"></a>Mini Batch Gradient Descent</h2><p>To gain the advantages of both of these techniques, we can achieve a trade-off by calculating the average of some instances. This is called the mini batch gradient descent. Mini Batch refers to the number of instances we consider to compute the gradient. In practice, it varies from 8 to 256, with more larger numbers rarely used. The illustration below summarizes all the variants of gradient descent we’ve seen so far.</p><p><img src="https://lh3.googleusercontent.com/0tkXnBNkBgG-OODx0_kY8MWHh2WVxEv4x1kyP5vhH3LK2EweOqvmaOIRtA5n78FZU3zSkibbbu4rrifi-7m59nxYvtXxtUQdEVNBYGgCQKajTocXAGPMAALJtqwGq0ZmlKKTvxilqpjcgeT5GbQQ8jl3w4bwZ80wurdHEKoUC3eKXEP_gPErktpMYM11vSSGR9bAkyJyPhEtYfHqS8g40JwYMnqL_HiUriSu-aF-6bSVQvZr2cw_SAkq6V18YbUitFt3FTdNysCxonyv3_EaJUuY5xRlVIjjDG1h7KPqleuL0h0W9oMviIYjcDVWodpFmxpu4WRVsmBtoF6oabPVHsnaoXoZ-XKkuyF0L59uiwtgyxvr0yGU7wW-QczLMUhuv1VGwCsjSJ0CQUi1TbrVkjA5y7o9RVX7O6mUo2_8NJ37-RymRXDcVtCZvzklrICXvN8sPQutjWgGgP_RUzczyGFZcZvXfhUlwuD1Mn_nnrwVLCij7k2JV5AdicdKyYyH7idKtpz4HyzzhL0SsEpVEsiAlEKsx58-2WwqmTrVO5mtWr6guqN9HeYJIifFU2pO_M_EUBR8y_cIrjspnc-nkz_LML3DTWPdB_oRMQ=w1005-h710-no" alt="Gradient descent variants"></p><h2 id="Adaptive-Learning-Rate"><a href="#Adaptive-Learning-Rate" class="headerlink" title="Adaptive Learning Rate"></a>Adaptive Learning Rate</h2><p>With gradient descent we can be sure that we’ll be better off on each of the iterations of training. But this does come at a cost. While during initial stages the loss can drop down dramatically and then the drop in loss becomes so small that you’ll be on the verge of giving up on your whole network. This may be due to the fact that there are many places where the gradient is stuck between valleys or the movement towards the minima is so small. Also there are surfaces which curve more steeply along one dimension and less along other. These are called ravines. Stochastic Gradient Descent oscillates along the slopes of the ravine and makes little progress towards the minimum.</p><p>To mitigate these problems to a certain extent, we give gradient descent some memory of its previous steps, so as to control the oscillations and take a more bold step towards the minima. This is the general idea of <strong>momentum</strong>. It can also adjust the learning rate according to the previous progress achieved by the gradient descent.</p><p>We’ll see more details about momentum and will start a mini-series on the gradient descent optimization algorithms. Till then soak all the things we’ve seen so far and enjoy the comic.</p><p><img src="https://imgs.xkcd.com/comics/drone_problems.png" alt="XKCD Comic"></p>]]></content>
      
      <categories>
          
          <category> AI </category>
          
          <category> Deep Learning Fundamentals </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Neural Networks </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Regularization</title>
      <link href="/regularization/"/>
      <url>/regularization/</url>
      <content type="html"><![CDATA[<p>In the last <a href="/overfitting-and-underfitting/">post</a> on overfitting and underfitting. We’ve seen that to achieve a perfect classifier is a tedious task. In this post let’s see some common tricks and methods to control the generalization of a network.<br><a id="more"></a></p><h2 id="The-cost-function-curve"><a href="#The-cost-function-curve" class="headerlink" title="The cost function curve"></a>The cost function curve</h2><p>In general, the overall trend of the training and testing errors is as shown below.</p><p><img src="https://lh3.googleusercontent.com/Yyz2VMAlD9QTbtyTf2oCYRHswEbAj7PTpyTrQVS1O07NKATrdObfDG_0rq_nB-J0suK2wp3JLWJJo37xJgWgzbtcgiTvFCri_OctaYM9bV4We544IfOcKyN3bjrMVrci_yTf3iibVRYO9-RcQtGF8Te9Y14oOZG7_Jhu670SUBZRhD9U3NC99G6EGMGzyutzpNI7YjdFrhntaXlgAbEDr7D0Ww7QgKItu5h-8VBtpdqkFhvNLboiEoOzmr0RpSjuhJOFHsg2AC6thHEJeZSrjZHEJk-bDrwfzkJCvVSdhVGqbabq3gmwAxB8vBrZpB_6nJNMC77ZqlKh4PIBxG1aB5vVBDCGohGbUqTfyHsPaRP-KMCrjX56xa8s9ZHA6R4dVwcDOBQubdbihFx1LF_5Akp01sFuGeMALioCMHpfOv5tKk8d300zbO1gHLTcc2U1uePhYWBNESxRtvz4_4GaoVA4DiGSg90JEsE8sZ6dizOLjnnKYfEkDTygBNdwhAVv1wO4cKzrLpCpn2MhAvJhd32J-XrpA6J42yAiZUtnyzfu3_fdLk6YGcN8yrHl6vGOlEFRXXAAdZm_IV-mmOmXuGwGbybc0OcaAtM7mQ=w1005-h710-no" alt="training and testing error curve"></p><p>This graph makes sense intuitively,</p><ul><li>In the early stages of training, the network is still trying to fit the training set. While doing this, it’s also gaining some generalization experience. This is reflected by the decrease in the test error. This is the region where the network is under-fit. It still has a lot to learn.</li><li>After a while, you achieve a good generalization. Both of your test and training errors hit a low point, which is good. This means that this is a very good generalization that you can achieve with the current network.</li><li>If we continue the training process, your network is going to over-fit to the training set. This is reflected by a steady increase in the generalization gap.</li></ul><h2 id="Early-Stopping"><a href="#Early-Stopping" class="headerlink" title="Early Stopping"></a>Early Stopping</h2><p>Observe the training and testing errors. You can see that the network enters the over-fitting region when the test error starts to rise. That’s when you know you’re losing the generalization capability of your network. So, your basic instinct will be to stop the training process as soon as the testing error starts to rise. This is known as <strong>Early Stopping</strong>.</p><p>In practice, it’s best not to stop the training immediately after the rise in testing error. There will be many fluctuations, but you can see the trend. So, it’s best to leave some headroom before you stop training.</p><h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>You know that the derivative received by each of the weights tells it by how much it should change for the final loss function to decrease. Notice that this is not an isolated operation. The calculation of these gradients happens in a way that it depends on many other neurons. So, a neuron may not only be just correcting for its own mistakes, but also for other neuron’s mistakes. This is a desired feature that gives rise to complex co-adaptions between the neurons. But it also contributes to over-fitting.</p><p>To avoid this to a certain extent, we can randomly choose some neurons in a hidden layer and shut them off. This is the idea of <strong>Dropout</strong>. So, we update our weights in such a way that some of these neurons never existed. The figure from the <a href="http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf" target="_blank" rel="noopener">original dropout paper</a> illustrates this well.</p><p><img src="https://lh3.googleusercontent.com/zpD5G45o1eSdLmZmBUWVw4T56DTAxFtsa3peuTEJ166ckman5hHqInqSRrkmRUF1hegWdSFZbnSvr0SYZBLjKaXOA3QlQViwH49egsX4jEDkNabk9Mw0gzK92Q4aYALfNdAvgJ2-FpBB_wSutxhI2_CsgT4bnyAIQB8tSVrUOvFb0tyxuwi_Bypf8qIu8ctN551cLVL8WO0XWY4ccdnQ6hfr__Gc_txwoEykAg5-oCeSa-9_kFkigvqKER4t5IFQ9y24TGWFuMKIB1DgynZVz8JfahgfkjbGaTPvC2E2e4C80aU5t4LN__efR-rU5h5pkjYv-Zo5nVL0AEq398-YSzDImcSH5h_SE_Elhr5HEZKOkPzqxF2MI6O5XCmM5HxE-8PZeYCJoZnMB1FDL9XS1JS8FpEsbg-j2fY6zHe-2Bf_9LDB_twfQ3w2LpzSPPLqKWCo4XZ0K37uRQ2byytFxzFwOcSesfWvx4L-B8yFbuUNYXMeaA-yabLw2Z3R465F0m66mQ3TEzMdbYmLFsX4rxdWAptPOTTfscifCcyVlKhboCu3-ac0GtRqmCWHimrsI1VW6WEyYliEw_UkbiC8AlVaBlmJJg-UdVMMbw=w716-h394-no" alt="Insert dropout illustration"></p><p>Notice that the dropout is applied only during training. It would make no sense to apply dropout during testing. You might as well test with a new neural network that reflects the old one after dropout. Each of the neurons is dropped-off with a probability that can be set by you. This is called the <strong>dropout rate</strong>. Selecting a good dropout strategy is for another post.</p><h2 id="L1-and-L2-Regularization"><a href="#L1-and-L2-Regularization" class="headerlink" title="L1 and L2 Regularization"></a>L1 and L2 Regularization</h2><p>Another prominent problem with the neural network training is the imbalance between weights. Some weights get too huge and undermine the effects of others. This contributes to overfitting. So the idea is to penalize these weights and keep them under control. Intuitively, we penalize them by adding a function of weights to our final loss. So, our new loss function will look something like</p><p>$$L = \text{cross entropy} + \lambda f(W)$$</p><p>Here $\lambda$ controls by how much we penalize the weights. If you choose a lambda that’s too high, then your model will underfit. The choice of $f(W)$ can be approached in two ways</p><ul><li>$f(W) = \sum_{j=0}^{L}|W_j|$ - This is known as <strong>L1 Regularization</strong> AKA <strong>Lasso Regression</strong>. While using this, we generally end up with sparse vectors. All the small weights tend to go to zero. This is good for feature selection, which we’ll discuss in a later post.</li><li>$f(W) = \sum_{j=0}^{L}W_{j}^{2}$ - This is known as <strong>L2 Regularization</strong> AKA <strong>Ridge Regression</strong>. This type of regression tries to maintain the weights homogeneously. Meaning all the weights are balanced well. This is normally better for training models.</li></ul><p>Let’s see a concrete example to see what maintaining the weights homogeneously means. With two sets of weights, $(1, 0)$ and $(0.5, 0.5)$ the error calculated by L1 and L2 are as follows</p><ul><li>L1 calculates the errors as follows $f((1, 0)) = |1| + |0| = 1$ and $f((0.5, 0.5)) = |0.5| + |0.5| = 1$</li><li>L2 calculates the errors as follows $f((1, 0)) = 1^2 + 0^2 = 1$ and $f((0.5, 0.5)) = (0.5)^2 + (0.5)^2 = 0.5$</li></ul><p>While L1 has no preference over any, L2 tries to balance the weights and chooses the one with lower error i.e $(0.5, 0.5)$</p><h2 id="How-do-we-deal-with-under-fitting"><a href="#How-do-we-deal-with-under-fitting" class="headerlink" title="How do we deal with under-fitting"></a>How do we deal with under-fitting</h2><p>Till now we’ve only seen methods for controlling over-fitting. The trivial solution for under-fitting is to increase your model complexity. You can do this by increasing the number of hidden layers, the number of units in each of these layers, choice of activation functions etc.,</p><p>Relax a bit with the post comic.<br><img src="https://imgs.xkcd.com/comics/types.png" alt="xkcd comic"></p>]]></content>
      
      <categories>
          
          <category> AI </category>
          
          <category> Deep Learning Fundamentals </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Neural Networks </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Overfitting and Underfitting</title>
      <link href="/overfitting-and-underfitting/"/>
      <url>/overfitting-and-underfitting/</url>
      <content type="html"><![CDATA[<p>Just to recap what we’ve seen so far,</p><ul><li><a href="/neuron-and-the-perceptron-algorithm/">Part 1: Neuron and the perceptron algorithm</a></li><li><a href="/multilayer-perceptrons-and-activation-function/">Part 2: Multilayer Perceptrons and Activation Function</a></li><li><a href="/cost-function-and-gradient-descent/">Part 3: Cost Function and Gradient Descent</a></li><li><a href="/forward-and-backward-propagation/">Part 4: Forward and Backward Propagation</a></li><li><a href="/loss-function-and-cross-entropy/">Part 5: Loss function and Cross-entropy</a></li></ul><a id="more"></a><p>By this point, we know how a feed-forward neural network works.</p><ol><li>Take in some input and calculate the output</li><li>Calculate the error</li><li>Calculate each weight’s contribution to the error.</li><li>Update weights such that the error decreases.</li><li>Repeat until the desired classification is achieved.</li></ol><p>This is it. But it looks too good to work. In fact, you’re right. Following just these steps above will land you in a lot of trouble in practice. Although it should work, the sheer volume of the data we have presents us many challenges. From this post forward, we’ll be looking at these problems one after the other.</p><h2 id="Training-and-Testing"><a href="#Training-and-Testing" class="headerlink" title="Training and Testing"></a>Training and Testing</h2><p>Till this post, we’ve only used the classifier to classify the points we give it while <a href="/forward-and-backward-propagation/#Neural-Network-terminology">training</a>. But, our goal often is to use our trained network to predict the outputs of instances which don’t have a label. This makes sense, because if we have classified points, why would we classify them again. The part where we predict the outputs of the new inputs is called <strong>testing</strong>.</p><p>Our goal now is to not only do good on the training data but also on the testing data. For example, let’s say you’ve trained a Dog breed classifier. Let’s say your training data had a million pictures of dogs and you achieved a 100% accuracy on the training set. All that would be useless if it doesn’t correctly classify the pictures of dogs you took on the sidewalk. Though this is very unlikely.</p><p>So, we train our neural network on the training set and expect it do well on the testing data. Here you assume that a <strong>data generating function($f$)</strong> has created your training and testing data. It is this function that our network tries to mimic. By adjusting our weights we’re trying to identify the data generating function $f$. Thus learning the characteristics of training data usually tells us something about the testing set as well. In practice, we can rarely identify the true data generating function. Because it is generated by so many natural random processes. We can only estimate it to a certain extent. This is called <strong>Generalization</strong>. When we generalize well, we perform well on both the training and testing sets.</p><h2 id="Expressive-power-Capacity"><a href="#Expressive-power-Capacity" class="headerlink" title="Expressive power / Capacity"></a>Expressive power / Capacity</h2><p>Can a neural network with one hidden layer and only two nodes in the hidden layer classify a set of points such as these</p><p><img src="https://lh3.googleusercontent.com/TUCifVfyqXEbIFYYgEOBw7Uojelx0b66NCAKyJ3JNw32B16epg6JILzRyYCmM_KwKnnaX041QgSQN_qY5scnwe2qIRFNHF20eAty4aN4Sr2CkHRuHuG3sfPEP0tSRVARFx3I-TGjLtTss7XklDVX8lT6z8W_qSEwGVCSBhmttDBUvTQaVRBgTSABOZf1P6IlTSD1TttCUk4lFCbDTsZWINJn5orVcPRwRO8p4k7pZWa7Ht3HEIZywmy62jeMeD4U64hkllpjYuIqYfKrYiCXAFAf7EKiKVrSE49_GWqh9gP9vh9PwZhMhf8q9UvdGhEt4up2beHRRYR1CBWTDvfQFvrhnk3oJNnKDGsVL8fBgRs1Z9b3yh5Ayz-bDqnhnW93RGVHvoSbNMyVGLyujz8ycqlz-JCiLb8tRnyERfso2WK6xm4HvqKDC8zOQDsJsp2fQ9BolbWq2mFBgUmEW3LjvesrG022xoHYDmyQK8AFVJJxASBUnWH-qsTG7dMZkUiUcbBgZrARtBBf7AzEoJGHreGqJoD31r0QCF3485UsAobiyCANmQ8W_eTjRH9881wSBHFvjKXXjYOCkn4EfU9KxrL7ukx9Yf8Hepw9ag=s710-no" alt="annular points"></p><p>You can play with it <a href="https://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=2&amp;seed=0.97760&amp;showTestData=false&amp;discretize=true&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false&amp;problem_hide=true&amp;noise_hide=true&amp;dataset_hide=true&amp;batchSize_hide=true&amp;regularizationRate_hide=true&amp;percTrainData_hide=true&amp;discretize_hide=true&amp;regularization_hide=true" target="_blank" rel="noopener">here</a> if you want to try it out. Change the learning rate and activation functions as you like. Finally, press the play button to train the network and see how well it fits. You just can’t do it perfectly, unless you design a new activation. This is where we can talk about the expressive power/capacity of a neural network. It is the ability of a neural network to mimic the underlying data generator function. In other words, how well can it generalize over the given data.</p><h2 id="Under-fitting"><a href="#Under-fitting" class="headerlink" title="Under-fitting"></a>Under-fitting</h2><p>When a network has a low capacity, it struggles to generalize well. This results in a highly biased network. What does it mean for a network to be biased? It means that the network has learned very few rules to classify the data. For example, let’s say we’re trying to classify a set of cars. Let’s say our training set has 3 different cars, a cat, a dog and a bike. Our testing set contains two cars, a bike, and a chicken. We want our classifier to do something like this</p><p><img src="https://lh3.googleusercontent.com/ztOH7E0ZnQh3s4vJWx3o3fJtYP2PJyKyCjAYcp7YA0H_zaanoc58c5djoG6vxnID0EHs75Xp8z99xc6CIgE7fwbCxnPSgjzKHuPpzKHTlOX_Q1w2zgrVDlQDzc3zzqik9Pc5DMotNkayCDJE-elgztb-G5YlorB0_j1jGY9xaqDS_MzaRO3IlMLO3M5H0LXLJkG1p3ZgHCW2xVvjOrllk6zC_M5Zq0DwI_nZNcf9oPAx8p-qIUNA8-9Xsv-4mU4KIWAMT9iYBiS8pVyG4k0LihSWOHaDsU5HSGJn_0gvdkxHhqfCfLXIeel_k2mqDusc_EIKatz5yoZ0TRoWHqRlpS02FmCWXjqmzHqTxcgFfpmjqkdBbHI66l9rBEc7uEYTLBhaw1vqM6l_rqrIkRV236I_G7lfDdrIhNXHwzBEzA3y2MqVmxl0ra9ThZaWgT_gFM39Q3knioVTdZdWfPad5zEWDkoEFp4XGdDXHh8BNKcbTKTTpqgY2nsH50fyUfQPnBxR4O8WIiME-cCbWTqkHmmF7ks6dItgE7mBApt5EC3D9ACPBLv_ACRdsDoZzZPa8jsAXXQlaxgm8Gs3eD5wN9XXbEaWizZE_AyWug=w1005-h710-no" alt="perfect fit illustration"></p><p>As you can see a good classifier identifies cars and not cars. A low capacity network, which is highly biased is said to be under-fit. It learns something like the following</p><p><img src="https://lh3.googleusercontent.com/6IfsRnUJpc10j-0amRpiKcbFEPcmYvAl0AsmtxMAvDMn_hKDivNBWPS176dgPEzn23pADLzwSzRd1LAUcaNC3HhUXBReHNzu0McD5K3Np32RXhPorP1JGxGGfK1c9ilyI4yHIll4NWonY5RySB7SeMNfVM88ukVcbu7l7qijpI8EporEMvUSTgwuPKBR5NdcKbsKoUY-skrqV9Ah7Wb6PThHuZmIm2DVFKuoKo9y-xK4kzzoaYQIr2dMVa_SCixQzZEKiAuKbJxxi6yumEgBlP1EQG20JL109oEn1USxntPHUuQGJ4h0HrOF8bXyGr98G39G-ysY0fwrigao4d-cxUC4T9h4isyPiPrdMxQcFWzraiqUk8kA-xcQngbsgm4vpqizB3zG02tZzgMb9P7LRAikMpVr42Xk4x0ydvBMDzGUvGLp4lUEFDCkEuqC0KmbSifPlz6x8IJAfJ2UD1NOwLTwi1YzBpy4x2iwx49BpVVeawlLSWmBkiIDUPk2aHiyqgpwSr5_cNVq_OKOVRGOFPzd5Z5JgQwd1XbhhZcsRCWI0oqPbYJ1TI-wC3YpN1UPp877pyPD95xHL7YcQupuy76oxOatYAg8cnVxCA=w1005-h710-no" alt="under fit illustration"></p><p>So it has only learned to distinguish between vehicles and not vehicles, because of it’s less capacity.</p><h2 id="Over-fitting"><a href="#Over-fitting" class="headerlink" title="Over-fitting"></a>Over-fitting</h2><p>In contrast to under-fitting, when a model has more capacity, there is a chance that it can over-fit. when overfitting your network fits too good to the training set and doesn’t generalize well. It results in a high-variance network. It means that your network learns rules that are too specific to the training data.</p><p><img src="https://lh3.googleusercontent.com/TwCAPdPbIinxr2lS0cX-UWn6t6f8LRWxkANuUt5CyTJ_1z-KUTyRYvkipM_3tYm0OycfH0lzAxX2LqOSWXmR4vqS_X4oUAsLi4TAOkGpIAiczCE1EJAtFktksdj_1yWO-TKrg2Mtq9aGjO2EcIqi-DgLJnDtfQ5VM2YgJcISnWf9YTd-8F0wN80IimWEoYkmqoFdUqav_oF8k0HYFVkdri7LZ4u3XCcEXxuRSo0h2WvcpF_srecVYrNw0ClOSyf3WNFfh3QdodIB_BNKRQUtvnsMhj-Z6vHpyi26mmbUF9Q5s1p-rB0fmGjGHteQbrfTwPc0QPNeEH-esgaHbfOJQi74qHYeKL52g7KGuVYP1U3m3LpHyGMf9Rsxs6agVakys6AJ1ZvXhOpROyx7CXVdlqg90nwYIcrweS0YrB9Xq9pVJNIGp4EIpZnh3W4yLectfR72yaCzO8KvFP0vQ8CypSitYqtEhKf2S0xNJZ_IrF6KKhl3klG0TTOiLJS5dM-3aqZynwycvzFh-gfSwqsMD79_j-1cypZOdSlTSse3kubUzu1GFMMuXyri3OtO_KKfDYfmBJY9kLlHRwFNYV4D-BfB9xmdOPfQE7Srwg=w1005-h710-no" alt="over fit illustration"></p><p>Here it has learned the rule to classify red, blue and yellow cars. Because of its high capacity and maybe due to overtraining.</p><p><strong>Note:</strong> This version of underfitting and overfitting is just for illustrative purposes. In practice, you cannot really know what rules the network has learned. We’re dealing with very high dimensional data. Our understanding of it is very limited and is an active area of research.</p><p>Let’s see some techniques for overcoming these problems in the next post. Enjoy your comic for now :)</p><p><img src="https://imgs.xkcd.com/comics/electoral_precedent.png" alt="overfitting xkcd"></p><p>Icon Credits: Icons by <a href="https://www.freepik.com" target="_blank" rel="noopener">freepik</a></p>]]></content>
      
      <categories>
          
          <category> AI </category>
          
          <category> Deep Learning Fundamentals </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Neural Networks </tag>
            
            <tag> common problems </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Loss function and Cross Entropy</title>
      <link href="/loss-function-and-cross-entropy/"/>
      <url>/loss-function-and-cross-entropy/</url>
      <content type="html"><![CDATA[<p>In the <a href="/cost-function-and-gradient-descent">gradient descent post</a>, you’ve seen what an error function is. What the characteristics of a good loss function are. Let’s take a deep dive into how we achieve this.<br><a id="more"></a><br>The desired characteristics of a good loss function are</p><ul><li>Continuous and Differentiable.</li><li>Penalizes wrong classification heavily and correct classifications weakly.</li></ul><h2 id="Prediction-as-a-probability"><a href="#Prediction-as-a-probability" class="headerlink" title="Prediction as a probability"></a>Prediction as a probability</h2><p>So in the <a href="/neuron-and-the-perceptron-algorithm/">perceptron post</a> we’ve used Heaviside step function to decide whether the output was 0 or 1. But our ideal loss function demands a continuous and differentiable function. Thus we can hack our loss function in such a way that it gives the probability that the given input belongs to a class. For example if there are two classes $A$ and $B$. After the hack we expect a result $P(A)$ from our error function. The probability of our input being $B$ can be easily calculated with $P(B) = 1 - P(A)$.</p><p>All we have to do to achieve this is change the activation function. In the <a href="/multilayer-perceptrons-and-activation-function/">activation post</a> we’ve seen the three most common activation functions - $ReLU$, $sigmoid$ and $tanh$. $ReLU$ is only piece-wise continuous. For this reason it is avoided as the activation function of the final output. $tanh$ produces outputs in the range of $(-1, 1)$. But we want the output to be in the range of $[0, 1]$ for representing the probability. $sigmoid$ does the job here. It is a good choice producing outputs in the range of $(0, 1)$.</p><p>$$sigmoid(x) = \frac{1}{1+e^{-x}}$$<br><img src="https://www.dropbox.com/s/wacrmulmj3b090i/sigmoid.png?raw=1" alt="Sigmoid function graph"></p><h2 id="Finding-the-ideal-function"><a href="#Finding-the-ideal-function" class="headerlink" title="Finding the ideal function"></a>Finding the ideal function</h2><p>We now have a neural network that produces a probability $p$. Our error can take advantage of this fact. Now the only thing left is the penalty part. For a point labeled 1, we need our error function to be</p><ul><li>high when it’s output is closer to 0 (since we have a low probability that the point is 1)</li><li>low when it’s output is closer to 1. (zero when we’re absolutely confident i.e probability is 1.)</li></ul><p>You can explore functions that have this characteristic. One of the most used function for this purpose is $-ln(p)$. See the response of $-ln(p)$ below.</p><p><img src="https://www.dropbox.com/s/mzsndm5pgell6uq/log-loss.png?raw=1" alt="log-loss or cross entropy"></p><h2 id="Cross-Entropy"><a href="#Cross-Entropy" class="headerlink" title="Cross Entropy"></a>Cross Entropy</h2><p>It’s time to design ourselves an error function that would work for both the labels $0$ and $1$. If $-ln(p)$ gives us the penalty for classifying label $1$, $-ln(1-p)$ is the penalty for classifying label $0$. Since $1-p$ is the probability of the point being 0. Now all we have to do is take the penalty for 1 when the true label is $1$ and the penalty for 0 when the label is $0$. We can do so easily by the following formula</p><p>$$<br>\text{when y is the true label and p is the probability that the label is 1} \\<br>-y.ln(p) - (1-y).ln(1-p) =<br>\begin{cases}<br>-ln(1-p) &amp; {\text{if y=0}} \\<br>-ln(p) &amp; {\text{if y=1}}<br>\end{cases}\\<br>$$</p><p>$-y.ln(p) - (1-y).ln(1-p)$ is known as <strong>cross-entropy</strong>.</p><h2 id="Multi-class-Classification"><a href="#Multi-class-Classification" class="headerlink" title="Multi-class Classification"></a>Multi-class Classification</h2><p>Till now we’ve seen the problems where there only two outcomes i.e binary classification. Now let’s see a problem with more than two labels. Assume we are given four temperatures taken at 12PM, 6PM, 12AM, 6AM of a particular city. Our task is to classify the given temperatures into spring, summer, autumn or winter. So, we need to have four outputs here. Each corresponding to a season. In binary classification, our labels were straight forward, either $0$ or $1$. But how are labels for a multi-class problems set? We prefer not to set the labels to $0, 1, 2, 3$. We’ll discuss why this is the case in another post. But for now let’s see a popular way of setting labels for multi-class problems.</p><h3 id="One-hot-encoding"><a href="#One-hot-encoding" class="headerlink" title="One-hot encoding"></a>One-hot encoding</h3><p>We create an array of size $n$, where $n$ is the number of classes. Then we designate each of the classes a position in the array. For a label that needs to specify a class, the position corresponding to the class is filled with $1$ and rest all with $0$. Let’s see this in a concrete example. For the season problem above, one way to set labels is as follows<br>$$<br>label_{\text{spring}} = \begin{pmatrix}<br>1 \\<br>0 \\<br>0 \\<br>0<br>\end{pmatrix}, label_{\text{summer}} = \begin{pmatrix}<br>0 \\<br>1 \\<br>0 \\<br>0<br>\end{pmatrix}, label_{\text{autumn}} = \begin{pmatrix}<br>0 \\<br>0 \\<br>1 \\<br>0<br>\end{pmatrix}, label_{\text{winter}} = \begin{pmatrix}<br>0 \\<br>0 \\<br>0 \\<br>1<br>\end{pmatrix}<br>$$</p><p>This is known as <strong>One-hot encoding</strong>.</p><h2 id="Multi-class-Cross-Entropy"><a href="#Multi-class-Cross-Entropy" class="headerlink" title="Multi-class Cross Entropy"></a>Multi-class Cross Entropy</h2><p>We also need to change <em>cross entropy</em> to adapt to the multi-class problem. Before we do that, we need to change the output of our neural network accordingly. In binary classification the output was a probability of the label belonging to a class. Here we need multiple outputs producing the probability of each class.</p><h3 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h3><p>We used sigmoid function for a binary classifier to convert the output to a probability. We use a similar trick in the multi-class problem. The idea is to normalize each of the output with the sum of the outputs. In order to that we need all the outputs to be positive, as probability cannot be negative. So, we change each output $y_i$ to $e^{y_i}$. Now, we can normalize and get the probability of each output. This procedure is known as <strong>softmax</strong>. Let’s see a concrete example from the season problem defined above</p><p>$$\hat{Y} = \begin{pmatrix}<br>\hat{y_1} \\<br>\hat{y_2} \\<br>\hat{y_3} \\<br>\hat{y_4}<br>\end{pmatrix}<br>\begin{matrix}<br>\rightarrow \text{ this corresponds to spring} \\<br>\rightarrow \text{ this corresponds to summer} \\<br>\rightarrow \text{ this corresponds to autumn} \\<br>\rightarrow \text{ this corresponds to winter}<br>\end{matrix} \\<br>softmax(\hat{Y}) = \begin{pmatrix}<br>\dfrac{e^{\hat{y_1}}}{e^{\hat{y_1}}+e^{\hat{y_2}}+e^{\hat{y_3}}+e^{\hat{y_4}}} \\<br>\dfrac{e^{\hat{y_2}}}{e^{\hat{y_1}}+e^{\hat{y_2}}+e^{\hat{y_3}}+e^{\hat{y_4}}} \\<br>\dfrac{e^{\hat{y_3}}}{e^{\hat{y_1}}+e^{\hat{y_2}}+e^{\hat{y_3}}+e^{\hat{y_4}}} \\<br>\dfrac{e^{\hat{y_4}}}{e^{\hat{y_1}}+e^{\hat{y_2}}+e^{\hat{y_3}}+e^{\hat{y_4}}}<br>\end{pmatrix}<br>\begin{matrix}<br>\dfrac{\rightarrow \text{ probability of output being spring}}{} \\<br>\dfrac{\rightarrow \text{ probability of output being summer}}{} \\<br>\dfrac{\rightarrow \text{ probability of output being autumn}}{} \\<br>\dfrac{\rightarrow \text{ probability of output being winter}}{}<br>\end{matrix}<br>$$</p><p>With this, we’ve all the necessary elements for training and testing neural networks. From the next post, we’ll look at common problems and their respective solutions. Don’t forget your comic.</p><p><img src="https://imgs.xkcd.com/comics/wisdom_of_the_ancients.png" alt="XKCD Wisdom of the ancients"></p>]]></content>
      
      <categories>
          
          <category> AI </category>
          
          <category> Deep Learning Fundamentals </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Neural Networks </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Forward and Backward Propagation</title>
      <link href="/forward-and-backward-propagation/"/>
      <url>/forward-and-backward-propagation/</url>
      <content type="html"><![CDATA[<p>Till now we’ve seen</p><ul><li><a href="/multilayer-perceptrons-and-activation-function/">How a Multilayer Perceptron works</a></li><li><a href="/cost-function-and-gradient-descent/">How Gradient Descent helps Multilayer Perceptron</a></li></ul><p>Now let’s see some tricks that extend these ideas to Deep neural networks.<br><a id="more"></a></p><h2 id="Neural-Network-terminology"><a href="#Neural-Network-terminology" class="headerlink" title="Neural Network terminology"></a>Neural Network terminology</h2><p>From the previous posts, you know that neural networks are good at classification. It does this in four steps</p><ol><li>Calculate the output of the current network.</li><li>Compare the output with the correct output.</li><li>Update weights of the network by Gradient Descent.</li><li>Repeat steps 1-3 until you achieve a good classifier.</li></ol><p>Before we proceed further it’s better to know some terminology and notations.</p><div class="note info"><ul><li><strong>Training</strong>: The whole process shown in the 4 steps above.</li><li><strong>instance</strong>: A single data point from the set of data points we’re trying to classify.</li><li><strong>Label</strong>: The correct/desired output of an instance.</li><li><strong>Prediction</strong>: Calculating the output for a given input using our current set of weights.</li></ul></div><h2 id="Some-standard-notations"><a href="#Some-standard-notations" class="headerlink" title="Some standard notations"></a>Some standard notations</h2><p>In the <a href="/multilayer-perceptrons-and-activation-function/">activation and MLP post</a> the final output looked something like this $a(Max(0, w_{01}+w_{11}x+w_{21}y)) + b(Max(0, w_{02}+w_{12}x+w_{22}y))+c &gt; 0$. Imagine what a network with hundreds or even thousands of perceptrons would produce. To simplify this, we can stick to a standard notation and abstract away all the cumbersome stuff. Let’s use the following neural network as a reference.<br>(Note: This notation is adapted from <a href="https://www.deeplearning.ai/" target="_blank" rel="noopener">Andrew Ng’s Deep Learning series</a> and tweaked a little.)</p><p><img src="https://www.dropbox.com/s/iws8e9brfxmaj23/neural_network_terminology.png?raw=1" alt="Neural Network Architecture"></p><h3 id="Representation-of-the-neural-network"><a href="#Representation-of-the-neural-network" class="headerlink" title="Representation of the neural network"></a>Representation of the neural network</h3><p>Let $L$ be the number of layers. Input is not considered a layer and is often called Layer 0. The hidden layers till the output are numbered from 1 to L. Denote the number of units in the $l^{th}$ layer by $n^{[l]}$. Notations of other units follow.</p><h3 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h3><p>We use $X/A^{[0]}$ to denote the input. In our network it will be</p><div class="note info"><p>$$X = A^{[0]} = \begin{bmatrix}<br>x_1 \\<br>x_2<br>\end{bmatrix}$$</p></div><h3 id="Weights-and-Bias"><a href="#Weights-and-Bias" class="headerlink" title="Weights and Bias"></a>Weights and Bias</h3><p>Consider the weights between input layer and the first hidden layer as shown below</p><p><img src="https://www.dropbox.com/s/ddyj4njtrru1nzq/weights_and_biases.png?raw=1" alt="Weights and Biases"></p><p>We denote the weights between $i^{th}$ and ${i-1}^{th}$ layer as $W^{[i]}$ and the bias before $i^{th}$ layer as $B^{[i]}$. So in the figure above $W^{[1]}$ and $B^{[1]}$ represent</p><div class="note info"><p>$$<br>W^{[1]} = \begin{bmatrix}<br>w_{11}^{[1]} &amp; w_{21}^{[1]} \\<br>w_{12}^{[1]} &amp; w_{22}^{[1]}<br>\end{bmatrix}<br>$$</p><p>$$<br>B^{[1]} = \begin{bmatrix}<br>b_1^{[1]} \\<br>b_2^{[1]}<br>\end{bmatrix}<br>$$</p></div><h3 id="Activations"><a href="#Activations" class="headerlink" title="Activations"></a>Activations</h3><p>Activations are a little tricky. Because two operations are performed in the a single activation unit. For the $i^{th}$ activation layer the operations are</p><ol><li>$Z^{[i]} = W^{[i]}A^{[i-1]} + B^{[i]}$</li><li>$A^{[i]} = g^{[i]}(Z^{[i]}), \hspace{0.5cm} \text{where g is the activation function.}$</li></ol><p>Let’s make this a little more concrete by considering the first activation layer.</p><div class="note info"><p>$$<br>Z^{[1]} = W^{[1]}A^{[0]} + B^{[1]}\\<br>\implies Z^{[1]} = \begin{bmatrix}<br>w_{11}^{[1]} &amp; w_{21}^{[1]} \\<br>w_{12}^{[1]} &amp; w_{22}^{[1]}<br>\end{bmatrix}\begin{bmatrix}<br>x_1 \\<br>x_2<br>\end{bmatrix} + \begin{bmatrix}<br>b_1^{[1]} \\<br>b_2^{[1]}<br>\end{bmatrix}<br>$$</p><p>$$<br>\text{Assuming the activation is ReLU}\\<br>A^{[1]} = ReLU(Z^{[1]})\\<br>\implies A^{[1]} = ReLU\left(\begin{bmatrix}<br>w_{11}^{[1]} &amp; w_{21}^{[1]} \\<br>w_{12}^{[1]} &amp; w_{22}^{[1]}<br>\end{bmatrix}\begin{bmatrix}<br>x_1 \\<br>x_2<br>\end{bmatrix} + \begin{bmatrix}<br>b_1^{[1]} \\<br>b_2^{[1]}<br>\end{bmatrix}\right)\\<br>\implies A^{[1]} = ReLU\left(\begin{bmatrix}<br>w_{11}^{[1]}x_1 + w_{21}^{[1]}x_2 + b_1^{[1]} \\<br>w_{12}^{[1]}x_1 + w_{22}^{[1]}x_2 + b_2^{[1]}<br>\end{bmatrix}\right)<br>$$</p></div><h3 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h3><p>The output is the last activation layer $A^{[L]}$. It is also denoted by $\hat{Y}$ and calculated by the same logic of the activation layer.</p><h2 id="Forward-Propagation"><a href="#Forward-Propagation" class="headerlink" title="Forward Propagation"></a>Forward Propagation</h2><p>The process of forward propagation is to calculate the outputs of each hidden layer. Setting $A^{[0]} = X$, we can start calculating the outputs as follows</p><p>$$<br>Z^{[1]} = W^{[1]}A^{[0]} + B^{[1]},\hspace{0.5cm} A^{[1]} = g^{[1]}(Z^{[1]}) \\<br>Z^{[2]} = W^{[2]}A^{[1]} + B^{[2]},\hspace{0.5cm} A^{[2]} = g^{[2]}(Z^{[2]}) \\<br>Z^{[3]} = W^{[3]}A^{[2]} + B^{[3]},\hspace{0.5cm} A^{[3]} = g^{[3]}(Z^{[3]}) \\<br>. \\<br>. \\<br>. \\<br>Z^{[L]} = W^{[L]}A^{[L-1]} + B^{[L]},\hspace{0.5cm} A^{[L]} = g^{[L]}(Z^{[L]})$$</p><p>In a nutshell, given $A^{[l-1]}$ feed forward produces the output $A^{[l]}$. The outputs are calculated in a linear way, hence the name feed forward networks.</p><div class="note info"><ul><li><strong>Feed Forward</strong>: The process of calculating outputs of each hidden layer for a given input.</li></ul></div><h2 id="Backward-Propagation"><a href="#Backward-Propagation" class="headerlink" title="Backward Propagation"></a>Backward Propagation</h2><p>At this stage we have our predicted labels. It is time for us to update the weights based on our error. To update our weights, we must first look at the gradient with respect to the weights. Use the gradients and move the weights in the direction that decreases our error. In the gradient descent post we took it for granted that our error function is a function of weights. But this is indirectly the case. In practice any error function calculates it’s metric by comparing two things</p><ul><li>the predicted output $\hat{Y}$ and</li><li>the desired output $Y$(label)</li></ul><p>So it’s fair to write $E$ is a function of $Y $ and $\hat{Y}$.</p><p>Remember that $\hat{Y}$ is dependent on all the weights and biases of the previous layer. So, we can easily show $E$ as a function of every weight layer.</p><p>$$<br>\begin{align}<br>E(Y, \hat{Y}) &amp;= E(Y, A^{[L]})\\<br>&amp;= E(Y, g^{[L]}(Z^{[L]})) \\<br>&amp;= E(Y, g^{[L]}(W^{[L]}A^{[L-1]} + B^{[L]})) \\<br>&amp;= E(Y, g^{[L]}(W^{[L]}g^{[L-1]}(Z^{[L-1]}) + B^{[L]})) \\<br>&amp;= E(Y, g^{[L]}(W^{[L]}g^{[L-1]}(W^{[L-1]}A^{[L-2]} + B^{[L-1]}) + B^{[L]})) \\<br>\end{align}<br>$$<br>expanding any further is sheer lunacy. But, you get the point that the error function can be written in terms of each weight layer. Now, we can calculate the gradient with respect to each weight layer.</p><h3 id="Chain-Rule"><a href="#Chain-Rule" class="headerlink" title="Chain Rule"></a>Chain Rule</h3><p>For obtaining the partial derivative, we have to differentiate this composite function. This is where the chain rule from calculus can help us. If a variable z depends on variable y, which in turn depends on the variable x then</p><p>$$\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} \frac{\partial y}{\partial x}$$</p><p>Using this we can write the derivative of $E$ with respect to $W^{[L]}$ as follows<br>$$<br>\begin{align}<br>\frac{\partial E}{\partial W^{[L]}} &amp;= \frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial W^{[L]}} \\<br>&amp;= \frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial W^{[L]}}\\<br>&amp;=\frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}} \frac{\partial Z^{[L]}}{\partial W^{[L]}}\\<br>&amp;=\underbrace{\frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}}}_{\frac{\partial E}{\partial Z^{[L]}}} A^{[L-1]}<br>\end{align}<br>$$</p><p>In a similar fashion<br>$$<br>\frac{\partial E}{\partial B^{[L]}} = \frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}} \frac{\partial Z^{[L]}}{\partial B^{[L]}}\\<br>\implies \frac{\partial E}{\partial B^{[L]}} = \frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}} = \frac{\partial E}{\partial Z^{[L]}}\\<br>$$</p><h3 id="Cache-the-previous-output"><a href="#Cache-the-previous-output" class="headerlink" title="Cache the previous output"></a>Cache the previous output</h3><p>This looks fine for the $L^{th}$ layer but doesn’t it get huge as we approach the derivative with respect to weight layer 1. Let’s see what happens at the ${L-1}^{th}$ derviative.</p><p>$$<br>\begin{align}<br>\frac{\partial E}{\partial W^{[L-1]}} &amp;= \frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial W^{[L-1]}} \\<br>&amp;= \frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial W^{[L-1]}}\\<br>&amp;=\frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}} \frac{\partial Z^{[L]}}{\partial W^{[L-1]}}\\<br>&amp;=\frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}} \frac{\partial Z^{[L]}}{\partial W^{[L-1]}} \\<br>&amp;=\frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}} \frac{\partial Z^{[L]}}{\partial A^{[L-1]}} \frac{\partial A^{[L-1]}}{\partial W^{[L-1]}} \\<br>&amp;=\frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}} \frac{\partial Z^{[L]}}{\partial A^{[L-1]}} \frac{\partial A^{[L-1]}}{\partial g^{[L-1]}} \frac{\partial g^{[L-1]}}{\partial W^{[L-1]}}\\<br>&amp;=\frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}} \frac{\partial Z^{[L]}}{\partial A^{[L-1]}} \frac{\partial A^{[L-1]}}{\partial g^{[L-1]}} \frac{\partial g^{[L-1]}}{\partial Z^{[L-1]}} \frac{\partial Z^{[L-1]}}{\partial W^{[L-1]}}\\<br>&amp;=\underbrace{\frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}} \frac{\partial Z^{[L]}}{\partial A^{[L-1]}} \frac{\partial A^{[L-1]}}{\partial g^{[L-1]}} \frac{\partial g^{[L-1]}}{\partial Z^{[L-1]}}}_{\frac{\partial E}{\partial Z^{[L-1]}}} A^{[L-2]}\\<br>\end{align}<br>$$</p><p>Notice while calculating $\frac{\partial E}{\partial W^{[L-1]}}$ we already had the result of $\frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}}$ from computing $\frac{\partial E}{\partial W^{[L]}}$. We can save this calculation from every previous layer and use it in our current computation. So, it won’t be messy when we get down to the last layer. In general going down a layer each time you’ll notice a pattern that<br><img src="https://www.dropbox.com/s/l15ikka0kx50w82/back_prop_steps.svg?raw=1" alt="BackPropagation trick"></p><h3 id="Updating-the-weights"><a href="#Updating-the-weights" class="headerlink" title="Updating the weights"></a>Updating the weights</h3><p>So we’ve come to the final part of this simple algorithm. Remember that we need to take small steps in the direction decreasing the error. Here $\alpha$, the learning rate does the job. As long as you don’t set the learning rate high, you should be fine. Finding a good learning rate is for another post.</p><p>Updating the weights is straightforward</p><p>$$W^{[i]} := W^{[i]} - \alpha \frac{\partial E}{\partial W^{[i]}}$$<br>$$B^{[i]} := B^{[i]} - \alpha \frac{\partial E}{\partial B^{[i]}}$$</p><div class="note info"><ul><li><strong>Backward Propagation</strong>: The process of calculating each weights’ contribution to the error and updating them accordingly.</li></ul></div><p>If you don’t get something from the post, relax! Take a step back and rework it again. Despite what you’ve heard, back propagation is as simple idea. Uses basic calculus and linear algebra. Ending this post on a relief note :)<br><img src="https://imgs.xkcd.com/comics/too_old_for_this_shit.png" alt="Too old for math"></p>]]></content>
      
      <categories>
          
          <category> AI </category>
          
          <category> Deep Learning Fundamentals </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Neural Networks </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Cost Function and Gradient Descent</title>
      <link href="/cost-function-and-gradient-descent/"/>
      <url>/cost-function-and-gradient-descent/</url>
      <content type="html"><![CDATA[<p>In the last <a href="/multilayer-perceptrons-and-activation-function">post</a> we’ve discussed how a Multilayer Perceptron works. Now let’s see how to search/update a set of weights to achieve a good classifer.<br><a id="more"></a></p><h2 id="Error-Function"><a href="#Error-Function" class="headerlink" title="Error Function"></a>Error Function</h2><p>We are on the search for the best set of weights for our classifier. Before we do that, we need something that tells us how good we are doing with the current set of weights. This is exactly what an <em><strong>error function</strong></em> AKA <em><strong>loss function</strong></em> AKA <em><strong>cost function</strong></em> does. It provides us a quantifiable metric depending on which we can update our set of weights. Let’s say our error function is $E(W)$ where W is the set of weights.</p><h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><p>Our goal now is to minimize $E(W)$ for the classifier to work. To do that, we have $W$ that we can use to change $E(W)$. Here is where a basic idea from calculus helps. A small change in the input of a function is related to the derivative in the following way</p><div class="note info"><p>$$f(x + \epsilon) \approx f(x) + \epsilon f’(x) $$</p></div><p>So, an $\epsilon$ change in the input produces a $\epsilon f’(x)$ change in the output. This is all we need to decrease our error function. Some math recap before we build error minimizing algorithm.<br>The derivative of a function at a point is</p><ul><li><strong>positive</strong> if the function is increasing in the neighborhood of the point.</li><li><strong>negative</strong> if it’s decreasing in the neighborhood of the point.</li><li><strong>zero</strong> if it’s neither increasing or decreasing.(saddle points)</li></ul><p>We can use this fact in our error function as shown below</p><div class="note info"><p>$$E(W+\epsilon) \approx E(W) + \epsilon E’(W) \hspace{1cm}\text{for a small enough}\hspace{0.2cm} \epsilon$$<br>$$\implies E(W+\epsilon) &lt; E(W)\hspace{1cm} if \hspace{0.2cm}E’(W) &lt; 0 $$<br>$$\implies E(W-\epsilon) &lt; E(W)\hspace{1cm} if \hspace{0.2cm}E’(W) &gt; 0 $$</p></div><p>So, all we have to do now is decrease the weights when derivate is positive and vice versa. If we repeat this operation with a small enough $\epsilon$ we can minimize the error function. This is the whole idea of <strong>Gradient Descent</strong>. A simple form for the algorithm is</p><div class="note info"><p>until we reach an error threshold or $E’(W) = 0$<br>$$W ← W - \epsilon sgn(E’(W))<br>\hspace{1cm}where\hspace{0.2cm}sgn(x) :=<br>\begin{cases}<br>-1 &amp; {x &lt; 0,} \\<br>0 &amp; {x = 0,} \\<br>1 &amp; {x &gt; 0.}<br>\end{cases}<br>$$</p></div><h2 id="Choosing-an-error-function"><a href="#Choosing-an-error-function" class="headerlink" title="Choosing an error function"></a>Choosing an error function</h2><p>The simple yet powerful algorithm depends on the error function to work. So, let’s explore how to choose an error function. In the <a href="/neuron-and-the-perceptron-algorithm/">perceptron post</a> we chose the number of misclassified points as our error function. But, <code># misclassified points</code> can only have integer values and is a discrete function. This is a poor choice for an error function, because</p><ul><li>In many situations you’ll find that the number of misclassified points can’t be decreased. The weights keep bouncing between a set of values.</li><li>Differentiating a discrete function.</li><li>All the misclassified points receive the same treatment. Points misclassified by a small margin aren’t any different from large error margins.</li></ul><p>To overcome these challenges we can emphasize on the following characteristics</p><ul><li>The error function must be <em><strong>continuous and differentiable</strong></em>.</li><li>Penalize misclassified points based on their error margins.</li></ul><p>Designing such function is for another post. Let’s see some limitations of gradient descent before moving to the next post.</p><h2 id="The-limitations-gradient-descent"><a href="#The-limitations-gradient-descent" class="headerlink" title="The limitations gradient descent"></a>The limitations gradient descent</h2><p>As long as $E’(W)$ is not zero, our weights keep updating and we’ll be approaching a minima in the error function. But what about the case when it’s zero, our algorithm terminates. But we don’t have to be at a minima when this happens. Points where $E’(W) = 0$ are called <strong>critical points</strong>. Three different scenarios for critical points are</p><ul><li><p><strong>Minima:</strong> Moving in any direction doesn’t decrease the function. <img src="https://www.dropbox.com/s/ekpujat5y46ey9z/minima.png?raw=1" alt="Minima figure"></p></li><li><p><strong>Maxima:</strong> Moving in any direction doesn’t increase the function. <img src="https://www.dropbox.com/s/iwq9jj1htsbak19/maxima.png?raw=1" alt="Maxima figure"></p></li><li><p><strong>Saddle Point:</strong> Moving in any direction doesn’t increase or decrease the function. <img src="https://www.dropbox.com/s/03wdjpjsih8tc2f/saddle.png?raw=1" alt="Saddle point figure"></p></li></ul><p>So our algorithm can stop at any of these situations. Even if we land on a minima it may not be the absolute lowest that is possible. When there are many local minima, we find ourselves stuck at the local minima. Different scenarios that can occur are shown in the figure below.<br><img src="https://www.dropbox.com/s/xapmmirt8xgmyt0/approximate_minimization.png?raw=1" alt="Approximate Minimization"></p><ol><li>Ideally, we would like to achieve this point (Global Minima), but it may not be possible.</li><li>This is at par with the global minima and is acceptable.</li><li>These points lead to poor performance.</li></ol><p>We’ll discuss tricks and tips to mitigate these limitations in another post. Here’s a little treat for making it till the end.</p><p><img src="https://imgs.xkcd.com/comics/listening.png" alt="Alexa joke"></p>]]></content>
      
      <categories>
          
          <category> AI </category>
          
          <category> Deep Learning Fundamentals </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Neural Networks </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Multilayer Perceptrons and Activation function</title>
      <link href="/multilayer-perceptrons-and-activation-function/"/>
      <url>/multilayer-perceptrons-and-activation-function/</url>
      <content type="html"><![CDATA[<h2 id="The-XOR-problem"><a href="#The-XOR-problem" class="headerlink" title="The XOR problem"></a>The XOR problem</h2><p>In the previous <a href="/deep-learning-fundamentals-neuron-and-the-perceptron-algorithm">post</a> you’ve seen how a perceptron works. Now let’s dive into some more interesting problems in deep learning. What follows is the classic XOR problem. Develop a method for the correct classification of the following points.</p><a id="more"></a><p><img src="https://www.dropbox.com/s/rks7mlcp2vubj3c/XOR_problem.png?raw=1" alt="XOR PROBLEM"></p><h2 id="Multi-Layer-Perceptron"><a href="#Multi-Layer-Perceptron" class="headerlink" title="Multi Layer Perceptron"></a>Multi Layer Perceptron</h2><p>It is clear that these points are linear inseparable. So, a perceptron fails at classifying these points. To classify these points it is clear that we need a non-linear boundary. If one perceptron can’t do the job, let’s try combining many of them. To keep it simple and the weights manageable, let’s use the architecture below.<br>(Note: Every layer stacked between the input and output layer is a hidden layer. We’ll see more about it later in this post.)</p><p><img src="https://www.dropbox.com/s/4ulft6rx1jnsrc7/mlp_without_activation.png?raw=1" alt="MLP without activation"></p><p>But this doesn’t work. Because, the whole output of the last unit is a linear function in $x$ and $y$. We can write the output of $h_3$ as<br>$h_3 = Ax + By + C$ where<br>$A = aw_{11} + bw_{12}$<br>$B = aw_{12} + bw_{22}$<br>$C = aw_{01} + bw_{02} + c$</p><h2 id="Activation-Function"><a href="#Activation-Function" class="headerlink" title="Activation Function"></a>Activation Function</h2><p>This is where an activation function comes to play. An activation function is a non-linear function applied to the end result of a perceptron. This introduces non-linearity to the perceptron and to the network.</p><p>Some common activation functions are</p><h3 id="ReLU-Rectified-Linear-Unit"><a href="#ReLU-Rectified-Linear-Unit" class="headerlink" title="ReLU (Rectified Linear Unit)"></a>ReLU (Rectified Linear Unit)</h3><p><strong><br>$$y = max(0, x)$$</strong><br><img src="https://www.dropbox.com/s/7evkd5143dsk8tu/relu.png?raw=1" alt="ReLU activation graph"></p><h3 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h3><p><strong><br>$$y =\frac{1}{1 + e^{-x}}$$</strong><br><img src="https://www.dropbox.com/s/wacrmulmj3b090i/sigmoid.png?raw=1" alt="Sigmoid activation graph"></p><h3 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h3><p><strong><br>$$y = \frac{ e^x - e^{-x}}{ e^x + e^{-x} }$$</strong><br><img src="https://www.dropbox.com/s/n8wm85xjqwav69t/tanh.png?raw=1" alt="Tanh activation graph"></p><p>Ok, so let’s apply “ReLU” to a perceptron and see what the response is. You can fiddle around with ReLU and become more familiar with it’s response <a href="https://www.desmos.com/calculator/ap5gwdoznu" target="_blank" rel="noopener">here</a></p><h2 id="Multi-Layer-Perceptron-Activated"><a href="#Multi-Layer-Perceptron-Activated" class="headerlink" title="Multi Layer Perceptron Activated"></a>Multi Layer Perceptron Activated</h2><p>Now let’s try activating the perceptrons in our network with relu. Our previous network will now be<br><img src="https://www.dropbox.com/s/n7m305d5noa5205/activation_and_mlp.png?raw=1" alt="MLP with activated perceptrons"></p><p><strong>Note:</strong> The final unit isn’t activated by relu, because we only need it to do binary classification. So we use a <a href="https://en.wikipedia.org/wiki/Heaviside_step_function" target="_blank" rel="noopener">Heaviside step function</a> instead. The weights $w_{01}, w_{02}, c$ are biases and are added implicitly and are not shown in the graph.</p><p>Now let’s fiddle around with the parameters we mentioned in the formulas above. Try to come up with a set of weights ($w_{01}, w_{11}, w_{21}, w_{02}, w_{12}, w_{22}, a, b, c$), so that you create a region that sepearates these points. If the graph below is too small you can work on it <a href="https://www.desmos.com/calculator/ggs1zd0ogl" target="_blank" rel="noopener">here</a>.</p><iframe src="https://www.desmos.com/calculator/ggs1zd0ogl" width="100%" height="600px" frameborder="0" allowfullscreen></iframe><h2 id="MLP-Terminology"><a href="#MLP-Terminology" class="headerlink" title="MLP Terminology"></a>MLP Terminology</h2><p>Hope you got an intuitive sense of how it works. Before we go further, let’s clear up some terminology. An MLP is a class of feed-forward neural network which we’ll know more about in a later post. In a feed-forward network</p><ul><li><p><strong>Hidden Layer</strong>: Any layer that doesn’t directly provide input or output is a hidden layer. The outputs and inputs of these layers are not directly visible or not a point of interest. Thus the name, <em>hidden layer</em>.</p></li><li><p><strong>Depth of a neural network</strong>: The depth of a neural network is the number of hidden layers plus one. Plus one because output weights can be modified. So by increasing the number of hidden layers, you’re increasing the depth of a neural network.</p></li><li><p><strong>Width of a hidden layer</strong>: The number of units in a hidden layer is also known as the width of the layer.</p></li></ul><p><img src="https://www.dropbox.com/s/q0c8n47q6xzb9hk/nn_depth_width.png?raw=1" alt="Neural network - Depth and Width"></p><h2 id="The-search-for-parameters"><a href="#The-search-for-parameters" class="headerlink" title="The search for parameters"></a>The search for parameters</h2><p>In the example above you came up with the set of weights that classify the XOR points. But, it’s not practical to search for a set of a weights, especially when we stack many more perceptrons. To understand why a naive search for a best set of weights doesn’t work, let’s consider a simple scenario. Let’s say we have a fully connected network(every node in a layer is connected to every node in the next layer.) and we have 3 hidden layers each with a 100 nodes. Input nodes are 2 and the output is a single node. So the number of parameters will be</p><ul><li><strong>between input and hidden layer 1:</strong> 2 x 100</li><li><strong>between hidden layer 1 and hidden layer 2:</strong> 100 x 100</li><li><strong>between hidden layer 2 and hidden layer 3:</strong> 100 x 100</li><li><strong>between hidden layer 3 and output:</strong> 100 x 1</li></ul><p>So, we end up with a total of 200 + 10000 + 10000 + 100 = <strong>20,300</strong> parameters. But those are just the number of parameters. Each parameter can have any arbitrary weight. But for the sake of a concrete example, let’s say we bound the weights between -100 and 100 with a step size of 0.01. So the number of possible values for each parameter is $(100 - (-100))/0.01$ = $20,000$. To find an ideal set of weights through naive search, you must search for one instance in 20,300 * 20,000 = 406,000,000. Searching naively through 406 million instances for a simple network is inefficient and unnecessary. This is where <strong>Stochastic Gradient Descent</strong> can make the search faster towards the goal. We’ll learn all about it in the next post.</p><p>Don’t forget your dopamine shot for making it till the end.<br><img src="https://imgs.xkcd.com/comics/computers_vs_humans.png" alt="xkcd - too cool to care"></p>]]></content>
      
      <categories>
          
          <category> AI </category>
          
          <category> Deep Learning Fundamentals </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Neural Networks </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Neuron and the Perceptron Algorithm</title>
      <link href="/neuron-and-the-perceptron-algorithm/"/>
      <url>/neuron-and-the-perceptron-algorithm/</url>
      <content type="html"><![CDATA[<p>This is the first part of the series <em>“Deep Learning Fundamentals”</em>. The goal of this series is to explore the mechanisms of artificial neural networks. The focus is more on presenting an intuitive way of understanding neural networks. So, you can expect an emphasis on how and why things work rather than what does the job. More often than not I’ll try to use simple math without focusing on notation. Let’s jump into the fundamental unit of most of the neural networks - “a neuron”.<br><a id="more"></a></p><h2 id="Neuron"><a href="#Neuron" class="headerlink" title="Neuron"></a>Neuron</h2><p>A neuron is nothing but a binary classifier. Which means given an n-dimensional space, it can divide the space into two regions. Let’s try to come up with an algorithm that does this, given that the data is linearly separable. For the sake of brevity, let’s consider that the data we deal with is numerical across all dimensions. (Dealing with other kinds of data is for another post.)</p><p>Without loss of generality let’s start working with two-dimensional data. So, let’s say there are some green(0) and blue(1) points on a plane and we want a line that separates these points. An intuitive way would be to draw a line like shown in the graph below. So far so good. But what we need finally is an algorithm that does this for us all this by itself and adjusts to any new points.<br><iframe src="https://www.desmos.com/calculator/smmkhlshap?embed" width="100%" height="300px" frameborder="0" allowfullscreen></iframe></p><h2 id="The-Search-for-a-Classifier"><a href="#The-Search-for-a-Classifier" class="headerlink" title="The Search for a Classifier"></a>The Search for a Classifier</h2><p>Let’s start with a random line in this space and then see what we must do, for creating the two regions. Let’s try it ourselves first. One way would be to rotate and translate the line until it separates the two regions. This seems intuitive enough. Play around with the interactive graph below to separate the two groups of points. Make sure the points are in their respective colored regions.</p><iframe src="https://www.desmos.com/calculator/cadms8e21r" width="100%" height="600px" frameborder="0" allowfullscreen></iframe><p>You would’ve noticed that $a$ and $b$ contribute to the rotation and $c$ contributes to translation. This makes sense because $a$ and $b$ control the slope and $c$ controls the distance from the origin. But how did you know when to stop fiddling around with those parameters? A trivial but useful answer would be to stop when there are no misclassified points.</p><p>At this point, we are good to develop a general algorithm for classifying two groups of points. We would do something like this</p><div class="note info"><pre><code>Initialize random line parameterswhile misclassified points are present:    fiddle around with the line parameters</code></pre></div><h2 id="The-perceptron-trick"><a href="#The-perceptron-trick" class="headerlink" title="The perceptron trick"></a>The perceptron trick</h2><p>Let’s start making the algorithm a little more concrete. Let the line be <strong>$ax_1 + bx_2 + c = 0$</strong>, where we initialize <strong>$a$</strong>, <strong>$b$</strong>, <strong>$c$</strong> at random. For classifying the points, let’s first give the points some labels(0 and 1) to differentiate them. Let $P(x_1, x_2)$ be a point in our data space with a label <strong>$L_P$</strong>. If <strong>$(ax_1+ bx_2 + c &gt; 0) = L_P$</strong> then <strong>$P$</strong>‘s classification is correct.</p><p>Now let’s deal with the fiddling part of the algorithm. Before we see any math on this, let’s try to build our intuition for it. Below are two cases</p><ul><li>case 1: <strong>$0$</strong> label point is misclassified.</li><li>case 2: <strong>$1$</strong> label point is misclassified.</li></ul><p>See if you can come up with a general rule for modifying <strong>$a$</strong>, <strong>$b$</strong> and <strong>$c$</strong> that leads to the correct classification of these points.</p><iframe src="https://www.desmos.com/calculator/684cghhob5" width="100%" height="600px" frameborder="0" allowfullscreen></iframe><iframe src="https://www.desmos.com/calculator/nxtk6rxo4z" width="100%" height="600px" frameborder="0" allowfullscreen></iframe><p>With a bit of effort, you can observe that the following pattern works for any situation</p><ul><li>increase parameters for a point labeled <strong>$1$</strong> and classified as <strong>$0$</strong>.</li><li>decrease parameters for a point labeled <strong>$0$</strong> and classified as <strong>$1$</strong>.</li></ul><p>Don’t worry if you haven’t found this out. Try it now and see if it works. The reason you might have missed the pattern is that you were trying to</p><ul><li>increase one parameter and decrease another.</li><li>increase or decrease the parameters by a random amount until it does the job.</li></ul><p>There’s nothing wrong with the above two operations because they do get the job done. But the problem is that we can’t generalize those operations for any given scenario. So, by this point, you should have a decent amount of intuition on how all this works. So, here’s how you can update the parameters. The amount by which they have to be updated is directly proportional to <strong>$x_1$</strong> for <strong>$a$</strong>, <strong>$x_2$</strong> for <strong>$b$</strong>. This translates to the following formula which works for both kinds of misclassification.<br></p><div class="note info"><p>For every misclassified point <strong>$P(x_1, x_2)$</strong>:<br><strong>$\hspace{1cm}a = a + \alpha (expected - predicted) x_1$</strong><br><strong>$\hspace{1cm}b = b + \alpha (expected - predicted) x_2$</strong><br><strong>$\hspace{1cm}c = c + \alpha (expected - predicted)$</strong></p></div><p></p><p>Here <strong>$\alpha$</strong> is the proportionality constant and is like a fine-tuning knob. It controls the rate at which we change these parameters, also known as the <em>“learning rate”</em>. We need our perceptron to change the parameters in a slow manner. Because large changes often tend to misclassify points that were correctly classified before. There aren’t any fixed good values for the learning rate. See for yourself how learning rate affects the speed and performance <a href="https://www.cs.utexas.edu/~teammco/misc/perceptron/" target="_blank" rel="noopener">here</a>.</p><p>In a Deep Learning setting the parameters $a$, $b$ are usually denoted by $w_1$, $w_2$ and are part of a vector $w$. $c$ is known as bias. The inputs and outputs of a neuron are as shown in the figure below.</p><p>$$z = w_0 + w_1x_1 + w_2x_2 + … + w_mx_m$$</p><p>$$<br>H(z) =<br>\begin{cases}<br>0 &amp; {n &lt; 0} \\<br>1 &amp; {n \geq 0}<br>\end{cases}<br>$$<br><img src="https://www.dropbox.com/s/j6vksnlibf0yhgq/perceptron.png?raw=1" alt="Perceptron"></p><h2 id="The-Perceptron-Algorithm"><a href="#The-Perceptron-Algorithm" class="headerlink" title="The Perceptron Algorithm"></a>The Perceptron Algorithm</h2><p>To wrap up this section, here’s the formal definition in a deep learning setting.</p><div class="note info"><p><strong>Data</strong>: Training Data:<strong>$(x_i , y_i )$</strong>; <strong>$\forall i \in {0, 1, 2, . . . , N }$</strong>, Learning Rate: <strong>$\eta$</strong>, where</p><ul><li>$x_i$ is a m-dimensional input vector and $N$ is the total number of instances of our data.</li><li>$x_{i, 0} = 1;$ $\forall i \in {0, 1, 2, . . . , N }$</li><li>${\hat y}_i$ is the prediction of a point <strong>$(x_i , y_i )$</strong></li></ul><p><strong>Result</strong>: Separating Hyper-plane coefficients :<strong>$w^∗$</strong><br><strong>Initialize</strong> <strong>$w$</strong> ← random weights ; (Since $x_{i, 0} = 1, w_0$ acts as the bias without setting it explicitly.)<br><strong>repeat</strong></p><p>get example <strong>$(x_i , y_i )$</strong>;<br><strong>$\hspace{1cm}\hat y_i ← w^T x_i $</strong>;<br><strong>$\hspace{1cm}w ← w + \eta(y_i − {\hat y}_i )x_i$</strong></p><p><strong>until</strong> convergence;</p></div><p>Here’s your dopamine shot for making it till the end.</p><p><img src="https://imgs.xkcd.com/comics/machine_learning.png" alt="XKCD comic"></p>]]></content>
      
      <categories>
          
          <category> AI </category>
          
          <category> Deep Learning Fundamentals </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Neural Networks </tag>
            
        </tags>
      
    </entry>
    
  
  
    
    <entry>
      <title>Hello World</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
    <entry>
      <title>Portfolio</title>
      <link href="/portfolio/index.html"/>
      <url>/portfolio/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
    <entry>
      <title>Categories</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
    <entry>
      <title>links</title>
      <link href="/resources/index.html"/>
      <url>/resources/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
    <entry>
      <title>Topics</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
    <entry>
      <title>timeline</title>
      <link href="/timeline/index.html"/>
      <url>/timeline/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
  
</search>
