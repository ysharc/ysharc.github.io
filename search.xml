<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>Forward and Backward Propagation</title>
      <link href="/forward-and-backward-propagation/"/>
      <url>/forward-and-backward-propagation/</url>
      <content type="html"><![CDATA[<p>Till now we’ve seen</p><ul><li><a href="/multilayer-perceptrons-and-activation-function/">How a Multilayer Perceptron works</a></li><li><a href="/cost-function-and-gradient-descent/">How Gradient Descent helps Multilayer Perceptron</a></li></ul><p>Now let’s see some tricks that extend these ideas to Deep neural networks.<br><a id="more"></a></p><h2 id="Neural-Network-terminology"><a href="#Neural-Network-terminology" class="headerlink" title="Neural Network terminology"></a>Neural Network terminology</h2><p>From the previous posts, you know that neural networks are good at classification. It does this in four steps</p><ol><li>Calculate the output of the current network.</li><li>Compare the output with the correct output.</li><li>Update weights of the network by Gradient Descent.</li><li>Repeat steps 1-3 until you achieve a good classifier.</li></ol><p>Before we proceed further it’s better to know some terminology and notations.</p><div class="note info"><ul><li><strong>Training</strong>: The whole process shown in the 4 steps above.</li><li><strong>instance</strong>: A single data point from the set of data points we’re trying to classify.</li><li><strong>Label</strong>: The correct/desired output of an instance.</li><li><strong>Prediction</strong>: Calculating the output for a given input using our current set of weights.</li></ul></div><h2 id="Some-standard-notations"><a href="#Some-standard-notations" class="headerlink" title="Some standard notations"></a>Some standard notations</h2><p>In the <a href="/multilayer-perceptrons-and-activation-function/">activation and MLP post</a> the final output looked something like this $a(Max(0, w_{01}+w_{11}x+w_{21}y)) + b(Max(0, w_{02}+w_{12}x+w_{22}y))+c &gt; 0$. Imagine what a network with hundreds or even thousands of perceptrons would produce. To simplify this, we can stick to a standard notation and abstract away all the cumbersome stuff. Let’s use the following neural network as a reference.<br>(Note: This notation is adapted from <a href="https://www.deeplearning.ai/" target="_blank" rel="noopener">Andrew Ng’s Deep Learning series</a> and tweaked a little.)</p><p><img src="https://www.dropbox.com/s/iws8e9brfxmaj23/neural_network_terminology.png?raw=1" alt="Neural Network Architecture"></p><h3 id="Representation-of-the-neural-network"><a href="#Representation-of-the-neural-network" class="headerlink" title="Representation of the neural network"></a>Representation of the neural network</h3><p>Let $L$ be the number of layers. Input is not considered a layer and is often called Layer 0. The hidden layers till the output are numbered from 1 to L. Denote the number of units in the $l^{th}$ layer by $n^{[l]}$. Notations of other units follow.</p><h3 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h3><p>We use $X/A^{[0]}$ to denote the input. In our network it will be</p><div class="note info"><p>$$X = A^{[0]} = \begin{bmatrix}<br>x_1 \\<br>x_2<br>\end{bmatrix}$$</p></div><h3 id="Weights-and-Bias"><a href="#Weights-and-Bias" class="headerlink" title="Weights and Bias"></a>Weights and Bias</h3><p>Consider the weights between input layer and the first hidden layer as shown below</p><p><img src="https://www.dropbox.com/s/ddyj4njtrru1nzq/weights_and_biases.png?raw=1" alt="Weights and Biases"></p><p>We denote the weights between $i^{th}$ and ${i-1}^{th}$ layer as $W^{[i]}$ and the bias before $i^{th}$ layer as $B^{[i]}$. So in the figure above $W^{[1]}$ and $B^{[1]}$ represent</p><div class="note info"><p>$$<br>W^{[1]} = \begin{bmatrix}<br>w_{11}^{[1]} &amp; w_{21}^{[1]} \\<br>w_{12}^{[1]} &amp; w_{22}^{[1]}<br>\end{bmatrix}<br>$$</p><p>$$<br>B^{[1]} = \begin{bmatrix}<br>b_1^{[1]} \\<br>b_2^{[1]}<br>\end{bmatrix}<br>$$</p></div><h3 id="Activations"><a href="#Activations" class="headerlink" title="Activations"></a>Activations</h3><p>Activations are a little tricky. Because two operations are performed in the a single activation unit. For the $i^{th}$ activation layer the operations are</p><ol><li>$Z^{[i]} = W^{[i]}A^{[i-1]} + B^{[i]}$</li><li>$A^{[i]} = g^{[i]}(Z^{[i]}), \hspace{0.5cm} \text{where g is the activation function.}$</li></ol><p>Let’s make this a little more concrete by considering the first activation layer.</p><div class="note info"><p>$$<br>Z^{[1]} = W^{[1]}A^{[0]} + B^{[1]}\\<br>\implies Z^{[1]} = \begin{bmatrix}<br>w_{11}^{[1]} &amp; w_{21}^{[1]} \\<br>w_{12}^{[1]} &amp; w_{22}^{[1]}<br>\end{bmatrix}\begin{bmatrix}<br>x_1 \\<br>x_2<br>\end{bmatrix} + \begin{bmatrix}<br>b_1^{[1]} \\<br>b_2^{[1]}<br>\end{bmatrix}<br>$$</p><p>$$<br>\text{Assuming the activation is ReLU}\\<br>A^{[1]} = ReLU(Z^{[1]})\\<br>\implies A^{[1]} = ReLU\left(\begin{bmatrix}<br>w_{11}^{[1]} &amp; w_{21}^{[1]} \\<br>w_{12}^{[1]} &amp; w_{22}^{[1]}<br>\end{bmatrix}\begin{bmatrix}<br>x_1 \\<br>x_2<br>\end{bmatrix} + \begin{bmatrix}<br>b_1^{[1]} \\<br>b_2^{[1]}<br>\end{bmatrix}\right)\\<br>\implies A^{[1]} = ReLU\left(\begin{bmatrix}<br>w_{11}^{[1]}x_1 + w_{21}^{[1]}x_2 + b_1^{[1]} \\<br>w_{12}^{[1]}x_1 + w_{22}^{[1]}x_2 + b_2^{[1]}<br>\end{bmatrix}\right)<br>$$</p></div><h3 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h3><p>The output is the last activation layer $A^{[L]}$. It is also denoted by $\hat{Y}$ and calculated by the same logic of the activation layer.</p><h2 id="Forward-Propagation"><a href="#Forward-Propagation" class="headerlink" title="Forward Propagation"></a>Forward Propagation</h2><p>The process of forward propagation is to calculate the outputs of each hidden layer. Setting $A^{[0]} = X$, we can start calculating the outputs as follows</p><p>$$<br>Z^{[1]} = W^{[1]}A^{[0]} + B^{[1]},\hspace{0.5cm} A^{[1]} = g^{[1]}(Z^{[1]}) \\<br>Z^{[2]} = W^{[2]}A^{[1]} + B^{[2]},\hspace{0.5cm} A^{[2]} = g^{[2]}(Z^{[2]}) \\<br>Z^{[3]} = W^{[3]}A^{[2]} + B^{[3]},\hspace{0.5cm} A^{[3]} = g^{[3]}(Z^{[3]}) \\<br>. \\<br>. \\<br>. \\<br>Z^{[L]} = W^{[L]}A^{[L-1]} + B^{[L]},\hspace{0.5cm} A^{[L]} = g^{[L]}(Z^{[L]})$$</p><p>In a nutshell, given $A^{[l-1]}$ feed forward produces the output $A^{[l]}$. The outputs are calculated in a linear way, hence the name feed forward networks.</p><div class="note info"><ul><li><strong>Feed Forward</strong>: The process of calculating outputs of each hidden layer for a given input.</li></ul></div><h2 id="Backward-Propagation"><a href="#Backward-Propagation" class="headerlink" title="Backward Propagation"></a>Backward Propagation</h2><p>At this stage we have our predicted labels. It is time for us to update the weights based on our error. To update our weights, we must first look at the gradient with respect to the weights. Use the gradients and move the weights in the direction that decreases our error. In the gradient descent post we took it for granted that our error function is a function of weights. But this is indirectly the case. In practice any error function calculates it’s metric by comparing two things</p><ul><li>the predicted output $\hat{Y}$ and</li><li>the desired output $Y$(label)</li></ul><p>So it’s fair to write $E$ is a function of $Y $ and $\hat{Y}$.</p><p>Remember that $\hat{Y}$ is dependent on all the weights and biases of the previous layer. So, we can easily show $E$ as a function of every weight layer.</p><p>$$<br>\begin{align}<br>E(Y, \hat{Y}) &amp;= E(Y, A^{[L]})\\<br>&amp;= E(Y, g^{[L]}(Z^{[L]})) \\<br>&amp;= E(Y, g^{[L]}(W^{[L]}A^{[L-1]} + B^{[L]})) \\<br>&amp;= E(Y, g^{[L]}(W^{[L]}g^{[L-1]}(Z^{[L-1]}) + B^{[L]})) \\<br>&amp;= E(Y, g^{[L]}(W^{[L]}g^{[L-1]}(W^{[L-1]}A^{[L-2]} + B^{[L-1]}) + B^{[L]})) \\<br>\end{align}<br>$$<br>expanding any further is sheer lunacy. But, you get the point that the error function can be written in terms of each weight layer. Now, we can calculate the gradient with respect to each weight layer.</p><h3 id="Chain-Rule"><a href="#Chain-Rule" class="headerlink" title="Chain Rule"></a>Chain Rule</h3><p>For obtaining the partial derivative, we have to differentiate this composite function. This is where the chain rule from calculus can help us. If a variable z depends on variable y, which in turn depends on the variable x then</p><p>$$\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} \frac{\partial y}{\partial x}$$</p><p>Using this we can write the derivative of $E$ with respect to $W^{[L]}$ as follows<br>$$<br>\begin{align}<br>\frac{\partial E}{\partial W^{[L]}} &amp;= \frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial W^{[L]}} \\<br>&amp;= \frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial W^{[L]}}\\<br>&amp;=\frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}} \frac{\partial Z^{[L]}}{\partial W^{[L]}}\\<br>&amp;=\underbrace{\frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}}}_{\frac{\partial E}{\partial Z^{[L]}}} A^{[L-1]}<br>\end{align}<br>$$</p><p>In a similar fashion<br>$$<br>\frac{\partial E}{\partial B^{[L]}} = \frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}} \frac{\partial Z^{[L]}}{\partial B^{[L]}}\\<br>\implies \frac{\partial E}{\partial B^{[L]}} = \frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}} = \frac{\partial E}{\partial Z^{[L]}}\\<br>$$</p><h3 id="Cache-the-previous-output"><a href="#Cache-the-previous-output" class="headerlink" title="Cache the previous output"></a>Cache the previous output</h3><p>This looks fine for the $L^{th}$ layer but doesn’t it get huge as we approach the derivative with respect to weight layer 1. Let’s see what happens at the ${L-1}^{th}$ derviative.</p><p>$$<br>\begin{align}<br>\frac{\partial E}{\partial W^{[L-1]}} &amp;= \frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial W^{[L-1]}} \\<br>&amp;= \frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial W^{[L-1]}}\\<br>&amp;=\frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}} \frac{\partial Z^{[L]}}{\partial W^{[L-1]}}\\<br>&amp;=\frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}} \frac{\partial Z^{[L]}}{\partial W^{[L-1]}} \\<br>&amp;=\frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}} \frac{\partial Z^{[L]}}{\partial A^{[L-1]}} \frac{\partial A^{[L-1]}}{\partial W^{[L-1]}} \\<br>&amp;=\frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}} \frac{\partial Z^{[L]}}{\partial A^{[L-1]}} \frac{\partial A^{[L-1]}}{\partial g^{[L-1]}} \frac{\partial g^{[L-1]}}{\partial W^{[L-1]}}\\<br>&amp;=\frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}} \frac{\partial Z^{[L]}}{\partial A^{[L-1]}} \frac{\partial A^{[L-1]}}{\partial g^{[L-1]}} \frac{\partial g^{[L-1]}}{\partial Z^{[L-1]}} \frac{\partial Z^{[L-1]}}{\partial W^{[L-1]}}\\<br>&amp;=\underbrace{\frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}} \frac{\partial Z^{[L]}}{\partial A^{[L-1]}} \frac{\partial A^{[L-1]}}{\partial g^{[L-1]}} \frac{\partial g^{[L-1]}}{\partial Z^{[L-1]}}}_{\frac{\partial E}{\partial Z^{[L-1]}}} A^{[L-2]}\\<br>\end{align}<br>$$</p><p>Notice while calculating $\frac{\partial E}{\partial W^{[L-1]}}$ we already had the result of $\frac{\partial E}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial g^{[L]}} \frac{\partial g^{[L]}}{\partial Z^{[L]}}$ from computing $\frac{\partial E}{\partial W^{[L]}}$. We can save this calculation from every previous layer and use it in our current computation. So, it won’t be messy when we get down to the last layer. In general going down a layer each time you’ll notice a pattern that<br><img src="https://www.dropbox.com/s/l15ikka0kx50w82/back_prop_steps.svg?raw=1" alt="BackPropagation trick"></p><h3 id="Updating-the-weights"><a href="#Updating-the-weights" class="headerlink" title="Updating the weights"></a>Updating the weights</h3><p>So we’ve come to the final part of this simple algorithm. Remember that we need to take small steps in the direction decreasing the error. Here $\alpha$, the learning rate does the job. As long as you don’t set the learning rate high, you should be fine. Finding a good learning rate is for another post.</p><p>Updating the weights is straightforward</p><p>$$W^{[i]} := W^{[i]} - \alpha \frac{\partial E}{\partial W^{[i]}}$$<br>$$B^{[i]} := B^{[i]} - \alpha \frac{\partial E}{\partial B^{[i]}}$$</p><div class="note info"><ul><li><strong>Backward Propagation</strong>: The process of calculating each weights’ contribution to the error and updating them accordingly.</li></ul></div><p>If you don’t get something from the post, relax! Take a step back and rework it again. Despite what you’ve heard, back propagation is as simple idea. Uses basic calculus and linear algebra. Ending this post on a relief note :)<br><img src="https://imgs.xkcd.com/comics/too_old_for_this_shit.png" alt="Too old for math"></p>]]></content>
      
      <categories>
          
          <category> AI </category>
          
          <category> Deep Learning Fundamentals </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Neural Networks </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Cost Function and Gradient Descent</title>
      <link href="/cost-function-and-gradient-descent/"/>
      <url>/cost-function-and-gradient-descent/</url>
      <content type="html"><![CDATA[<p>In the last <a href="/multilayer-perceptrons-and-activation-function">post</a> we’ve discussed how a Multilayer Perceptron works. Now let’s see how to search/update a set of weights to achieve a good classifer.<br><a id="more"></a></p><h2 id="Error-Function"><a href="#Error-Function" class="headerlink" title="Error Function"></a>Error Function</h2><p>We are on the search for the best set of weights for our classifier. Before we do that, we need something that tells us how good we are doing with the current set of weights. This is exactly what an <em><strong>error function</strong></em> AKA <em><strong>loss function</strong></em> AKA <em><strong>cost function</strong></em> does. It provides us a quantifiable metric depending on which we can update our set of weights. Let’s say our error function is $E(W)$ where W is the set of weights.</p><h2 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h2><p>Our goal now is to minimize $E(W)$ for the classifier to work. To do that, we have $W$ that we can use to change $E(W)$. Here is where a basic idea from calculus helps. A small change in the input of a function is related to the derivative in the following way</p><div class="note info"><p>$$f(x + \epsilon) \approx f(x) + \epsilon f’(x) $$</p></div><p>So, an $\epsilon$ change in the input produces a $\epsilon f’(x)$ change in the output. This is all we need to decrease our error function. Some math recap before we build error minimizing algorithm.<br>The derivative of a function at a point is</p><ul><li><strong>positive</strong> if the function is increasing in the neighborhood of the point.</li><li><strong>negative</strong> if it’s decreasing in the neighborhood of the point.</li><li><strong>zero</strong> if it’s neither increasing or decreasing.(saddle points)</li></ul><p>We can use this fact in our error function as shown below</p><div class="note info"><p>$$E(W+\epsilon) \approx E(W) + \epsilon E’(W) \hspace{1cm}\text{for a small enough}\hspace{0.2cm} \epsilon$$<br>$$\implies E(W+\epsilon) &lt; E(W)\hspace{1cm} if \hspace{0.2cm}E’(W) &lt; 0 $$<br>$$\implies E(W-\epsilon) &lt; E(W)\hspace{1cm} if \hspace{0.2cm}E’(W) &gt; 0 $$</p></div><p>So, all we have to do now is decrease the weights when derivate is positive and vice versa. If we repeat this operation with a small enough $\epsilon$ we can minimize the error function. This is the whole idea of <strong>Gradient Descent</strong>. A simple form for the algorithm is</p><div class="note info"><p>until we reach an error threshold or $E’(W) = 0$<br>$$W ← W - \epsilon sgn(E’(W))<br>\hspace{1cm}where\hspace{0.2cm}sgn(x) :=<br>\begin{cases}<br>-1 &amp; {x &lt; 0,} \\<br>0 &amp; {x = 0,} \\<br>1 &amp; {x &gt; 0.}<br>\end{cases}<br>$$</p></div><h2 id="Choosing-an-error-function"><a href="#Choosing-an-error-function" class="headerlink" title="Choosing an error function"></a>Choosing an error function</h2><p>The simple yet powerful algorithm depends on the error function to work. So, let’s explore how to choose an error function. In the <a href="/neuron-and-the-perceptron-algorithm/">perceptron post</a> we chose the number of misclassified points as our error function. But, <code># misclassified points</code> can only have integer values and is a discrete function. This is a poor choice for an error function, because</p><ul><li>In many situations you’ll find that the number of misclassified points can’t be decreased. The weights keep bouncing between a set of values.</li><li>Differentiating a discrete function.</li><li>All the misclassified points receive the same treatment. Points misclassified by a small margin aren’t any different from large error margins.</li></ul><p>To overcome these challenges we can emphasize on the following characteristics</p><ul><li>The error function must be <em><strong>continuous and differentiable</strong></em>.</li><li>Penalize misclassified points based on their error margins.</li></ul><p>Designing such function is for another post. Let’s see some limitations of gradient descent before moving to the next post.</p><h2 id="The-limitations-gradient-descent"><a href="#The-limitations-gradient-descent" class="headerlink" title="The limitations gradient descent"></a>The limitations gradient descent</h2><p>As long as $E’(W)$ is not zero, our weights keep updating and we’ll be approaching a minima in the error function. But what about the case when it’s zero, our algorithm terminates. But we don’t have to be at a minima when this happens. Points where $E’(W) = 0$ are called <strong>critical points</strong>. Three different scenarios for critical points are</p><ul><li><p><strong>Minima:</strong> Moving in any direction doesn’t decrease the function. <img src="https://www.dropbox.com/s/ekpujat5y46ey9z/minima.png?raw=1" alt="Minima figure"></p></li><li><p><strong>Maxima:</strong> Moving in any direction doesn’t increase the function. <img src="https://www.dropbox.com/s/iwq9jj1htsbak19/maxima.png?raw=1" alt="Maxima figure"></p></li><li><p><strong>Saddle Point:</strong> Moving in any direction doesn’t increase or decrease the function. <img src="https://www.dropbox.com/s/03wdjpjsih8tc2f/saddle.png?raw=1" alt="Saddle point figure"></p></li></ul><p>So our algorithm can stop at any of these situations. Even if we land on a minima it may not be the absolute lowest that is possible. When there are many local minima, we find ourselves stuck at the local minima. Different scenarios that can occur are shown in the figure below.<br><img src="https://www.dropbox.com/s/xapmmirt8xgmyt0/approximate_minimization.png?raw=1" alt="Approximate Minimization"></p><ol><li>Ideally, we would like to achieve this point (Global Minima), but it may not be possible.</li><li>This is at par with the global minima and is acceptable.</li><li>These points lead to poor performance.</li></ol><p>We’ll discuss tricks and tips to mitigate these limitations in another post. Here’s a little treat for making it till the end.</p><p><img src="https://imgs.xkcd.com/comics/listening.png" alt="Alexa joke"></p>]]></content>
      
      <categories>
          
          <category> AI </category>
          
          <category> Deep Learning Fundamentals </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Neural Networks </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Multilayer Perceptrons and Activation function</title>
      <link href="/multilayer-perceptrons-and-activation-function/"/>
      <url>/multilayer-perceptrons-and-activation-function/</url>
      <content type="html"><![CDATA[<h2 id="The-XOR-problem"><a href="#The-XOR-problem" class="headerlink" title="The XOR problem"></a>The XOR problem</h2><p>In the previous <a href="/deep-learning-fundamentals-neuron-and-the-perceptron-algorithm">post</a> you’ve seen how a perceptron works. Now let’s dive into some more interesting problems in deep learning. What follows is the classic XOR problem. Develop a method for the correct classification of the following points.</p><a id="more"></a><p><img src="https://www.dropbox.com/s/rks7mlcp2vubj3c/XOR_problem.png?raw=1" alt="XOR PROBLEM"></p><h2 id="Multi-Layer-Perceptron"><a href="#Multi-Layer-Perceptron" class="headerlink" title="Multi Layer Perceptron"></a>Multi Layer Perceptron</h2><p>It is clear that these points are linear inseparable. So, a perceptron fails at classifying these points. To classify these points it is clear that we need a non-linear boundary. If one perceptron can’t do the job, let’s try combining many of them. To keep it simple and the weights manageable, let’s use the architecture below.<br>(Note: Every layer stacked between the input and output layer is a hidden layer. We’ll see more about it later in this post.)</p><p><img src="https://www.dropbox.com/s/4ulft6rx1jnsrc7/mlp_without_activation.png?raw=1" alt="MLP without activation"></p><p>But this doesn’t work. Because, the whole output of the last unit is a linear function in $x$ and $y$. We can write the output of $h_3$ as<br>$h_3 = Ax + By + C$ where<br>$A = aw_{11} + bw_{12}$<br>$B = aw_{12} + bw_{22}$<br>$C = aw_{01} + bw_{02} + c$</p><h2 id="Activation-Function"><a href="#Activation-Function" class="headerlink" title="Activation Function"></a>Activation Function</h2><p>This is where an activation function comes to play. An activation function is a non-linear function applied to the end result of a perceptron. This introduces non-linearity to the perceptron and to the network.</p><p>Some common activation functions are</p><h3 id="ReLU-Rectified-Linear-Unit"><a href="#ReLU-Rectified-Linear-Unit" class="headerlink" title="ReLU (Rectified Linear Unit)"></a>ReLU (Rectified Linear Unit)</h3><p><strong><br>$$y = max(0, x)$$</strong><br><img src="https://www.dropbox.com/s/7evkd5143dsk8tu/relu.png?raw=1" alt="ReLU activation graph"></p><h3 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h3><p><strong><br>$$y =\frac{1}{1 + e^{-x}}$$</strong><br><img src="https://www.dropbox.com/s/wacrmulmj3b090i/sigmoid.png?raw=1" alt="Sigmoid activation graph"></p><h3 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h3><p><strong><br>$$y = \frac{ e^x - e^{-x}}{ e^x + e^{-x} }$$</strong><br><img src="https://www.dropbox.com/s/n8wm85xjqwav69t/tanh.png?raw=1" alt="Tanh activation graph"></p><p>Ok, so let’s apply “ReLU” to a perceptron and see what the response is. You can fiddle around with ReLU and become more familiar with it’s response <a href="https://www.desmos.com/calculator/ap5gwdoznu" target="_blank" rel="noopener">here</a></p><h2 id="Multi-Layer-Perceptron-Activated"><a href="#Multi-Layer-Perceptron-Activated" class="headerlink" title="Multi Layer Perceptron Activated"></a>Multi Layer Perceptron Activated</h2><p>Now let’s try activating the perceptrons in our network with relu. Our previous network will now be<br><img src="https://www.dropbox.com/s/n7m305d5noa5205/activation_and_mlp.png?raw=1" alt="MLP with activated perceptrons"></p><p><strong>Note:</strong> The final unit isn’t activated by relu, because we only need it to do binary classification. So we use a <a href="https://en.wikipedia.org/wiki/Heaviside_step_function" target="_blank" rel="noopener">Heaviside step function</a> instead. The weights $w_{01}, w_{02}, c$ are biases and are added implicitly and are not shown in the graph.</p><p>Now let’s fiddle around with the parameters we mentioned in the formulas above. Try to come up with a set of weights ($w_{01}, w_{11}, w_{21}, w_{02}, w_{12}, w_{22}, a, b, c$), so that you create a region that sepearates these points. If the graph below is too small you can work on it <a href="https://www.desmos.com/calculator/ggs1zd0ogl" target="_blank" rel="noopener">here</a>.</p><iframe src="https://www.desmos.com/calculator/ggs1zd0ogl" width="100%" height="600px" frameborder="0" allowfullscreen></iframe><h2 id="MLP-Terminology"><a href="#MLP-Terminology" class="headerlink" title="MLP Terminology"></a>MLP Terminology</h2><p>Hope you got an intuitive sense of how it works. Before we go further, let’s clear up some terminology. An MLP is a class of feed-forward neural network which we’ll know more about in a later post. In a feed-forward network</p><ul><li><p><strong>Hidden Layer</strong>: Any layer that doesn’t directly provide input or output is a hidden layer. The outputs and inputs of these layers are not directly visible or not a point of interest. Thus the name, <em>hidden layer</em>.</p></li><li><p><strong>Depth of a neural network</strong>: The depth of a neural network is the number of hidden layers plus one. Plus one because output weights can be modified. So by increasing the number of hidden layers, you’re increasing the depth of a neural network.</p></li><li><p><strong>Width of a hidden layer</strong>: The number of units in a hidden layer is also known as the width of the layer.</p></li></ul><p><img src="https://www.dropbox.com/s/q0c8n47q6xzb9hk/nn_depth_width.png?raw=1" alt="Neural network - Depth and Width"></p><h2 id="The-search-for-parameters"><a href="#The-search-for-parameters" class="headerlink" title="The search for parameters"></a>The search for parameters</h2><p>In the example above you came up with the set of weights that classify the XOR points. But, it’s not practical to search for a set of a weights, especially when we stack many more perceptrons. To understand why a naive search for a best set of weights doesn’t work, let’s consider a simple scenario. Let’s say we have a fully connected network(every node in a layer is connected to every node in the next layer.) and we have 3 hidden layers each with a 100 nodes. Input nodes are 2 and the output is a single node. So the number of parameters will be</p><ul><li><strong>between input and hidden layer 1:</strong> 2 x 100</li><li><strong>between hidden layer 1 and hidden layer 2:</strong> 100 x 100</li><li><strong>between hidden layer 2 and hidden layer 3:</strong> 100 x 100</li><li><strong>between hidden layer 3 and output:</strong> 100 x 1</li></ul><p>So, we end up with a total of 200 + 10000 + 10000 + 100 = <strong>20,300</strong> parameters. But those are just the number of parameters. Each parameter can have any arbitrary weight. But for the sake of a concrete example, let’s say we bound the weights between -100 and 100 with a step size of 0.01. So the number of possible values for each parameter is $(100 - (-100))/0.01$ = $20,000$. To find an ideal set of weights through naive search, you must search for one instance in 20,300 * 20,000 = 406,000,000. Searching naively through 406 million instances for a simple network is inefficient and unnecessary. This is where <strong>Stochastic Gradient Descent</strong> can make the search faster towards the goal. We’ll learn all about it in the next post.</p><p>Don’t forget your dopamine shot for making it till the end.<br><img src="https://imgs.xkcd.com/comics/computers_vs_humans.png" alt="xkcd - too cool to care"></p>]]></content>
      
      <categories>
          
          <category> AI </category>
          
          <category> Deep Learning Fundamentals </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Neural Networks </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Neuron and the Perceptron Algorithm</title>
      <link href="/neuron-and-the-perceptron-algorithm/"/>
      <url>/neuron-and-the-perceptron-algorithm/</url>
      <content type="html"><![CDATA[<p>This is the first part of the series <em>“Deep Learning Fundamentals”</em>. The goal of this series is to explore the mechanisms of artificial neural networks. The focus is more on presenting an intuitive way of understanding neural networks. So, you can expect an emphasis on how and why things work rather than what does the job. More often than not I’ll try to use simple math without focusing on notation. Let’s jump into the fundamental unit of most of the neural networks - “a neuron”.<br><a id="more"></a></p><h2 id="Neuron"><a href="#Neuron" class="headerlink" title="Neuron"></a>Neuron</h2><p>A neuron is nothing but a binary classifier. Which means given an n-dimensional space, it can divide the space into two regions. Let’s try to come up with an algorithm that does this, given that the data is linearly separable. For the sake of brevity, let’s consider that the data we deal with is numerical across all dimensions. (Dealing with other kinds of data is for another post.)</p><p>Without loss of generality let’s start working with two-dimensional data. So, let’s say there are some green(0) and blue(1) points on a plane and we want a line that separates these points. An intuitive way would be to draw a line like shown in the graph below. So far so good. But what we need finally is an algorithm that does this for us all this by itself and adjusts to any new points.<br><iframe src="https://www.desmos.com/calculator/smmkhlshap?embed" width="100%" height="300px" frameborder="0" allowfullscreen></iframe></p><h2 id="The-Search-for-a-Classifier"><a href="#The-Search-for-a-Classifier" class="headerlink" title="The Search for a Classifier"></a>The Search for a Classifier</h2><p>Let’s start with a random line in this space and then see what we must do, for creating the two regions. Let’s try it ourselves first. One way would be to rotate and translate the line until it separates the two regions. This seems intuitive enough. Play around with the interactive graph below to separate the two groups of points. Make sure the points are in their respective colored regions.</p><iframe src="https://www.desmos.com/calculator/cadms8e21r" width="100%" height="600px" frameborder="0" allowfullscreen></iframe><p>You would’ve noticed that $a$ and $b$ contribute to the rotation and $c$ contributes to translation. This makes sense because $a$ and $b$ control the slope and $c$ controls the distance from the origin. But how did you know when to stop fiddling around with those parameters? A trivial but useful answer would be to stop when there are no misclassified points.</p><p>At this point, we are good to develop a general algorithm for classifying two groups of points. We would do something like this</p><div class="note info"><pre><code>Initialize random line parameterswhile misclassified points are present:    fiddle around with the line parameters</code></pre></div><h2 id="The-perceptron-trick"><a href="#The-perceptron-trick" class="headerlink" title="The perceptron trick"></a>The perceptron trick</h2><p>Let’s start making the algorithm a little more concrete. Let the line be <strong>$ax_1 + bx_2 + c = 0$</strong>, where we initialize <strong>$a$</strong>, <strong>$b$</strong>, <strong>$c$</strong> at random. For classifying the points, let’s first give the points some labels(0 and 1) to differentiate them. Let $P(x_1, x_2)$ be a point in our data space with a label <strong>$L_P$</strong>. If <strong>$(ax_1+ bx_2 + c &gt; 0) = L_P$</strong> then <strong>$P$</strong>‘s classification is correct.</p><p>Now let’s deal with the fiddling part of the algorithm. Before we see any math on this, let’s try to build our intuition for it. Below are two cases</p><ul><li>case 1: <strong>$0$</strong> label point is misclassified.</li><li>case 2: <strong>$1$</strong> label point is misclassified.</li></ul><p>See if you can come up with a general rule for modifying <strong>$a$</strong>, <strong>$b$</strong> and <strong>$c$</strong> that leads to the correct classification of these points.</p><iframe src="https://www.desmos.com/calculator/684cghhob5" width="100%" height="600px" frameborder="0" allowfullscreen></iframe><iframe src="https://www.desmos.com/calculator/nxtk6rxo4z" width="100%" height="600px" frameborder="0" allowfullscreen></iframe><p>With a bit of effort, you can observe that the following pattern works for any situation</p><ul><li>increase parameters for a point labeled <strong>$1$</strong> and classified as <strong>$0$</strong>.</li><li>decrease parameters for a point labeled <strong>$0$</strong> and classified as <strong>$1$</strong>.</li></ul><p>Don’t worry if you haven’t found this out. Try it now and see if it works. The reason you might have missed the pattern is that you were trying to</p><ul><li>increase one parameter and decrease another.</li><li>increase or decrease the parameters by a random amount until it does the job.</li></ul><p>There’s nothing wrong with the above two operations because they do get the job done. But the problem is that we can’t generalize those operations for any given scenario. So, by this point, you should have a decent amount of intuition on how all this works. So, here’s how you can update the parameters. The amount by which they have to be updated is directly proportional to <strong>$x_1$</strong> for <strong>$a$</strong>, <strong>$x_2$</strong> for <strong>$b$</strong>. This translates to the following formula which works for both kinds of misclassification.<br></p><div class="note info"><p>For every misclassified point <strong>$P(x_1, x_2)$</strong>:<br><strong>$\hspace{1cm}a = a + \alpha (expected - predicted) x_1$</strong><br><strong>$\hspace{1cm}b = b + \alpha (expected - predicted) x_2$</strong><br><strong>$\hspace{1cm}c = c + \alpha (expected - predicted)$</strong></p></div><p></p><p>Here <strong>$\alpha$</strong> is the proportionality constant and is like a fine-tuning knob. It controls the rate at which we change these parameters, also known as the <em>“learning rate”</em>. We need our perceptron to change the parameters in a slow manner. Because large changes often tend to misclassify points that were correctly classified before. There aren’t any fixed good values for the learning rate. See for yourself how learning rate affects the speed and performance <a href="https://www.cs.utexas.edu/~teammco/misc/perceptron/" target="_blank" rel="noopener">here</a>.</p><p>In a Deep Learning setting the parameters $a$, $b$ are usually denoted by $w_1$, $w_2$ and are part of a vector $w$. $c$ is known as bias. The inputs and outputs of a neuron are as shown in the figure below.</p><p>$$z = w_0 + w_1x_1 + w_2x_2 + … + w_mx_m$$</p><p>$$<br>H(z) =<br>\begin{cases}<br>0 &amp; {n &lt; 0} \\<br>1 &amp; {n \geq 0}<br>\end{cases}<br>$$<br><img src="https://www.dropbox.com/s/j6vksnlibf0yhgq/perceptron.png?raw=1" alt="Perceptron"></p><h2 id="The-Perceptron-Algorithm"><a href="#The-Perceptron-Algorithm" class="headerlink" title="The Perceptron Algorithm"></a>The Perceptron Algorithm</h2><p>To wrap up this section, here’s the formal definition in a deep learning setting.</p><div class="note info"><p><strong>Data</strong>: Training Data:<strong>$(x_i , y_i )$</strong>; <strong>$\forall i \in {0, 1, 2, . . . , N }$</strong>, Learning Rate: <strong>$\eta$</strong>, where</p><ul><li>$x_i$ is a m-dimensional input vector and $N$ is the total number of instances of our data.</li><li>$x_{i, 0} = 1;$ $\forall i \in {0, 1, 2, . . . , N }$</li><li>${\hat y}_i$ is the prediction of a point <strong>$(x_i , y_i )$</strong></li></ul><p><strong>Result</strong>: Separating Hyper-plane coefficients :<strong>$w^∗$</strong><br><strong>Initialize</strong> <strong>$w$</strong> ← random weights ; (Since $x_{i, 0} = 1, w_0$ acts as the bias without setting it explicitly.)<br><strong>repeat</strong></p><p>get example <strong>$(x_i , y_i )$</strong>;<br><strong>$\hspace{1cm}\hat y_i ← w^T x_i $</strong>;<br><strong>$\hspace{1cm}w ← w + \eta(y_i − {\hat y}_i )x_i$</strong></p><p><strong>until</strong> convergence;</p></div><p>Here’s your dopamine shot for making it till the end.</p><p><img src="https://imgs.xkcd.com/comics/machine_learning.png" alt="XKCD comic"></p>]]></content>
      
      <categories>
          
          <category> AI </category>
          
          <category> Deep Learning Fundamentals </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Neural Networks </tag>
            
        </tags>
      
    </entry>
    
  
  
    
    <entry>
      <title>Hello World</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
    <entry>
      <title>Categories</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
    <entry>
      <title>Portfolio</title>
      <link href="/portfolio/index.html"/>
      <url>/portfolio/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
    <entry>
      <title>links</title>
      <link href="/resources/index.html"/>
      <url>/resources/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
    <entry>
      <title>Topics</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
    <entry>
      <title>timeline</title>
      <link href="/timeline/index.html"/>
      <url>/timeline/index.html</url>
      <content type="html"><![CDATA[]]></content>
    </entry>
    
  
</search>
