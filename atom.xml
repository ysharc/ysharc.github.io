<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>All Matters AI</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://blog.allmattersai.com/"/>
  <updated>2018-06-06T05:14:01.555Z</updated>
  <id>https://blog.allmattersai.com/</id>
  
  <author>
    <name>Yeshwanth Arcot</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Regularization</title>
    <link href="https://blog.allmattersai.com/regularization/"/>
    <id>https://blog.allmattersai.com/regularization/</id>
    <published>2018-06-06T00:42:04.000Z</published>
    <updated>2018-06-06T05:14:01.555Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;In the last &lt;a href=&quot;/overfitting-and-underfitting/&quot;&gt;post&lt;/a&gt; on overfitting and underfitting. We’ve seen that to achieve a perfect classifier is a tedious task. In this post let’s see some common tricks and methods to control the generalization of a network.&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="https://blog.allmattersai.com/categories/AI/"/>
    
      <category term="Deep Learning Fundamentals" scheme="https://blog.allmattersai.com/categories/AI/Deep-Learning-Fundamentals/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.allmattersai.com/tags/Deep-Learning/"/>
    
      <category term="Neural Networks" scheme="https://blog.allmattersai.com/tags/Neural-Networks/"/>
    
  </entry>
  
  <entry>
    <title>Overfitting and Underfitting</title>
    <link href="https://blog.allmattersai.com/overfitting-and-underfitting/"/>
    <id>https://blog.allmattersai.com/overfitting-and-underfitting/</id>
    <published>2018-06-02T00:42:04.000Z</published>
    <updated>2018-06-04T07:37:52.210Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Just to recap what we’ve seen so far,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;/neuron-and-the-perceptron-algorithm/&quot;&gt;Part 1: Neuron and the perceptron algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/multilayer-perceptrons-and-activation-function/&quot;&gt;Part 2: Multilayer Perceptrons and Activation Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/cost-function-and-gradient-descent/&quot;&gt;Part 3: Cost Function and Gradient Descent&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/forward-and-backward-propagation/&quot;&gt;Part 4: Forward and Backward Propagation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/loss-function-and-cross-entropy/&quot;&gt;Part 5: Loss function and Cross-entropy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="AI" scheme="https://blog.allmattersai.com/categories/AI/"/>
    
      <category term="Deep Learning Fundamentals" scheme="https://blog.allmattersai.com/categories/AI/Deep-Learning-Fundamentals/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.allmattersai.com/tags/Deep-Learning/"/>
    
      <category term="Neural Networks" scheme="https://blog.allmattersai.com/tags/Neural-Networks/"/>
    
      <category term="common problems" scheme="https://blog.allmattersai.com/tags/common-problems/"/>
    
  </entry>
  
  <entry>
    <title>Loss function and Cross Entropy</title>
    <link href="https://blog.allmattersai.com/loss-function-and-cross-entropy/"/>
    <id>https://blog.allmattersai.com/loss-function-and-cross-entropy/</id>
    <published>2018-05-30T00:42:04.000Z</published>
    <updated>2018-05-30T06:27:08.580Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;In the &lt;a href=&quot;/cost-function-and-gradient-descent&quot;&gt;gradient descent post&lt;/a&gt;, you’ve seen what an error function is. What the characteristics of a good loss function are. Let’s take a deep dive into how we achieve this.&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="https://blog.allmattersai.com/categories/AI/"/>
    
      <category term="Deep Learning Fundamentals" scheme="https://blog.allmattersai.com/categories/AI/Deep-Learning-Fundamentals/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.allmattersai.com/tags/Deep-Learning/"/>
    
      <category term="Neural Networks" scheme="https://blog.allmattersai.com/tags/Neural-Networks/"/>
    
  </entry>
  
  <entry>
    <title>Forward and Backward Propagation</title>
    <link href="https://blog.allmattersai.com/forward-and-backward-propagation/"/>
    <id>https://blog.allmattersai.com/forward-and-backward-propagation/</id>
    <published>2018-05-26T00:42:04.000Z</published>
    <updated>2018-05-28T05:30:29.961Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Till now we’ve seen&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;/multilayer-perceptrons-and-activation-function/&quot;&gt;How a Multilayer Perceptron works&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/cost-function-and-gradient-descent/&quot;&gt;How Gradient Descent helps Multilayer Perceptron&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now let’s see some tricks that extend these ideas to Deep neural networks.&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="https://blog.allmattersai.com/categories/AI/"/>
    
      <category term="Deep Learning Fundamentals" scheme="https://blog.allmattersai.com/categories/AI/Deep-Learning-Fundamentals/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.allmattersai.com/tags/Deep-Learning/"/>
    
      <category term="Neural Networks" scheme="https://blog.allmattersai.com/tags/Neural-Networks/"/>
    
  </entry>
  
  <entry>
    <title>Cost Function and Gradient Descent</title>
    <link href="https://blog.allmattersai.com/cost-function-and-gradient-descent/"/>
    <id>https://blog.allmattersai.com/cost-function-and-gradient-descent/</id>
    <published>2018-05-21T03:39:54.000Z</published>
    <updated>2018-05-24T06:23:55.185Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;In the last &lt;a href=&quot;/multilayer-perceptrons-and-activation-function&quot;&gt;post&lt;/a&gt; we’ve discussed how a Multilayer Perceptron works. Now let’s see how to search/update a set of weights to achieve a good classifer.&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="https://blog.allmattersai.com/categories/AI/"/>
    
      <category term="Deep Learning Fundamentals" scheme="https://blog.allmattersai.com/categories/AI/Deep-Learning-Fundamentals/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.allmattersai.com/tags/Deep-Learning/"/>
    
      <category term="Neural Networks" scheme="https://blog.allmattersai.com/tags/Neural-Networks/"/>
    
  </entry>
  
  <entry>
    <title>Multilayer Perceptrons and Activation function</title>
    <link href="https://blog.allmattersai.com/multilayer-perceptrons-and-activation-function/"/>
    <id>https://blog.allmattersai.com/multilayer-perceptrons-and-activation-function/</id>
    <published>2018-04-09T05:42:04.000Z</published>
    <updated>2018-05-19T01:44:28.670Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;The-XOR-problem&quot;&gt;&lt;a href=&quot;#The-XOR-problem&quot; class=&quot;headerlink&quot; title=&quot;The XOR problem&quot;&gt;&lt;/a&gt;The XOR problem&lt;/h2&gt;&lt;p&gt;In the previous &lt;a href=&quot;/deep-learning-fundamentals-neuron-and-the-perceptron-algorithm&quot;&gt;post&lt;/a&gt; you’ve seen how a perceptron works. Now let’s dive into some more interesting problems in deep learning. What follows is the classic XOR problem. Develop a method for the correct classification of the following points.&lt;/p&gt;
    
    </summary>
    
      <category term="AI" scheme="https://blog.allmattersai.com/categories/AI/"/>
    
      <category term="Deep Learning Fundamentals" scheme="https://blog.allmattersai.com/categories/AI/Deep-Learning-Fundamentals/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.allmattersai.com/tags/Deep-Learning/"/>
    
      <category term="Neural Networks" scheme="https://blog.allmattersai.com/tags/Neural-Networks/"/>
    
  </entry>
  
  <entry>
    <title>Neuron and the Perceptron Algorithm</title>
    <link href="https://blog.allmattersai.com/neuron-and-the-perceptron-algorithm/"/>
    <id>https://blog.allmattersai.com/neuron-and-the-perceptron-algorithm/</id>
    <published>2018-03-28T04:10:04.000Z</published>
    <updated>2018-05-24T06:19:07.946Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;This is the first part of the series &lt;em&gt;“Deep Learning Fundamentals”&lt;/em&gt;. The goal of this series is to explore the mechanisms of artificial neural networks. The focus is more on presenting an intuitive way of understanding neural networks. So, you can expect an emphasis on how and why things work rather than what does the job. More often than not I’ll try to use simple math without focusing on notation. Let’s jump into the fundamental unit of most of the neural networks - “a neuron”.&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="https://blog.allmattersai.com/categories/AI/"/>
    
      <category term="Deep Learning Fundamentals" scheme="https://blog.allmattersai.com/categories/AI/Deep-Learning-Fundamentals/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.allmattersai.com/tags/Deep-Learning/"/>
    
      <category term="Neural Networks" scheme="https://blog.allmattersai.com/tags/Neural-Networks/"/>
    
  </entry>
  
</feed>
