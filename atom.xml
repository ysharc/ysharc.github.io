<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>All Matters AI</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://blog.allmattersai.com/"/>
  <updated>2018-07-03T00:12:13.295Z</updated>
  <id>https://blog.allmattersai.com/</id>
  
  <author>
    <name>Yeshwanth Arcot</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Nadam</title>
    <link href="https://blog.allmattersai.com/nadam/"/>
    <id>https://blog.allmattersai.com/nadam/</id>
    <published>2018-07-30T00:42:04.000Z</published>
    <updated>2018-07-03T00:12:13.295Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Momentum-and-Nesterov-Accelerated-Gradient&quot;&gt;&lt;a href=&quot;#Momentum-and-Nesterov-Accelerated-Gradient&quot; class=&quot;headerlink&quot; title=&quot;Momentum and Nesterov Accelerated Gradient&quot;&gt;&lt;/a&gt;Momentum and Nesterov Accelerated Gradient&lt;/h2&gt;&lt;p&gt;We know that momentum updates the weights in the following way&lt;/p&gt;&lt;p&gt;$$&lt;br&gt;g_t \leftarrow \dfrac{\partial E}{\partial W_t}\\&lt;br&gt;V \leftarrow \beta V + \epsilon g_t\\&lt;br&gt;W \leftarrow W - V\\&lt;br&gt;$$&lt;/p&gt;&lt;p&gt;Writing the update in one step gives us&lt;/p&gt;&lt;p&gt;$$&lt;br&gt;W_{t+1} \leftarrow W_{t} - (\beta V_{t-1} + \epsilon g_t)&lt;br&gt;$$&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="https://blog.allmattersai.com/categories/AI/"/>
    
      <category term="Deep Learning Fundamentals" scheme="https://blog.allmattersai.com/categories/AI/Deep-Learning-Fundamentals/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.allmattersai.com/tags/Deep-Learning/"/>
    
      <category term="Neural Networks" scheme="https://blog.allmattersai.com/tags/Neural-Networks/"/>
    
      <category term="Momentum Mini Series" scheme="https://blog.allmattersai.com/tags/Momentum-Mini-Series/"/>
    
  </entry>
  
  <entry>
    <title>AdaMax</title>
    <link href="https://blog.allmattersai.com/adamax/"/>
    <id>https://blog.allmattersai.com/adamax/</id>
    <published>2018-06-30T00:42:04.000Z</published>
    <updated>2018-06-30T00:02:25.936Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;A-quick-tweak-to-Adam&quot;&gt;&lt;a href=&quot;#A-quick-tweak-to-Adam&quot; class=&quot;headerlink&quot; title=&quot;A quick tweak to Adam&quot;&gt;&lt;/a&gt;A quick tweak to Adam&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;AdaMax&lt;/strong&gt; is a variant of Adam. It uses an infinity norm to normalize $\hat{m_t}$ instead of the $L^2$ norm of individual current and past gradients used in Adam. So the new steps for updating weights is as follows&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="https://blog.allmattersai.com/categories/AI/"/>
    
      <category term="Deep Learning Fundamentals" scheme="https://blog.allmattersai.com/categories/AI/Deep-Learning-Fundamentals/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.allmattersai.com/tags/Deep-Learning/"/>
    
      <category term="Neural Networks" scheme="https://blog.allmattersai.com/tags/Neural-Networks/"/>
    
      <category term="Momentum Mini Series" scheme="https://blog.allmattersai.com/tags/Momentum-Mini-Series/"/>
    
  </entry>
  
  <entry>
    <title>Adam</title>
    <link href="https://blog.allmattersai.com/adam/"/>
    <id>https://blog.allmattersai.com/adam/</id>
    <published>2018-06-28T00:42:04.000Z</published>
    <updated>2018-07-02T23:48:38.151Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;First-and-Second-Moments&quot;&gt;&lt;a href=&quot;#First-and-Second-Moments&quot; class=&quot;headerlink&quot; title=&quot;First and Second Moments&quot;&gt;&lt;/a&gt;First and Second Moments&lt;/h2&gt;&lt;p&gt;In contrast to what we’ve seen so far, &lt;strong&gt;Adam&lt;/strong&gt; relies on two variables for changing the learning rate. They are $m_t$, $v_t$ and are initialized by zeros at the start. They are updated in the following way&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="https://blog.allmattersai.com/categories/AI/"/>
    
      <category term="Deep Learning Fundamentals" scheme="https://blog.allmattersai.com/categories/AI/Deep-Learning-Fundamentals/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.allmattersai.com/tags/Deep-Learning/"/>
    
      <category term="Neural Networks" scheme="https://blog.allmattersai.com/tags/Neural-Networks/"/>
    
      <category term="Momentum Mini Series" scheme="https://blog.allmattersai.com/tags/Momentum-Mini-Series/"/>
    
  </entry>
  
  <entry>
    <title>RMSProp</title>
    <link href="https://blog.allmattersai.com/rmsprop/"/>
    <id>https://blog.allmattersai.com/rmsprop/</id>
    <published>2018-06-26T00:42:04.000Z</published>
    <updated>2018-06-25T23:46:41.040Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Exponentially-Decaying-average&quot;&gt;&lt;a href=&quot;#Exponentially-Decaying-average&quot; class=&quot;headerlink&quot; title=&quot;Exponentially Decaying average&quot;&gt;&lt;/a&gt;Exponentially Decaying average&lt;/h2&gt;&lt;p&gt;The idea of RMSProp is similar to what you saw in the AdaDelta’s first step. It maintains a exponentially decaying average of the squared gradients.&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="https://blog.allmattersai.com/categories/AI/"/>
    
      <category term="Deep Learning Fundamentals" scheme="https://blog.allmattersai.com/categories/AI/Deep-Learning-Fundamentals/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.allmattersai.com/tags/Deep-Learning/"/>
    
      <category term="Neural Networks" scheme="https://blog.allmattersai.com/tags/Neural-Networks/"/>
    
      <category term="Momentum Mini Series" scheme="https://blog.allmattersai.com/tags/Momentum-Mini-Series/"/>
    
  </entry>
  
  <entry>
    <title>AdaDelta</title>
    <link href="https://blog.allmattersai.com/adadelta/"/>
    <id>https://blog.allmattersai.com/adadelta/</id>
    <published>2018-06-24T00:42:04.000Z</published>
    <updated>2018-06-25T00:07:19.093Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Average-over-a-window&quot;&gt;&lt;a href=&quot;#Average-over-a-window&quot; class=&quot;headerlink&quot; title=&quot;Average over a window&quot;&gt;&lt;/a&gt;Average over a window&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;AdaDelta&lt;/strong&gt; corrects the decreasing gradient problem caused by AdaGrad. It does this with a simple idea.
    
    </summary>
    
      <category term="AI" scheme="https://blog.allmattersai.com/categories/AI/"/>
    
      <category term="Deep Learning Fundamentals" scheme="https://blog.allmattersai.com/categories/AI/Deep-Learning-Fundamentals/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.allmattersai.com/tags/Deep-Learning/"/>
    
      <category term="Neural Networks" scheme="https://blog.allmattersai.com/tags/Neural-Networks/"/>
    
      <category term="Momentum Mini Series" scheme="https://blog.allmattersai.com/tags/Momentum-Mini-Series/"/>
    
  </entry>
  
  <entry>
    <title>AdaGrad</title>
    <link href="https://blog.allmattersai.com/adagrad/"/>
    <id>https://blog.allmattersai.com/adagrad/</id>
    <published>2018-06-22T00:42:04.000Z</published>
    <updated>2018-06-22T05:59:00.879Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Recognizing-Important-Features&quot;&gt;&lt;a href=&quot;#Recognizing-Important-Features&quot; class=&quot;headerlink&quot; title=&quot;Recognizing Important Features&quot;&gt;&lt;/a&gt;Recognizing Important Features&lt;/h2&gt;&lt;p&gt;The idea of &lt;strong&gt;AdaGrad&lt;/strong&gt; is a simple one. Normalize the gradient updates by the sum of the gradients so far. Suppose we run the training for t steps then the weight update for the t+1 step is as follows&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="https://blog.allmattersai.com/categories/AI/"/>
    
      <category term="Deep Learning Fundamentals" scheme="https://blog.allmattersai.com/categories/AI/Deep-Learning-Fundamentals/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.allmattersai.com/tags/Deep-Learning/"/>
    
      <category term="Neural Networks" scheme="https://blog.allmattersai.com/tags/Neural-Networks/"/>
    
      <category term="Momentum Mini Series" scheme="https://blog.allmattersai.com/tags/Momentum-Mini-Series/"/>
    
  </entry>
  
  <entry>
    <title>Nesterov Accelerated Gradient</title>
    <link href="https://blog.allmattersai.com/nesterov-accelerated-gradient/"/>
    <id>https://blog.allmattersai.com/nesterov-accelerated-gradient/</id>
    <published>2018-06-20T00:42:04.000Z</published>
    <updated>2018-07-03T00:12:48.244Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Correction-Factor&quot;&gt;&lt;a href=&quot;#Correction-Factor&quot; class=&quot;headerlink&quot; title=&quot;Correction Factor&quot;&gt;&lt;/a&gt;Correction Factor&lt;/h2&gt;&lt;p&gt;While using momentum, we updated $V$ and then applied part of $V$ to the weight update. The idea of Nesterov Accelerated Gradient(NAG) is to update the current weight by a small part of $V$ and then calculate the gradient of the updated weight. The small part by which we update the current weight is called the &lt;strong&gt;correction factor&lt;/strong&gt; which is $\alpha V$. Simply put, we changed the position where the gradient is calculated. So for each iteration in gradient descent, the updates would be as follows&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="https://blog.allmattersai.com/categories/AI/"/>
    
      <category term="Deep Learning Fundamentals" scheme="https://blog.allmattersai.com/categories/AI/Deep-Learning-Fundamentals/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.allmattersai.com/tags/Deep-Learning/"/>
    
      <category term="Neural Networks" scheme="https://blog.allmattersai.com/tags/Neural-Networks/"/>
    
      <category term="Momentum Mini Series" scheme="https://blog.allmattersai.com/tags/Momentum-Mini-Series/"/>
    
  </entry>
  
  <entry>
    <title>Momentum</title>
    <link href="https://blog.allmattersai.com/momentum/"/>
    <id>https://blog.allmattersai.com/momentum/</id>
    <published>2018-06-15T00:42:04.000Z</published>
    <updated>2018-06-18T14:00:59.162Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Gradient Descent promises us to take a step closer to a minima (local or global) on each iteration. But due to the high dimensionality, it’s dealing with a very complicated error surface. This introduces a problem of very slow convergence towards the minima. Let’s see a simple yet effective technique to overcome this problem.&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="https://blog.allmattersai.com/categories/AI/"/>
    
      <category term="Deep Learning Fundamentals" scheme="https://blog.allmattersai.com/categories/AI/Deep-Learning-Fundamentals/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.allmattersai.com/tags/Deep-Learning/"/>
    
      <category term="Neural Networks" scheme="https://blog.allmattersai.com/tags/Neural-Networks/"/>
    
  </entry>
  
  <entry>
    <title>Optimizing Gradient Descent</title>
    <link href="https://blog.allmattersai.com/optimizing-gradient-descent/"/>
    <id>https://blog.allmattersai.com/optimizing-gradient-descent/</id>
    <published>2018-06-12T00:42:04.000Z</published>
    <updated>2018-06-14T11:22:00.737Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;The Gradient descent algorithm we saw in an &lt;a href=&quot;/cost-function-and-gradient-descent&quot;&gt;earlier post&lt;/a&gt; just gives you an idea of how gradient descent works. We’ve seen that &lt;/p&gt;
&lt;p&gt;$$W \leftarrow W - \alpha E’(W)\\&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;This is just a rough formula to get you started. In practice what $E’(W)$ is calculating is the average of all the gradients due to every point in your training data. This is known as Vanilla Gradient Descent or Batch Gradient Descent. Large datasets with millions of instances are easily available nowadays. This presents a challenge to the vanilla gradient descent. The weights are updated after calculating the average of all the data points available, which significantly increases training time.&lt;/p&gt;
&lt;p&gt;Let’s see some optimizations to keep these under control.&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="https://blog.allmattersai.com/categories/AI/"/>
    
      <category term="Deep Learning Fundamentals" scheme="https://blog.allmattersai.com/categories/AI/Deep-Learning-Fundamentals/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.allmattersai.com/tags/Deep-Learning/"/>
    
      <category term="Neural Networks" scheme="https://blog.allmattersai.com/tags/Neural-Networks/"/>
    
  </entry>
  
  <entry>
    <title>Regularization</title>
    <link href="https://blog.allmattersai.com/regularization/"/>
    <id>https://blog.allmattersai.com/regularization/</id>
    <published>2018-06-06T00:42:04.000Z</published>
    <updated>2018-06-06T05:14:01.555Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;In the last &lt;a href=&quot;/overfitting-and-underfitting/&quot;&gt;post&lt;/a&gt; on overfitting and underfitting. We’ve seen that to achieve a perfect classifier is a tedious task. In this post let’s see some common tricks and methods to control the generalization of a network.&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="https://blog.allmattersai.com/categories/AI/"/>
    
      <category term="Deep Learning Fundamentals" scheme="https://blog.allmattersai.com/categories/AI/Deep-Learning-Fundamentals/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.allmattersai.com/tags/Deep-Learning/"/>
    
      <category term="Neural Networks" scheme="https://blog.allmattersai.com/tags/Neural-Networks/"/>
    
  </entry>
  
  <entry>
    <title>Overfitting and Underfitting</title>
    <link href="https://blog.allmattersai.com/overfitting-and-underfitting/"/>
    <id>https://blog.allmattersai.com/overfitting-and-underfitting/</id>
    <published>2018-06-02T00:42:04.000Z</published>
    <updated>2018-06-04T07:37:52.210Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Just to recap what we’ve seen so far,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;/neuron-and-the-perceptron-algorithm/&quot;&gt;Part 1: Neuron and the perceptron algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/multilayer-perceptrons-and-activation-function/&quot;&gt;Part 2: Multilayer Perceptrons and Activation Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/cost-function-and-gradient-descent/&quot;&gt;Part 3: Cost Function and Gradient Descent&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/forward-and-backward-propagation/&quot;&gt;Part 4: Forward and Backward Propagation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/loss-function-and-cross-entropy/&quot;&gt;Part 5: Loss function and Cross-entropy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="AI" scheme="https://blog.allmattersai.com/categories/AI/"/>
    
      <category term="Deep Learning Fundamentals" scheme="https://blog.allmattersai.com/categories/AI/Deep-Learning-Fundamentals/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.allmattersai.com/tags/Deep-Learning/"/>
    
      <category term="Neural Networks" scheme="https://blog.allmattersai.com/tags/Neural-Networks/"/>
    
      <category term="common problems" scheme="https://blog.allmattersai.com/tags/common-problems/"/>
    
  </entry>
  
  <entry>
    <title>Loss function and Cross Entropy</title>
    <link href="https://blog.allmattersai.com/loss-function-and-cross-entropy/"/>
    <id>https://blog.allmattersai.com/loss-function-and-cross-entropy/</id>
    <published>2018-05-30T00:42:04.000Z</published>
    <updated>2018-05-30T06:27:08.580Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;In the &lt;a href=&quot;/cost-function-and-gradient-descent&quot;&gt;gradient descent post&lt;/a&gt;, you’ve seen what an error function is. What the characteristics of a good loss function are. Let’s take a deep dive into how we achieve this.&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="https://blog.allmattersai.com/categories/AI/"/>
    
      <category term="Deep Learning Fundamentals" scheme="https://blog.allmattersai.com/categories/AI/Deep-Learning-Fundamentals/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.allmattersai.com/tags/Deep-Learning/"/>
    
      <category term="Neural Networks" scheme="https://blog.allmattersai.com/tags/Neural-Networks/"/>
    
  </entry>
  
  <entry>
    <title>Forward and Backward Propagation</title>
    <link href="https://blog.allmattersai.com/forward-and-backward-propagation/"/>
    <id>https://blog.allmattersai.com/forward-and-backward-propagation/</id>
    <published>2018-05-26T00:42:04.000Z</published>
    <updated>2018-05-28T05:30:29.961Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Till now we’ve seen&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;/multilayer-perceptrons-and-activation-function/&quot;&gt;How a Multilayer Perceptron works&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;/cost-function-and-gradient-descent/&quot;&gt;How Gradient Descent helps Multilayer Perceptron&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now let’s see some tricks that extend these ideas to Deep neural networks.&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="https://blog.allmattersai.com/categories/AI/"/>
    
      <category term="Deep Learning Fundamentals" scheme="https://blog.allmattersai.com/categories/AI/Deep-Learning-Fundamentals/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.allmattersai.com/tags/Deep-Learning/"/>
    
      <category term="Neural Networks" scheme="https://blog.allmattersai.com/tags/Neural-Networks/"/>
    
  </entry>
  
  <entry>
    <title>Cost Function and Gradient Descent</title>
    <link href="https://blog.allmattersai.com/cost-function-and-gradient-descent/"/>
    <id>https://blog.allmattersai.com/cost-function-and-gradient-descent/</id>
    <published>2018-05-21T03:39:54.000Z</published>
    <updated>2018-05-24T06:23:55.185Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;In the last &lt;a href=&quot;/multilayer-perceptrons-and-activation-function&quot;&gt;post&lt;/a&gt; we’ve discussed how a Multilayer Perceptron works. Now let’s see how to search/update a set of weights to achieve a good classifer.&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="https://blog.allmattersai.com/categories/AI/"/>
    
      <category term="Deep Learning Fundamentals" scheme="https://blog.allmattersai.com/categories/AI/Deep-Learning-Fundamentals/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.allmattersai.com/tags/Deep-Learning/"/>
    
      <category term="Neural Networks" scheme="https://blog.allmattersai.com/tags/Neural-Networks/"/>
    
  </entry>
  
  <entry>
    <title>Multilayer Perceptrons and Activation function</title>
    <link href="https://blog.allmattersai.com/multilayer-perceptrons-and-activation-function/"/>
    <id>https://blog.allmattersai.com/multilayer-perceptrons-and-activation-function/</id>
    <published>2018-04-09T05:42:04.000Z</published>
    <updated>2018-05-19T01:44:28.670Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;The-XOR-problem&quot;&gt;&lt;a href=&quot;#The-XOR-problem&quot; class=&quot;headerlink&quot; title=&quot;The XOR problem&quot;&gt;&lt;/a&gt;The XOR problem&lt;/h2&gt;&lt;p&gt;In the previous &lt;a href=&quot;/deep-learning-fundamentals-neuron-and-the-perceptron-algorithm&quot;&gt;post&lt;/a&gt; you’ve seen how a perceptron works. Now let’s dive into some more interesting problems in deep learning. What follows is the classic XOR problem. Develop a method for the correct classification of the following points.&lt;/p&gt;
    
    </summary>
    
      <category term="AI" scheme="https://blog.allmattersai.com/categories/AI/"/>
    
      <category term="Deep Learning Fundamentals" scheme="https://blog.allmattersai.com/categories/AI/Deep-Learning-Fundamentals/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.allmattersai.com/tags/Deep-Learning/"/>
    
      <category term="Neural Networks" scheme="https://blog.allmattersai.com/tags/Neural-Networks/"/>
    
  </entry>
  
  <entry>
    <title>Neuron and the Perceptron Algorithm</title>
    <link href="https://blog.allmattersai.com/neuron-and-the-perceptron-algorithm/"/>
    <id>https://blog.allmattersai.com/neuron-and-the-perceptron-algorithm/</id>
    <published>2018-03-28T04:10:04.000Z</published>
    <updated>2018-05-24T06:19:07.946Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;This is the first part of the series &lt;em&gt;“Deep Learning Fundamentals”&lt;/em&gt;. The goal of this series is to explore the mechanisms of artificial neural networks. The focus is more on presenting an intuitive way of understanding neural networks. So, you can expect an emphasis on how and why things work rather than what does the job. More often than not I’ll try to use simple math without focusing on notation. Let’s jump into the fundamental unit of most of the neural networks - “a neuron”.&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="https://blog.allmattersai.com/categories/AI/"/>
    
      <category term="Deep Learning Fundamentals" scheme="https://blog.allmattersai.com/categories/AI/Deep-Learning-Fundamentals/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.allmattersai.com/tags/Deep-Learning/"/>
    
      <category term="Neural Networks" scheme="https://blog.allmattersai.com/tags/Neural-Networks/"/>
    
  </entry>
  
</feed>
