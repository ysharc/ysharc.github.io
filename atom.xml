<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>All Matters AI</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://blog.allmattersai.com/"/>
  <updated>2018-05-27T03:55:39.266Z</updated>
  <id>https://blog.allmattersai.com/</id>
  
  <author>
    <name>Yeshwanth Arcot</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Forward and Backward Propagation</title>
    <link href="https://blog.allmattersai.com/forward-and-backward-propagation/"/>
    <id>https://blog.allmattersai.com/forward-and-backward-propagation/</id>
    <published>2018-05-26T00:42:04.000Z</published>
    <updated>2018-05-27T03:55:39.266Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Till now we’ve seen&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;/multilayer-perceptrons-and-activation-function/&quot;&gt;How a Multilayer Perceptron works&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;/cost-function-and-gradient-descent/&quot;&gt;How Gradient Descent helps Multilayer Perceptron&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Now let’s see some tricks that extend these ideas to Deep neural networks and speed up the whole learning process.&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="https://blog.allmattersai.com/categories/AI/"/>
    
      <category term="Deep Learning Fundamentals" scheme="https://blog.allmattersai.com/categories/AI/Deep-Learning-Fundamentals/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.allmattersai.com/tags/Deep-Learning/"/>
    
      <category term="Neural Networks" scheme="https://blog.allmattersai.com/tags/Neural-Networks/"/>
    
  </entry>
  
  <entry>
    <title>Cost Function and Gradient Descent</title>
    <link href="https://blog.allmattersai.com/cost-function-and-gradient-descent/"/>
    <id>https://blog.allmattersai.com/cost-function-and-gradient-descent/</id>
    <published>2018-05-21T03:39:54.000Z</published>
    <updated>2018-05-24T06:23:55.185Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;In the last &lt;a href=&quot;/multilayer-perceptrons-and-activation-function&quot;&gt;post&lt;/a&gt; we’ve discussed how a Multilayer Perceptron works. Now let’s see how to search/update a set of weights to achieve a good classifer.&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="https://blog.allmattersai.com/categories/AI/"/>
    
      <category term="Deep Learning Fundamentals" scheme="https://blog.allmattersai.com/categories/AI/Deep-Learning-Fundamentals/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.allmattersai.com/tags/Deep-Learning/"/>
    
      <category term="Neural Networks" scheme="https://blog.allmattersai.com/tags/Neural-Networks/"/>
    
  </entry>
  
  <entry>
    <title>Multilayer Perceptrons and Activation function</title>
    <link href="https://blog.allmattersai.com/multilayer-perceptrons-and-activation-function/"/>
    <id>https://blog.allmattersai.com/multilayer-perceptrons-and-activation-function/</id>
    <published>2018-04-09T05:42:04.000Z</published>
    <updated>2018-05-19T01:44:28.670Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;The-XOR-problem&quot;&gt;&lt;a href=&quot;#The-XOR-problem&quot; class=&quot;headerlink&quot; title=&quot;The XOR problem&quot;&gt;&lt;/a&gt;The XOR problem&lt;/h2&gt;&lt;p&gt;In the previous &lt;a href=&quot;/deep-learning-fundamentals-neuron-and-the-perceptron-algorithm&quot;&gt;post&lt;/a&gt; you’ve seen how a perceptron works. Now let’s dive into some more interesting problems in deep learning. What follows is the classic XOR problem. Develop a method for the correct classification of the following points.&lt;/p&gt;
    
    </summary>
    
      <category term="AI" scheme="https://blog.allmattersai.com/categories/AI/"/>
    
      <category term="Deep Learning Fundamentals" scheme="https://blog.allmattersai.com/categories/AI/Deep-Learning-Fundamentals/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.allmattersai.com/tags/Deep-Learning/"/>
    
      <category term="Neural Networks" scheme="https://blog.allmattersai.com/tags/Neural-Networks/"/>
    
  </entry>
  
  <entry>
    <title>Neuron and the Perceptron Algorithm</title>
    <link href="https://blog.allmattersai.com/neuron-and-the-perceptron-algorithm/"/>
    <id>https://blog.allmattersai.com/neuron-and-the-perceptron-algorithm/</id>
    <published>2018-03-28T04:10:04.000Z</published>
    <updated>2018-05-24T06:19:07.946Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;This is the first part of the series &lt;em&gt;“Deep Learning Fundamentals”&lt;/em&gt;. The goal of this series is to explore the mechanisms of artificial neural networks. The focus is more on presenting an intuitive way of understanding neural networks. So, you can expect an emphasis on how and why things work rather than what does the job. More often than not I’ll try to use simple math without focusing on notation. Let’s jump into the fundamental unit of most of the neural networks - “a neuron”.&lt;br&gt;
    
    </summary>
    
      <category term="AI" scheme="https://blog.allmattersai.com/categories/AI/"/>
    
      <category term="Deep Learning Fundamentals" scheme="https://blog.allmattersai.com/categories/AI/Deep-Learning-Fundamentals/"/>
    
    
      <category term="Deep Learning" scheme="https://blog.allmattersai.com/tags/Deep-Learning/"/>
    
      <category term="Neural Networks" scheme="https://blog.allmattersai.com/tags/Neural-Networks/"/>
    
  </entry>
  
</feed>
